{
    "Artificial Intelligence": [
        {
            "title": "Artificial intelligence",
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "content": "artificial intelligence ai in its broadest sense is intelligence exhibited by machines particularly computer systems it is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals such machines may be called ais high profile applications of ai include advanced web search engines e g google search recommendation systems used by youtube amazon and netflix virtual assistants e g google assistant siri and alexa autonomous vehicles e g waymo generative and creative tools e g chatgpt and ai art and superhuman play and analysis in strategy games e g chess and go however many ai applications are not perceived as ai a lot of cutting edge ai has filtered into general applications often without being called ai because once something becomes useful enough and common enough it s not labeled ai anymore various subfields of ai research are centered around particular goals and the use of particular tools the traditional goals of ai research include reasoning knowledge representation planning learning natural language processing perception and support for robotics general intelligence the ability to complete any task performed by a human on an at least equal level is among the field s long term goals to reach these goals ai researchers have adapted and integrated a wide range of techniques including search and mathematical optimization formal logic artificial neural networks and methods based on statistics operations research and economics ai also draws upon psychology linguistics philosophy neuroscience and other fields artificial intelligence was founded as an academic discipline in 1956 and the field went through multiple cycles of optimism followed by periods of disappointment and loss of funding known as ai winters funding and interest vastly increased after 2012 when deep learning outperformed previous ai techniques this growth accelerated further after 2017 with the transformer architecture and by the early 2020s many billions of dollars were being invested in ai known as the ai boom the widespread use of ai in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long term effects in the future prompting discussions about regulatory policies to ensure the safety and benefits of the technology goals the general problem of simulating or creating intelligence has been broken into subproblems these consist of particular traits or capabilities that researchers expect an intelligent system to display the traits described below have received the most attention and cover the scope of ai research reasoning and problem solving early researchers developed algorithms that imitated step by step reasoning that humans use when they solve puzzles or make logical deductions by the late 1980s and 1990s methods were developed for dealing with uncertain or incomplete information employing concepts from probability and economics many of these algorithms are insufficient for solving large reasoning problems because they experience a combinatorial explosion they become exponentially slower as the problems grow even humans rarely use the step by step deduction that early ai research could model they solve most of their problems using fast intuitive judgments accurate and efficient reasoning is an unsolved problem knowledge representation knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real world facts formal knowledge representations are used in content based indexing and retrieval scene interpretation clinical decision support knowledge discovery mining interesting and actionable inferences from large databases and other areas a knowledge base is a body of knowledge represented in a form that can be used by a program an ontology is the set of objects relations concepts and properties used by a particular domain of knowledge knowledge bases need to represent things such as objects properties categories and relations between objects situations events states and time causes and effects knowledge about knowledge what we know about what other people know default reasoning things that humans assume are true until they are told differently and will remain true even when other facts are changing and many other aspects and domains of knowledge among the most difficult problems in knowledge representation are the breadth of commonsense knowledge the set of atomic facts that the average person knows is enormous and the sub symbolic form of most commonsense knowledge much of what people know is not represented as facts or statements that they could express verbally there is also the difficulty of knowledge acquisition the problem of obtaining knowledge for ai applications planning and decision making an agent is anything that perceives and takes actions in the world a rational agent has goals or preferences and takes actions to make them happen in automated planning the agent has a specific goal in automated decision making the agent has preferences there are some situations it would prefer to be in and some situations it is trying to avoid the decision making agent assigns a number to each situation called the utility that measures how much the agent prefers it for each possible action it can calculate the expected utility the utility of all possible outcomes of the action weighted by the probability that the outcome will occur it can then choose the action with the maximum expected utility in classical planning the agent knows exactly what the effect of any action will be in most real world problems however the agent may not be certain about the situation they are in it is unknown or unobservable and it may not know for certain what will happen after each possible action it is not deterministic it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked in some problems the agent s preferences may be uncertain especially if there are other agents or humans involved these can be learned e g with inverse reinforcement learning or the agent can seek information to improve its preferences information value theory can be used to weigh the value of exploratory or experimental actions the space of possible future actions and situations is typically intractably large so the agents must take actions and evaluate situations while being uncertain of what the outcome will be a markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action a policy associates a decision with each possible state the policy could be calculated e g by iteration be heuristic or it can be learned game theory describes the rational behavior of multiple interacting agents and is used in ai programs that make decisions that involve other agents learning machine learning is the study of programs that can improve their performance on a given task automatically it has been a part of ai from the beginning there are several kinds of machine learning unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance supervised learning requires labeling the training data with the expected answers and comes in two main varieties classification where the program must learn to predict what category the input belongs in and regression where the program must deduce a numeric function based on numeric input in reinforcement learning the agent is rewarded for good responses and punished for bad ones the agent learns to choose responses that are classified as good transfer learning is when the knowledge gained from one problem is applied to a new problem deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning computational learning theory can assess learners by computational complexity by sample complexity how much data is required or by other notions of optimization natural language processing natural language processing nlp allows programs to read write and communicate in human languages such as english specific problems include speech recognition speech synthesis machine translation information extraction information retrieval and question answering early work based on noam chomsky s generative grammar and semantic networks had difficulty with word sense disambiguation unless restricted to small domains called micro worlds due to the common sense knowledge problem margaret masterman believed that it was meaning and not grammar that was the key to understanding languages and that thesauri and not dictionaries should be the basis of computational language structure modern deep learning techniques for nlp include word embedding representing words typically as vectors encoding their meaning transformers a deep learning architecture using an attention mechanism and others in 2019 generative pre trained transformer or gpt language models began to generate coherent text and by 2023 these models were able to get human level scores on the bar exam sat test gre test and many other real world applications perception machine perception is the ability to use input from sensors such as cameras microphones wireless signals active lidar sonar radar and tactile sensors to deduce aspects of the world computer vision is the ability to analyze visual input the field includes speech recognition image classification facial recognition object recognition object tracking and robotic perception social intelligence affective computing is an interdisciplinary umbrella that comprises systems that recognize interpret process or simulate human feeling emotion and mood for example some virtual assistants are programmed to speak conversationally or even to banter humorously it makes them appear more sensitive to the emotional dynamics of human interaction or to otherwise facilitate human computer interaction however this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents moderate successes related to affective computing include textual sentiment analysis and more recently multimodal sentiment analysis wherein ai classifies the effects displayed by a videotaped subject general intelligence a machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence techniques ai research uses a wide variety of techniques to accomplish the goals above search and optimization ai can solve many problems by intelligently searching through many possible solutions there are two very different kinds of search used in ai state space search and local search state space search state space search searches through a tree of possible states to try to find a goal state for example planning algorithms search through trees of goals and subgoals attempting to find a path to a target goal a process called means ends analysis simple exhaustive searches are rarely sufficient for most real world problems the search space the number of places to search quickly grows to astronomical numbers the result is a search that is too slow or never completes heuristics or rules of thumb can help prioritize choices that are more likely to reach a goal adversarial search is used for game playing programs such as chess or go it searches through a tree of possible moves and countermoves looking for a winning position local search local search uses mathematical optimization to find a solution to a problem it begins with some form of guess and refines it incrementally gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function variants of gradient descent are commonly used to train neural networks another type of local search is evolutionary computation which aims to iteratively improve a set of candidate solutions by mutating and recombining them selecting only the fittest to survive each generation distributed search processes can coordinate via swarm intelligence algorithms two popular swarm algorithms used in search are particle swarm optimization inspired by bird flocking and ant colony optimization inspired by ant trails logic formal logic is used for reasoning and knowledge representation formal logic comes in two main forms propositional logic which operates on statements that are true or false and uses logical connectives such as and or not and implies and predicate logic which also operates on objects predicates and relations and uses quantifiers such as every x is a y and there are some xs that are ys deductive reasoning in logic is the process of proving a new statement conclusion from other statements that are given and assumed to be true the premises proofs can be structured as proof trees in which nodes are labelled by sentences and children nodes are connected to parent nodes by inference rules given a problem and a set of premises problem solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms in the case of horn clauses problem solving search can be performed by reasoning forwards from the premises or backwards from the problem in the more general case of the clausal form of first order logic resolution is a single axiom free rule of inference in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved inference in both horn clause logic and first order logic is undecidable and therefore intractable however backward reasoning with horn clauses which underpins computation in the logic programming language prolog is turing complete moreover its efficiency is competitive with computation in other symbolic programming languages fuzzy logic assigns a degree of truth between 0 and 1 it can therefore handle propositions that are vague and partially true non monotonic logics including logic programming with negation as failure are designed to handle default reasoning other specialized versions of logic have been developed to describe many complex domains probabilistic methods for uncertain reasoning many problems in ai including in reasoning planning learning perception and robotics require the agent to operate with incomplete or uncertain information ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics precise mathematical tools have been developed that analyze how an agent can make choices and plan using decision theory decision analysis and information value theory these tools include models such as markov decision processes dynamic decision networks game theory and mechanism design bayesian networks are a tool that can be used for reasoning using the bayesian inference algorithm learning using the expectation maximization algorithm planning using decision networks and perception using dynamic bayesian networks probabilistic algorithms can also be used for filtering prediction smoothing and finding explanations for streams of data thus helping perception systems analyze processes that occur over time e g hidden markov models or kalman filters classifiers and statistical learning methods the simplest ai applications can be divided into two types classifiers e g if shiny then diamond on one hand and controllers e g if diamond then pick up on the other hand classifiers are functions that use pattern matching to determine the closest match they can be fine tuned based on chosen examples using supervised learning each pattern also called an observation is labeled with a certain predefined class all the observations combined with their class labels are known as a data set when a new observation is received that observation is classified based on previous experience there are many kinds of classifiers in use the decision tree is the simplest and most widely used symbolic machine learning algorithm k nearest neighbor algorithm was the most widely used analogical ai until the mid 1990s and kernel methods such as the support vector machine svm displaced k nearest neighbor in the 1990s the naive bayes classifier is reportedly the most widely used learner at google due in part to its scalability neural networks are also used as classifiers artificial neural networks an artificial neural network is based on a collection of nodes also known as artificial neurons which loosely model the neurons in a biological brain it is trained to recognise patterns once trained it can recognise those patterns in fresh data there is an input at least one hidden layer of nodes and an output each node applies a function and once the weight crosses its specified threshold the data is transmitted to the next layer a network is typically called a deep neural network if it has at least 2 hidden layers learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training the most common training technique is the backpropagation algorithm neural networks learn to model complex relationships between inputs and outputs and find patterns in data in theory a neural network can learn any function in feedforward neural networks the signal passes in only one direction recurrent neural networks feed the output signal back into the input which allows short term memories of previous input events long short term memory is the most successful network architecture for recurrent networks perceptrons use only a single layer of neurons deep learning uses multiple layers convolutional neural networks strengthen the connection between neurons that are close to each other this is especially important in image processing where a local set of neurons must identify an edge before the network can identify an object deep learning deep learning uses several layers of neurons between the network s inputs and outputs the multiple layers can progressively extract higher level features from the raw input for example in image processing lower layers may identify edges while higher layers may identify the concepts relevant to a human such as digits letters or faces deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence including computer vision speech recognition natural language processing image classification and others the reason that deep learning performs so well in so many applications is not known as of 2023 the sudden success of deep learning in 2012 2015 did not occur because of some new discovery or theoretical breakthrough deep neural networks and backpropagation had been described by many people as far back as the 1950s but because of two factors the incredible increase in computer power including the hundred fold increase in speed by switching to gpus and the availability of vast amounts of training data especially the giant curated datasets used for benchmark testing such as imagenet gpt generative pre trained transformers gpt are large language models llms that generate text based on the semantic relationships between words in sentences text based gpt models are pretrained on a large corpus of text that can be from the internet the pretraining consists of predicting the next token a token being usually a word subword or punctuation throughout this pretraining gpt models accumulate knowledge about the world and can then generate human like text by repeatedly predicting the next token typically a subsequent training phase makes the model more truthful useful and harmless usually with a technique called reinforcement learning from human feedback rlhf current gpt models are prone to generating falsehoods called hallucinations although this can be reduced with rlhf and quality data they are used in chatbots which allow people to ask a question or request a task in simple text current models and services include gemini formerly bard chatgpt grok claude copilot and llama multimodal gpt models can process different types of data modalities such as images videos sound and text hardware and software in the late 2010s graphics processing units gpus that were increasingly designed with ai specific enhancements and used with specialized tensorflow software had replaced previously used central processing unit cpus as the dominant means for large scale commercial and academic machine learning models training specialized programming languages such as prolog were used in early ai research but general purpose programming languages like python have become predominant the transistor density in integrated circuits has been observed to roughly double every 18 months a trend known as moore s law named after the intel co founder gordon moore who first identified it improvements in gpus have been even faster applications ai and machine learning technology is used in most of the essential applications of the 2020s including search engines such as google search targeting online advertisements recommendation systems offered by netflix youtube or amazon driving internet traffic targeted advertising adsense facebook virtual assistants such as siri or alexa autonomous vehicles including drones adas and self driving cars automatic language translation microsoft translator google translate facial recognition apple s face id or microsoft s deepface and google s facenet and image labeling used by facebook apple s iphoto and tiktok the deployment of ai may be overseen by a chief automation officer cao health and medicine the application of ai in medicine and medical research has the potential to increase patient care and quality of life through the lens of the hippocratic oath medical professionals are ethically compelled to use ai if applications can more accurately diagnose and treat patients for medical research ai is an important tool for processing and integrating big data this is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication it has been suggested that ai can overcome discrepancies in funding allocated to different fields of research new ai tools can deepen the understanding of biomedically relevant pathways for example alphafold 2 2021 demonstrated the ability to approximate in hours rather than months the 3d structure of a protein in 2023 it was reported that ai guided drug discovery helped find a class of antibiotics capable of killing two different types of drug resistant bacteria in 2024 researchers used machine learning to accelerate the search for parkinson s disease drug treatments their aim was to identify compounds that block the clumping or aggregation of alpha synuclein the protein that characterises parkinson s disease they were able to speed up the initial screening process ten fold and reduce the cost by a thousand fold sexuality applications of ai in this domain include ai enabled menstruation and fertility trackers that analyze user data to offer prediction ai integrated sex toys e g teledildonics ai generated sexual education content and ai agents that simulate sexual and romantic partners e g replika ai is also used for the production of non consensual deepfake pornography raising significant ethical and legal concerns ai technologies have also been used to attempt to identify online gender based violence and online sexual grooming of minors games game playing programs have been used since the 1950s to demonstrate and test ai s most advanced techniques deep blue became the first computer chess playing system to beat a reigning world chess champion garry kasparov on 11 may 1997 in 2011 in a jeopardy quiz show exhibition match ibm s question answering system watson defeated the two greatest jeopardy champions brad rutter and ken jennings by a significant margin in march 2016 alphago won 4 out of 5 games of go in a match with go champion lee sedol becoming the first computer go playing system to beat a professional go player without handicaps then in 2017 it defeated ke jie who was the best go player in the world other programs handle imperfect information games such as the poker playing program pluribus deepmind developed increasingly generalistic reinforcement learning models such as with muzero which could be trained to play chess go or atari games in 2019 deepmind s alphastar achieved grandmaster level in starcraft ii a particularly challenging real time strategy game that involves incomplete knowledge of what happens on the map in 2021 an ai agent competed in a playstation gran turismo competition winning against four of the world s best gran turismo drivers using deep reinforcement learning in 2024 google deepmind introduced sima a type of ai capable of autonomously playing nine previously unseen open world video games by observing screen output as well as executing short specific tasks in response to natural language instructions mathematics in mathematics special forms of formal step by step reasoning are used in contrast llms such as gpt 4 turbo gemini ultra claude opus llama 2 or mistral large are working with probabilistic models which can produce wrong answers in the form of hallucinations therefore they need not only a large database of mathematical problems to learn from but also methods such as supervised fine tuning or trained classifiers with human annotated data to improve answers for new problems and learn from corrections a 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low even for problems with only minor deviations from trained data alternatively dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alpha tensor alpha geometry and alpha proof all from google deepmind llemma from eleuther or julius when natural language is used to describe mathematical problems converters transform such prompts into a formal language such as lean to define mathematical tasks some models have been developed to solve challenging problems and reach good results in benchmark tests others to serve as educational tools in mathematics finance finance is one of the fastest growing sectors where applied ai tools are being deployed from retail online banking to investment advice and insurance where automated robot advisers have been in use for some years world pensions experts like nicolas firzli insist it may be too early to see the emergence of highly innovative ai informed financial products and services the deployment of ai tools will simply further automatise things destroying tens of thousands of jobs in banking financial planning and pension advice in the process but i m not sure it will unleash a new wave of pension innovation military various countries are deploying ai military applications the main applications enhance command and control communications sensors integration and interoperability research is targeting intelligence collection and analysis logistics cyber operations information operations and semiautonomous and autonomous vehicles ai technologies enable coordination of sensors and effectors threat detection and identification marking of enemy positions target acquisition coordination and deconfliction of distributed joint fires between networked combat vehicles involving manned and unmanned teams ai has been used in military operations in iraq syria israel and ukraine generative ai in the early 2020s generative ai gained widespread prominence genai is ai capable of generating text images videos or other data using generative models often in response to prompts in march 2023 58 of u s adults had heard about chatgpt and 14 had tried it the increasing realism and ease of use of ai based text to image generators such as midjourney dall e and stable diffusion sparked a trend of viral ai generated photos widespread attention was gained by a fake photo of pope francis wearing a white puffer coat the fictional arrest of donald trump and a hoax of an attack on the pentagon as well as the usage in professional creative arts agents artificial intelligent ai agents are software entities designed to perceive their environment make decisions and take actions autonomously to achieve specific goals these agents can interact with users their environment or other agents ai agents are used in various applications including virtual assistants chatbots autonomous vehicles game playing systems and industrial robotics ai agents operate within the constraints of their programming available computational resources and hardware limitations this means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities in real world applications ai agents often face time constraints for decision making and action execution many ai agents incorporate learning algorithms enabling them to improve their performance over time through experience or training using machine learning ai agents can adapt to new situations and optimise their behaviour for their designated tasks other industry specific tasks there are also thousands of successful ai applications used to solve specific problems for specific industries or institutions in a 2017 survey one in five companies reported having incorporated ai in some offerings or processes a few examples are energy storage medical diagnosis military logistics applications that predict the result of judicial decisions foreign policy or supply chain management ai applications for evacuation and disaster management are growing ai has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from gps videos or social media further ai can provide real time information on the real time evacuation conditions in agriculture ai has helped farmers identify areas that need irrigation fertilization pesticide treatments or increasing yield agronomists use ai to conduct research and development ai has been used to predict the ripening time for crops such as tomatoes monitor soil moisture operate agricultural robots conduct predictive analytics classify livestock pig call emotions automate greenhouses detect diseases and pests and save water artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications mainly for classification regression clustering forecasting generation discovery and the development of new scientific insights for example it is used for discovering exoplanets forecasting solar activity and distinguishing between signals and instrumental effects in gravitational wave astronomy additionally it could be used for activities in space such as space exploration including the analysis of data from space missions real time science decisions of spacecraft space debris avoidance and more autonomous operation during the 2024 indian elections us 50 millions was spent on authorized ai generated content notably by creating deepfakes of allied including sometimes deceased politicians to better engage with voters and by translating speeches to various local languages ethics ai has potential benefits and potential risks ai may be able to advance science and find solutions for serious problems demis hassabis of deepmind hopes to solve intelligence and then use that to solve everything else however as the use of ai has become widespread several unintended consequences and risks have been identified in production systems can sometimes not factor ethics and bias into their ai training processes especially when the ai algorithms are inherently unexplainable in deep learning risks and harm privacy and copyright machine learning algorithms require large amounts of data the techniques used to acquire this data have raised concerns about privacy surveillance and copyright ai powered devices and services such as virtual assistants and iot products continuously collect personal information raising concerns about intrusive data gathering and unauthorized access by third parties the loss of privacy is further exacerbated by ai s ability to process and combine vast amounts of data potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency sensitive user data collected may include online activity records geolocation data video or audio for example in order to build speech recognition algorithms amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy ai developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data such as data aggregation de identification and differential privacy since 2016 some privacy experts such as cynthia dwork have begun to view privacy in terms of fairness brian christian wrote that experts have pivoted from the question of what they know to the question of what they re doing with it generative ai is often trained on unlicensed copyrighted works including in domains such as images or computer code the output is then used under the rationale of fair use experts disagree about how well and under what circumstances this rationale will hold up in courts of law relevant factors may include the purpose and character of the use of the copyrighted work and the effect upon the potential market for the copyrighted work website owners who do not wish to have their content scraped can indicate it in a robots txt file in 2023 leading authors including john grisham and jonathan franzen sued ai companies for using their work to train generative ai another discussed approach is to envision a separate sui generis system of protection for creations generated by ai to ensure fair attribution and compensation for human authors dominance by tech giants the commercial ai scene is dominated by big tech companies such as alphabet inc amazon apple inc meta platforms and microsoft some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers allowing them to entrench further in the marketplace power needs and environmental impacts in january 2024 the international energy agency iea released electricity 2024 analysis and forecast to 2026 forecasting electric power use this is the first iea report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency the report states that power demand for these uses might double by 2026 with additional electric power usage equal to electricity used by the whole japanese nation prodigious power consumption by ai is responsible for the growth of fossil fuels use and might delay closings of obsolete carbon emitting coal energy facilities there is a feverish rise in the construction of data centers throughout the us making large technology firms e g microsoft meta google amazon into voracious consumers of electric power projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source a chatgpt search involves the use of 10 times the electrical energy as a google search the large firms are in haste to find power sources from nuclear energy to geothermal to fusion the tech firms argue that in the long view ai will be eventually kinder to the environment but they need the energy now ai makes the power grid more efficient and intelligent will assist in the growth of nuclear power and track overall carbon emissions according to technology firms a 2024 goldman sachs research paper ai data centers and the coming us power demand surge found us power demand is likely to experience growth not seen in a generation and forecasts that by 2030 us data centers will consume 8 of us power as opposed to 3 in 2022 presaging growth for the electrical power generation industry by a variety of means data centers need for more and more electrical power is such that they might max out the electrical grid the big tech companies counter that ai can be used to maximize the utilization of the grid by all in 2024 the wall street journal reported that big ai companies have begun negotiations with the us nuclear power providers to provide electricity to the data centers in march 2024 amazon purchased a pennsylvania nuclear powered data center for 650 million us nvidia ceo jen hsun huang said nuclear power is a good option for the data centers in september 2024 microsoft announced an agreement with constellation energy to re open the three mile island nuclear power plant to provide microsoft with 100 of all electric power produced by the plant for 20 years reopening the plant which suffered a partial nuclear meltdown of its unit 2 reactor in 1979 will require constellation to get through strict regulatory processes which will include extensive safety scrutiny from the us nuclear regulatory commission if approved this will be the first ever us re commissioning of a nuclear plant over 835 megawatts of power enough for 800 000 homes of energy will be produced the cost for re opening and upgrading is estimated at 1 6 billion us and is dependent on tax breaks for nuclear power contained in the 2022 us inflation reduction act the us government and the state of michigan are investing almost 2 billion us to reopen the palisades nuclear reactor on lake michigan closed since 2022 the plant is planned to be reopened in october 2025 the three mile island facility will be renamed the crane clean energy center after chris crane a nuclear proponent and former ceo of exelon who was responsible for exelon spinoff of constellation after the last approval in september 2023 taiwan suspended the approval of data centers north of taoyuan with a capacity of more than 5 mw in 2024 due to power supply shortages taiwan aims to phase out nuclear power by 2025 on the other hand singapore imposed a ban on the opening of data centers in 2019 due to electric power but in 2022 lifted this ban although most nuclear plants in japan have been shut down after the 2011 fukushima nuclear accident according to an october 2024 bloomberg article in japanese cloud gaming services company ubitus in which nvidia has a stake is looking for land in japan near nuclear power plant for a new data center for generative ai ubitus ceo wesley kuo said nuclear power plants are the most efficient cheap and stable power for ai on 1 november 2024 the federal energy regulatory commission ferc rejected an application submitted by talen energy for approval to supply some electricity from the nuclear power station susquehanna to amazon s data center according to the commission chairman willie l phillips it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors misinformation youtube facebook and others use recommender systems to guide users to more content these ai programs were given the goal of maximizing user engagement that is the only goal was to keep people watching the ai learned that users tended to choose misinformation conspiracy theories and extreme partisan content and to keep them watching the ai recommended more of it users also tended to watch more content on the same subject so the ai led people into filter bubbles where they received multiple versions of the same misinformation this convinced many users that the misinformation was true and ultimately undermined trust in institutions the media and the government the ai program had correctly learned to maximize its goal but the result was harmful to society after the u s election in 2016 major technology companies took steps to mitigate the problem in 2022 generative ai began to create images audio video and text that are indistinguishable from real photographs recordings films or human writing it is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda ai pioneer geoffrey hinton expressed concern about ai enabling authoritarian leaders to manipulate their electorates on a large scale among other risks algorithmic bias and fairness machine learning applications will be biased if they learn from biased data the developers may not be aware that the bias exists bias can be introduced by the way training data is selected and by the way a model is deployed if a biased algorithm is used to make decisions that can seriously harm people as it can in medicine finance recruitment housing or policing then the algorithm may cause discrimination the field of fairness studies how to prevent harms from algorithmic biases on june 28 2015 google photos s new image labeling feature mistakenly identified jacky alcine and a friend as gorillas because they were black the system was trained on a dataset that contained very few images of black people a problem called sample size disparity google fixed this problem by preventing the system from labelling anything as a gorilla eight years later in 2023 google photos still could not identify a gorilla and neither could similar products from apple facebook microsoft and amazon compas is a commercial program widely used by u s courts to assess the likelihood of a defendant becoming a recidivist in 2016 julia angwin at propublica discovered that compas exhibited racial bias despite the fact that the program was not told the races of the defendants although the error rate for both whites and blacks was calibrated equal at exactly 61 the errors for each race were different the system consistently overestimated the chance that a black person would re offend and would underestimate the chance that a white person would not re offend in 2017 several researchers showed that it was mathematically impossible for compas to accommodate all possible measures of fairness when the base rates of re offense were different for whites and blacks in the data a program can make biased decisions even if the data does not explicitly mention a problematic feature such as race or gender the feature will correlate with other features like address shopping history or first name and the program will make the same decisions based on these features as it would on race or gender moritz hardt said the most robust fact in this research area is that fairness through blindness doesn t work criticism of compas highlighted that machine learning models are designed to make predictions that are only valid if we assume that the future will resemble the past if they are trained on data that includes the results of racist decisions in the past machine learning models must predict that racist decisions will be made in the future if an application then uses these predictions as recommendations some of these recommendations will likely be racist thus machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past it is descriptive rather than prescriptive bias and unfairness may go undetected because the developers are overwhelmingly white and male among ai engineers about 4 are black and 20 are women there are various conflicting definitions and mathematical models of fairness these notions depend on ethical assumptions and are influenced by beliefs about society one broad category is distributive fairness which focuses on the outcomes often identifying groups and seeking to compensate for statistical disparities representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible procedural fairness focuses on the decision process rather than the outcome the most relevant notions of fairness may depend on the context notably the type of ai application and the stakeholders the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases but it may conflict with anti discrimination laws at its 2022 conference on fairness accountability and transparency acm facct 2022 the association for computing machinery in seoul south korea presented and published findings that recommend that until ai and robotics systems are demonstrated to be free of bias mistakes they are unsafe and the use of self learning neural networks trained on vast unregulated sources of flawed internet data should be curtailed lack of transparency many ai systems are so complex that their designers cannot explain how they reach their decisions particularly with deep neural networks in which there are a large amount of non linear relationships between inputs and outputs but some popular explainability techniques exist it is impossible to be certain that a program is operating correctly if no one knows how exactly it works there have been many cases where a machine learning program passed rigorous tests but nevertheless learned something different than what the programmers intended for example a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as cancerous because pictures of malignancies typically include a ruler to show the scale another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at low risk of dying from pneumonia having asthma is actually a severe risk factor but since the patients having asthma would usually get much more medical care they were relatively unlikely to die according to the training data the correlation between asthma and low risk of dying from pneumonia was real but misleading people who have been harmed by an algorithm s decision have a right to an explanation doctors for example are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make early drafts of the european union s general data protection regulation in 2016 included an explicit statement that this right exists industry experts noted that this is an unsolved problem with no solution in sight regulators argued that nevertheless the harm is real if the problem has no solution the tools should not be used darpa established the xai explainable artificial intelligence program in 2014 to try to solve these problems several approaches aim to address the transparency problem shap enables to visualise the contribution of each feature to the output lime can locally approximate a model s outputs with a simpler interpretable model multitask learning provides a large number of outputs in addition to the target classification these other outputs can help developers deduce what the network has learned deconvolution deepdream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned and produce output that can suggest what the network is learning for generative pre trained transformers anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human understandable concepts bad actors and weaponized ai artificial intelligence provides a number of tools that are useful to bad actors such as authoritarian governments terrorists criminals or rogue states a lethal autonomous weapon is a machine that locates selects and engages human targets without human supervision widely available ai tools can be used by bad actors to develop inexpensive autonomous weapons and if produced at scale they are potentially weapons of mass destruction even when used in conventional warfare it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person in 2014 30 nations including china supported a ban on autonomous weapons under the united nations convention on certain conventional weapons however the united states and others disagreed by 2015 over fifty countries were reported to be researching battlefield robots ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways face and voice recognition allow widespread surveillance machine learning operating this data can classify potential enemies of the state and prevent them from hiding recommendation systems can precisely target propaganda and misinformation for maximum effect deepfakes and generative ai aid in producing misinformation advanced ai can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets it lowers the cost and difficulty of digital warfare and advanced spyware all these technologies have been available since 2020 or earlier ai facial recognition systems are already being used for mass surveillance in china there many other ways that ai is expected to help bad actors some of which can not be foreseen for example machine learning ai is able to design tens of thousands of toxic molecules in a matter of hours technological unemployment economists have frequently highlighted the risks of redundancies from ai and speculated about unemployment if there is no adequate social policy for full employment in the past technology has tended to increase rather than reduce total employment but economists acknowledge that we re in uncharted territory with ai a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long term unemployment but they generally agree that it could be a net benefit if productivity gains are redistributed risk estimates vary for example in the 2010s michael osborne and carl benedikt frey estimated 47 of u s jobs are at high risk of potential automation while an oecd report classified only 9 of u s jobs as high risk the methodology of speculating about future employment levels has been criticised as lacking evidential foundation and for implying that technology rather than social policy creates unemployment as opposed to redundancies in april 2023 it was reported that 70 of the jobs for chinese video game illustrators had been eliminated by generative artificial intelligence unlike previous waves of automation many middle class jobs may be eliminated by artificial intelligence the economist stated in 2015 that the worry that ai could do to white collar jobs what steam power did to blue collar ones during the industrial revolution is worth taking seriously jobs at extreme risk range from paralegals to fast food cooks while job demand is likely to increase for care related professions ranging from personal healthcare to the clergy from the early days of the development of artificial intelligence there have been arguments for example those put forward by joseph weizenbaum about whether tasks that can be done by computers actually should be done by them given the difference between computers and humans and between quantitative calculation and qualitative value based judgement existential risk it has been argued ai will become so powerful that humanity may irreversibly lose control of it this could as physicist stephen hawking stated spell the end of the human race this scenario has been common in science fiction when a computer or robot suddenly develops a human like self awareness or sentience or consciousness and becomes a malevolent character these sci fi scenarios are misleading in several ways first ai does not require human like sentience to be an existential risk modern ai programs are given specific goals and use learning and intelligence to achieve them philosopher nick bostrom argued that if one gives almost any goal to a sufficiently powerful ai it may choose to destroy humanity to achieve it he used the example of a paperclip factory manager stuart russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged reasoning that you can t fetch the coffee if you re dead in order to be safe for humanity a superintelligence would have to be genuinely aligned with humanity s morality and values so that it is fundamentally on our side second yuval noah harari argues that ai does not require a robot body or physical control to pose an existential risk the essential parts of civilization are not physical things like ideologies law government money and the economy are built on language they exist because there are stories that billions of people believe the current prevalence of misinformation suggests that an ai could use language to convince people to believe anything even to take actions that are destructive the opinions amongst experts and industry insiders are mixed with sizable fractions both concerned and unconcerned by risk from eventual superintelligent ai personalities such as stephen hawking bill gates and elon musk as well as ai pioneers such as yoshua bengio stuart russell demis hassabis and sam altman have expressed concerns about existential risk from ai in may 2023 geoffrey hinton announced his resignation from google in order to be able to freely speak out about the risks of ai without considering how this impacts google he notably mentioned risks of an ai takeover and stressed that in order to avoid the worst outcomes establishing safety guidelines will require cooperation among those competing in use of ai in 2023 many leading ai experts endorsed the joint statement that mitigating the risk of extinction from ai should be a global priority alongside other societal scale risks such as pandemics and nuclear war some other researchers were more optimistic ai pioneer jürgen schmidhuber did not sign the joint statement emphasising that in 95 of all cases ai research is about making human lives longer and healthier and easier while the tools that are now being used to improve lives can also be used by bad actors they can also be used against the bad actors andrew ng also argued that it s a mistake to fall for the doomsday hype on ai and that regulators who do will only benefit vested interests yann lecun scoffs at his peers dystopian scenarios of supercharged misinformation and even eventually human extinction in the early 2010s experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine however after 2016 the study of current and future risks and possible solutions became a serious area of research ethical machines and alignment friendly ai are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans eliezer yudkowsky who coined the term argues that developing friendly ai should be a higher research priority it may require a large investment and it must be completed before ai becomes an existential risk machines with intelligence have the potential to use their intelligence to make ethical decisions the field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas the field of machine ethics is also called computational morality and was founded at an aaai symposium in 2005 other approaches include wendell wallach s artificial moral agents and stuart j russell s three principles for developing provably beneficial machines open source active organizations in the ai open source community include hugging face google eleutherai and meta various ai models such as llama 2 mistral or stable diffusion have been made open weight meaning that their architecture and trained parameters the weights are publicly available open weight models can be freely fine tuned which allows companies to specialize them with their own data and for their own use case open weight models are useful for research and innovation but can also be misused since they can be fine tuned any built in security measure such as objecting to harmful requests can be trained away until it becomes ineffective some researchers warn that future ai models may develop dangerous capabilities such as the potential to drastically facilitate bioterrorism and that once released on the internet they cannot be deleted everywhere if needed they recommend pre release audits and cost benefit analyses frameworks artificial intelligence projects can have their ethical permissibility tested while designing developing and implementing an ai system an ai framework such as the care and act framework containing the sum values developed by the alan turing institute tests projects in four main areas respect the dignity of individual people connect with other people sincerely openly and inclusively care for the wellbeing of everyone protect social values justice and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference the montreal declaration for responsible ai and the ieee s ethics of autonomous systems initiative among others however these principles do not go without their criticisms especially regards to the people chosen contributes to these frameworks promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of ai system design development and implementation and collaboration between job roles such as data scientists product managers data engineers domain experts and delivery managers the uk ai safety institute released in 2024 a testing toolset called inspect for ai safety evaluations available under a mit open source licence which is freely available on github and can be improved with third party packages it can be used to evaluate ai models in a range of areas including core knowledge ability to reason and autonomous capabilities regulation the regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating ai it is therefore related to the broader regulation of algorithms the regulatory and policy landscape for ai is an emerging issue in jurisdictions globally according to ai index at stanford the annual number of ai related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone between 2016 and 2020 more than 30 countries adopted dedicated strategies for ai most eu member states had released national ai strategies as had canada china india japan mauritius the russian federation saudi arabia united arab emirates u s and vietnam others were in the process of elaborating their own ai strategy including bangladesh malaysia and tunisia the global partnership on artificial intelligence was launched in june 2020 stating a need for ai to be developed in accordance with human rights and democratic values to ensure public confidence and trust in the technology henry kissinger eric schmidt and daniel huttenlocher published a joint statement in november 2021 calling for a government commission to regulate ai in 2023 openai leaders published recommendations for the governance of superintelligence which they believe may happen in less than 10 years in 2023 the united nations also launched an advisory body to provide recommendations on ai governance the body comprises technology company executives governments officials and academics in 2024 the council of europe created the first international legally binding treaty on ai called the framework convention on artificial intelligence and human rights democracy and the rule of law it was adopted by the european union the united states the united kingdom and other signatories in a 2022 ipsos survey attitudes towards ai varied greatly by country 78 of chinese citizens but only 35 of americans agreed that products and services using ai have more benefits than drawbacks a 2023 reuters ipsos poll found that 61 of americans agree and 22 disagree that ai poses risks to humanity in a 2023 fox news poll 35 of americans thought it very important and an additional 41 thought it somewhat important for the federal government to regulate ai versus 13 responding not very important and 8 responding not at all important in november 2023 the first global ai safety summit was held in bletchley park in the uk to discuss the near and far term risks of ai and the possibility of mandatory and voluntary regulatory frameworks 28 countries including the united states china and the european union issued a declaration at the start of the summit calling for international co operation to manage the challenges and risks of artificial intelligence in may 2024 at the ai seoul summit 16 global ai tech companies agreed to safety commitments on the development of ai history the study of mechanical or formal reasoning began with philosophers and mathematicians in antiquity the study of logic led directly to alan turing s theory of computation which suggested that a machine by shuffling symbols as simple as 0 and 1 could simulate any conceivable form of mathematical reasoning this along with concurrent discoveries in cybernetics information theory and neurobiology led researchers to consider the possibility of building an electronic brain they developed several areas of research that would become part of ai such as mccullouch and pitts design for artificial neurons in 1943 and turing s influential 1950 paper computing machinery and intelligence which introduced the turing test and showed that machine intelligence was plausible the field of ai research was founded at a workshop at dartmouth college in 1956 the attendees became the leaders of ai research in the 1960s they and their students produced programs that the press described as astonishing computers were learning checkers strategies solving word problems in algebra proving logical theorems and speaking english artificial intelligence laboratories were set up at a number of british and u s universities in the latter 1950s and early 1960s researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field in 1965 herbert simon predicted machines will be capable within twenty years of doing any work a man can do in 1967 marvin minsky agreed writing that within a generation the problem of creating artificial intelligence will substantially be solved they had however underestimated the difficulty of the problem in 1974 both the u s and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u s congress to fund more productive projects minsky s and papert s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real world tasks thus discrediting the approach altogether the ai winter a period when obtaining funding for ai projects was difficult followed in the early 1980s ai research was revived by the commercial success of expert systems a form of ai program that simulated the knowledge and analytical skills of human experts by 1985 the market for ai had reached over a billion dollars at the same time japan s fifth generation computer project inspired the u s and british governments to restore funding for academic research however beginning with the collapse of the lisp machine market in 1987 ai once again fell into disrepute and a second longer lasting winter began up to this point most of ai s funding had gone to projects that used high level symbols to represent mental objects like plans goals beliefs and known facts in the 1980s some researchers began to doubt that this approach would be able to imitate all the processes of human cognition especially perception robotics learning and pattern recognition and began to look into sub symbolic approaches rodney brooks rejected representation in general and focussed directly on engineering machines that move and survive judea pearl lofti zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic but the most important development was the revival of connectionism including neural network research by geoffrey hinton and others in 1990 yann lecun successfully showed that convolutional neural networks can recognize handwritten digits the first of many successful applications of neural networks ai gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems this narrow and formal focus allowed researchers to produce verifiable results and collaborate with other fields such as statistics economics and mathematics by 2000 solutions developed by ai researchers were being widely used although in the 1990s they were rarely described as artificial intelligence a tendency known as the ai effect however several academic researchers became concerned that ai was no longer pursuing its original goal of creating versatile fully intelligent machines beginning around 2002 they founded the subfield of artificial general intelligence or agi which had several well funded institutions by the 2010s deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field for many specific tasks other methods were abandoned deep learning s success was based on both hardware improvements faster computers graphics processing units cloud computing and access to large amounts of data including curated datasets such as imagenet deep learning s success led to an enormous increase in interest and funding in ai the amount of machine learning research measured by total publications increased by 50 in the years 2015 2019 in 2016 issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences publications vastly increased funding became available and many researchers re focussed their careers on these issues the alignment problem became a serious field of academic study in the late teens and early 2020s agi companies began to deliver programs that created enormous interest in 2015 alphago developed by deepmind beat the world champion go player the program was taught only the rules of the game and developed strategy by itself gpt 3 is a large language model that was released in 2020 by openai and is capable of generating high quality human like text chatgpt launched on november 30 2022 became the fastest growing consumer software application in history gaining over 100 million users in two months it marked what is widely regarded as ai s breakout year bringing it into the public consciousness these programs and others inspired an aggressive ai boom where large companies began investing billions of dollars in ai research according to ai impacts about 50 billion annually was invested in ai around 2022 in the u s alone and about 20 of the new u s computer science phd graduates have specialized in ai about 800 000 ai related u s job openings existed in 2022 philosophy philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines another major focus has been whether machines can be conscious and the associated ethical implications many other topics in philosophy are relevant to ai such as epistemology and free will rapid advancements have intensified public discussions on the philosophy and ethics of ai defining artificial intelligence alan turing wrote in 1950 i propose to consider the question can machines think he advised changing the question from whether a machine thinks to whether or not it is possible for machinery to show intelligent behaviour he devised the turing test which measures the ability of a machine to simulate human conversation since we can only observe the behavior of the machine it does not matter if it is actually thinking or literally has a mind turing notes that we can not determine these things about other people but it is usual to have a polite convention that everyone thinks russell and norvig agree with turing that intelligence must be defined in terms of external behavior not internal structure however they are critical that the test requires the machine to imitate humans aeronautical engineering texts they wrote do not define the goal of their field as making machines that fly so exactly like pigeons that they can fool other pigeons ai founder john mccarthy agreed writing that artificial intelligence is not by definition simulation of human intelligence mccarthy defines intelligence as the computational part of the ability to achieve goals in the world another ai founder marvin minsky similarly describes it as the ability to solve hard problems the leading ai textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals these definitions view intelligence in terms of well defined problems with well defined solutions where both the difficulty of the problem and the performance of the program are direct measures of the intelligence of the machine and no other philosophical discussion is required or may not even be possible another definition has been adopted by google a major practitioner in the field of ai this definition stipulates the ability of systems to synthesize information as the manifestation of intelligence similar to the way it is defined in biological intelligence some authors have suggested in practice that the definition of ai is vague and difficult to define with contention as to whether classical algorithms should be categorised as ai with many companies during the early 2020s ai boom using the term as a marketing buzzword often even if they did not actually use ai in a material way evaluating approaches to ai no established unifying theory or paradigm has guided ai research for most of its history the unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches so much so that some sources especially in the business world use the term artificial intelligence to mean machine learning with neural networks this approach is mostly sub symbolic soft and narrow critics argue that these questions may have to be revisited by future generations of ai researchers symbolic ai and its limits symbolic ai or gofai simulated the high level conscious reasoning that people use when they solve puzzles express legal reasoning and do mathematics they were highly successful at intelligent tasks such as algebra or iq tests in the 1960s newell and simon proposed the physical symbol systems hypothesis a physical symbol system has the necessary and sufficient means of general intelligent action however the symbolic approach failed on many tasks that humans solve easily such as learning recognizing an object or commonsense reasoning moravec s paradox is the discovery that high level intelligent tasks were easy for ai but low level instinctive tasks were extremely difficult philosopher hubert dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a feel for the situation rather than explicit symbolic knowledge although his arguments had been ridiculed and ignored when they were first presented eventually ai research came to agree with him the issue is not resolved sub symbolic reasoning can make many of the same inscrutable mistakes that human intuition does such as algorithmic bias critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence in part because sub symbolic ai is a move away from explainable ai it can be difficult or impossible to understand why a modern statistical ai program made a particular decision the emerging field of neuro symbolic artificial intelligence attempts to bridge the two approaches neat vs scruffy neats hope that intelligent behavior is described using simple elegant principles such as logic optimization or neural networks scruffies expect that it necessarily requires solving a large number of unrelated problems neats defend their programs with theoretical rigor scruffies rely mainly on incremental testing to see if they work this issue was actively discussed in the 1970s and 1980s but eventually was seen as irrelevant modern ai has elements of both soft vs hard computing finding a provably correct or optimal solution is intractable for many important problems soft computing is a set of techniques including genetic algorithms fuzzy logic and neural networks that are tolerant of imprecision uncertainty partial truth and approximation soft computing was introduced in the late 1980s and most successful ai programs in the 21st century are examples of soft computing with neural networks narrow vs general ai ai researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible narrow ai in hopes these solutions will lead indirectly to the field s long term goals general intelligence is difficult to define and difficult to measure and modern ai has had more verifiable successes by focusing on specific problems with specific solutions the sub field of artificial general intelligence studies this area exclusively machine consciousness sentience and mind the philosophy of mind does not know whether a machine can have a mind consciousness and mental states in the same sense that human beings do this issue considers the internal experiences of the machine rather than its external behavior mainstream ai research considers this issue irrelevant because it does not affect the goals of the field to build machines that can solve problems using intelligence russell and norvig add that he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on however the question has become central to the philosophy of mind it is also typically the central question at issue in artificial intelligence in fiction consciousness david chalmers identified two problems in understanding the mind which he named the hard and easy problems of consciousness the easy problem is understanding how the brain processes signals makes plans and controls behavior the hard problem is explaining how this feels or why it should feel like anything at all assuming we are right in thinking that it truly does feel like something dennett s consciousness illusionism says this is an illusion while human information processing is easy to explain human subjective experience is difficult to explain for example it is easy to imagine a color blind person who has learned to identify which objects in their field of view are red but it is not clear what would be required for the person to know what red looks like computationalism and functionalism computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind body problem this philosophical position was inspired by the work of ai researchers and cognitive scientists in the 1960s and was originally proposed by philosophers jerry fodor and hilary putnam philosopher john searle characterized this position as strong ai the appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds searle challenges this claim with his chinese room argument which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind ai welfare and rights it is difficult or impossible to reliably evaluate whether an advanced ai is sentient has the ability to feel and if so to what degree but if there is a significant chance that a given machine can feel and suffer then it may be entitled to certain rights or welfare protection measures similarly to animals sapience a set of capacities related to high intelligence such as discernment or self awareness may provide another moral basis for ai rights robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society in 2017 the european union considered granting electronic personhood to some of the most capable ai systems similarly to the legal status of companies it would have conferred rights but also responsibilities critics argued in 2018 that granting rights to ai systems would downplay the importance of human rights and that legislation should focus on user needs rather than speculative futuristic scenarios they also noted that robots lacked the autonomy to take part to society on their own progress in ai increased interest in the topic proponents of ai welfare and rights often argue that ai sentience if it emerges would be particularly easy to deny they warn that this may be a moral blind spot analogous to slavery or factory farming which could lead to large scale suffering if sentient ai is created and carelessly exploited future superintelligence and the singularity a superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind if research into artificial general intelligence produced sufficiently intelligent software it might be able to reprogram and improve itself the improved software would be even better at improving itself leading to what i j good called an intelligence explosion and vernor vinge called a singularity however technologies cannot improve exponentially indefinitely and typically follow an s shaped curve slowing when they reach the physical limits of what the technology can do transhumanism robot designer hans moravec cyberneticist kevin warwick and inventor ray kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either this idea called transhumanism has roots in the writings of aldous huxley and robert ettinger edward fredkin argues that artificial intelligence is the next step in evolution an idea first proposed by samuel butler s darwin among the machines as far back as 1863 and expanded upon by george dyson in his 1998 book darwin among the machines the evolution of global intelligence in fiction thought capable artificial beings have appeared as storytelling devices since antiquity and have been a persistent theme in science fiction a common trope in these works began with mary shelley s frankenstein where a human creation becomes a threat to its masters this includes such works as arthur c clarke s and stanley kubrick s 2001 a space odyssey both 1968 with hal 9000 the murderous computer in charge of the discovery one spaceship as well as the terminator 1984 and the matrix 1999 in contrast the rare loyal robots such as gort from the day the earth stood still 1951 and bishop from aliens 1986 are less prominent in popular culture isaac asimov introduced the three laws of robotics in many stories most notably with the multivac super intelligent computer asimov s laws are often brought up during lay discussions of machine ethics while almost all artificial intelligence researchers are familiar with asimov s laws through popular culture they generally consider the laws useless for many reasons one of which is their ambiguity several works use ai to force us to confront the fundamental question of what makes us human showing us artificial beings that have the ability to feel and thus to suffer this appears in karel čapek s r u r the films a i artificial intelligence and ex machina as well as the novel do androids dream of electric sheep by philip k dick dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence see also artificial intelligence and elections use and impact of ai on political elections artificial intelligence content detection software to detect ai generated content behavior selection algorithm algorithm that selects actions for intelligent agents business process automation automation of business processes case based reasoning process of solving new problems based on the solutions of similar past problems computational intelligence ability of a computer to learn a specific task from data or experimental observation digital immortality hypothetical concept of storing a personality in digital form emergent algorithm algorithm exhibiting emergent behavior female gendering of ai technologies gender biases in digital technologypages displaying short descriptions of redirect targets glossary of artificial intelligence list of definitions of terms and concepts commonly used in the study of artificial intelligence intelligence amplification use of information technology to augment human intelligence intelligent agent software agent which acts autonomously mind uploading hypothetical process of digitally emulating a brain organoid intelligence use of brain cells and brain organoids for intelligent computing robotic process automation form of business process automation technology wetware computer computer composed of organic material explanatory notes references ai textbooks the two most widely used textbooks in 2023 see the open syllabus russell stuart j norvig peter 2021 artificial intelligence a modern approach 4th ed hoboken pearson isbn 978 0 1346 1099 3 lccn 20190474 rich elaine knight kevin nair shivashankar b 2010 artificial intelligence 3rd ed new delhi tata mcgraw hill india isbn 978 0 0700 8770 5 the four most widely used ai textbooks in 2008 other textbooks ertel wolfgang 2017 introduction to artificial intelligence 2nd ed springer isbn 978 3 3195 8486 7 ciaramella alberto ciaramella marco 2024 introduction to artificial intelligence from data analysis to generative ai 1st ed intellisemantic editions isbn 978 8 8947 8760 3 history of ai other sources further reading external links artificial intelligence internet encyclopedia of philosophy ",
            "total_words": 13168,
            "unique_words_percentage": 24.027946537059538,
            "stopwords_percentage": 37.91008505467801
        },
        {
            "title": "Artificial general intelligence",
            "link": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
            "content": "artificial general intelligence agi is a type of artificial intelligence ai that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks this contrasts with narrow ai which is limited to specific tasks artificial superintelligence asi on the other hand refers to agi that greatly exceeds human cognitive capabilities agi is considered one of the definitions of strong ai creating agi is a primary goal of ai research and of companies such as openai and meta a 2020 survey identified 72 active agi research and development projects across 37 countries the timeline for achieving agi remains a subject of ongoing debate among researchers and experts as of 2023 some argue that it may be possible in years or decades others maintain it might take a century or longer a minority believe it may never be achieved and another minority claims that it is already here notable ai researcher geoffrey hinton has expressed concerns about the rapid progress towards agi suggesting it could be achieved sooner than many expect there is debate on the exact definition of agi and regarding whether modern large language models llms such as gpt 4 are early forms of agi agi is a common topic in science fiction and futures studies contention exists over whether agi represents an existential risk many experts on ai have stated that mitigating the risk of human extinction posed by agi should be a global priority others find the development of agi to be too remote to present such a risk terminology agi is also known as strong ai full ai human level ai human level intelligent ai or general intelligent action some academic sources reserve the term strong ai for computer programs that experience sentience or consciousness in contrast weak ai or narrow ai is able to solve one specific problem but lacks general cognitive abilities some academic sources use weak ai to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans related concepts include artificial superintelligence and transformative ai an artificial superintelligence asi is a hypothetical type of agi that is much more generally intelligent than humans while the notion of transformative ai relates to ai having a large impact on society for example similar to the agricultural or industrial revolution a framework for classifying agi in levels was proposed in 2023 by google deepmind researchers they define five levels of agi emerging competent expert virtuoso and superhuman for example a competent agi is defined as an ai that outperforms 50 of skilled adults in a wide range of non physical tasks and a superhuman agi i e an artificial superintelligence is similarly defined but with a threshold of 100 they consider large language models like chatgpt or llama 2 to be instances of emerging agi characteristics various popular definitions of intelligence have been proposed one of the leading proposals is the turing test however there are other well known definitions and some researchers disagree with the more popular approaches intelligence traits however researchers generally hold that intelligence is required to do all of the following reason use strategy solve puzzles and make judgments under uncertainty represent knowledge including common sense knowledge plan learn communicate in natural language if necessary integrate these skills in completion of any given goal many interdisciplinary approaches e g cognitive science computational intelligence and decision making consider additional traits such as imagination the ability to form novel mental images and concepts and autonomy computer based systems that exhibit many of these capabilities exist e g see computational creativity automated reasoning decision support system robot evolutionary computation intelligent agent there is debate about whether modern ai systems possess them to an adequate degree physical traits other capabilities are considered desirable in intelligent systems as they may affect intelligence or aid in its expression these include the ability to sense e g see hear etc and the ability to act e g move and manipulate objects change location to explore etc this includes the ability to detect and respond to hazard although the ability to sense e g see hear etc and the ability to act e g move and manipulate objects change location to explore etc can be desirable for some intelligent systems these physical capabilities are not strictly required for an entity to qualify as agi particularly under the thesis that large language models llms may already be or become agi even from a less optimistic perspective on llms there is no firm requirement for an agi to have a human like form being a silicon based computational system is sufficient provided it can process input language from the external world in place of human senses this interpretation aligns with the understanding that agi has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional eyes and ears tests for human level agi several tests meant to confirm human level agi have been considered including the turing test turing proposed by alan turing in his 1950 paper computing machinery and intelligence this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human like responses the machine passes the test if it can convince the judge it is human a significant fraction of the time turing proposed this as a practical measure of machine intelligence focusing on the ability to produce human like responses rather than on the internal workings of the machine turing described the test as follows the idea of the test is that the machine has to try and pretend to be a man by answering questions put to it and it will only pass if the pretence is reasonably convincing a considerable portion of a jury who should not be expert about machines must be taken in by the pretence in 2014 a chatbot named eugene goostman designed to imitate a 13 year old ukrainian boy reportedly passed a turing test event by convincing 33 of judges that it was human however this claim was met with significant skepticism from the ai research community who questioned the test s implementation and its relevance to agi more recently a 2024 study suggested that gpt 4 was identified as human 54 of the time in a randomized controlled version of the turing test surpassing older chatbots like eliza while still falling behind actual humans 67 the robot college student test goertzel a machine enrolls in a university taking and passing the same classes that humans would and obtaining a degree llms can now pass university degree level exams without even attending the classes the employment test nilsson a machine performs an economically important job at least as well as humans in the same job ais are now replacing humans in many roles as varied as fast food and marketing the ikea test marcus also known as the flat pack furniture test an ai views the parts and instructions of an ikea flat pack product then controls a robot to assemble the furniture correctly the coffee test wozniak a machine is required to enter an average american home and figure out how to make coffee find the coffee machine find the coffee add water find a mug and brew the coffee by pushing the proper buttons this has not yet been completed the modern turing test suleyman an ai model is given 100 000 and has to obtain 1 million ai complete problems a problem is informally called ai complete or ai hard if it is believed that in order to solve it one would need to implement agi because the solution is beyond the capabilities of a purpose specific algorithm there are many problems that have been conjectured to require general intelligence to solve as well as humans examples include computer vision natural language understanding and dealing with unexpected circumstances while solving any real world problem even a specific task like translation requires a machine to read and write in both languages follow the author s argument reason understand the context knowledge and faithfully reproduce the author s original intent social intelligence all of these problems need to be solved simultaneously in order to reach human level machine performance however many of these tasks can now be performed by modern large language models according to stanford university s 2024 ai index ai has reached human level performance on many benchmarks for reading comprehension and visual reasoning history classical ai modern ai research began in the mid 1950s the first generation of ai researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades ai pioneer herbert a simon wrote in 1965 machines will be capable within twenty years of doing any work a man can do their predictions were the inspiration for stanley kubrick and arthur c clarke s character hal 9000 who embodied what ai researchers believed they could create by the year 2001 ai pioneer marvin minsky was a consultant on the project of making hal 9000 as realistic as possible according to the consensus predictions of the time he said in 1967 within a generation the problem of creating artificial intelligence will substantially be solved several classical ai projects such as doug lenat s cyc project that began in 1984 and allen newell s soar project were directed at agi however in the early 1970s it became obvious that researchers had grossly underestimated the difficulty of the project funding agencies became skeptical of agi and put researchers under increasing pressure to produce useful applied ai in the early 1980s japan s fifth generation computer project revived interest in agi setting out a ten year timeline that included agi goals like carry on a casual conversation in response to this and the success of expert systems both industry and government pumped money into the field however confidence in ai spectacularly collapsed in the late 1980s and the goals of the fifth generation computer project were never fulfilled for the second time in 20 years ai researchers who predicted the imminent achievement of agi had been mistaken by the 1990s ai researchers had a reputation for making vain promises they became reluctant to make predictions at all and avoided mention of human level artificial intelligence for fear of being labeled wild eyed dreamer narrow ai research in the 1990s and early 21st century mainstream ai achieved commercial success and academic respectability by focusing on specific sub problems where ai can produce verifiable results and commercial applications such as speech recognition and recommendation algorithms these applied ai systems are now used extensively throughout the technology industry and research in this vein is heavily funded in both academia and industry as of 2018 development in this field was considered an emerging trend and a mature stage was expected to be reached in more than 10 years at the turn of the century many mainstream ai researchers hoped that strong ai could be developed by combining programs that solve various sub problems hans moravec wrote in 1988 i am confident that this bottom up route to artificial intelligence will one day meet the traditional top down route more than half way ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts however even at the time this was disputed for example stevan harnad of princeton university concluded his 1990 paper on the symbol grounding hypothesis by stating the expectation has often been voiced that top down symbolic approaches to modeling cognition will somehow meet bottom up sensory approaches somewhere in between if the grounding considerations in this paper are valid then this expectation is hopelessly modular and there is really only one viable route from sense to symbols from the ground up a free floating symbolic level like the software level of a computer will never be reached by this route or vice versa nor is it clear why we should even try to reach such a level since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings thereby merely reducing ourselves to the functional equivalent of a programmable computer modern artificial general intelligence research the term artificial general intelligence was used as early as 1997 by mark gubrud in a discussion of the implications of fully automated military production and operations a mathematical formalism of agi was proposed by marcus hutter in 2000 named aixi the proposed agi agent maximises the ability to satisfy goals in a wide range of environments this type of agi characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human like behaviour was also called universal artificial intelligence the term agi was re introduced and popularized by shane legg and ben goertzel around 2002 agi research activity in 2006 was described by pei wang and ben goertzel as producing publications and preliminary results the first summer school in agi was organized in xiamen china in 2009 by the xiamen university s artificial brain laboratory and opencog the first university course was given in 2010 and 2011 at plovdiv university bulgaria by todor arnaudov mit presented a course on agi in 2018 organized by lex fridman and featuring a number of guest lecturers as of 2023 a small number of computer scientists are active in agi research and many contribute to a series of agi conferences however increasingly more researchers are interested in open ended learning which is the idea of allowing ai to continuously learn and innovate like humans do feasibility as of 2023 the development and potential achievement of agi remains a subject of intense debate within the ai community while traditional consensus held that agi was a distant goal recent advancements have led some researchers and industry figures to claim that early forms of agi may already exist ai pioneer herbert a simon speculated in 1965 that machines will be capable within twenty years of doing any work a man can do this prediction failed to come true microsoft co founder paul allen believed that such intelligence is unlikely in the 21st century because it would require unforeseeable and fundamentally unpredictable breakthroughs and a scientifically deep understanding of cognition writing in the guardian roboticist alan winfield claimed the gulf between modern computing and human level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight a further challenge is the lack of clarity in defining what intelligence entails does it require consciousness must it display the ability to set goals as well as pursue them is it purely a matter of scale such that if model sizes increase sufficiently intelligence will emerge are facilities such as planning reasoning and causal understanding required does intelligence require explicitly replicating the brain and its specific faculties does it require emotions most ai researchers believe strong ai can be achieved in the future but some thinkers like hubert dreyfus and roger penrose deny the possibility of achieving strong ai john mccarthy is among those who believe human level ai will be accomplished but that the present level of progress is such that a date cannot accurately be predicted ai experts views on the feasibility of agi wax and wane four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50 confident agi would arrive was 2040 to 2050 depending on the poll with the mean being 2081 of the experts 16 5 answered with never when asked the same question but with a 90 confidence instead further current agi progress considerations can be found above tests for confirming human level agi a report by stuart armstrong and kaj sotala of the machine intelligence research institute found that over 60 year time frame there is a strong bias towards predicting the arrival of human level ai as between 15 and 25 years from the time the prediction was made they analyzed 95 predictions made between 1950 and 2012 on when human level ai will come about in 2023 microsoft researchers published a detailed evaluation of gpt 4 they concluded given the breadth and depth of gpt 4 s capabilities we believe that it could reasonably be viewed as an early yet still incomplete version of an artificial general intelligence agi system another study in 2023 reported that gpt 4 outperforms 99 of humans on the torrance tests of creative thinking blaise agüera y arcas and peter norvig wrote in 2023 that a significant level of general intelligence has already been achieved with frontier models they wrote that reluctance to this view comes from four main reasons a healthy skepticism about metrics for agi an ideological commitment to alternative ai theories or techniques a devotion to human or biological exceptionalism or a concern about the economic implications of agi 2023 also marked the emergence of large multimodal models large language models capable of processing or generating multiple modalities such as text audio and images in 2024 openai released o1 preview the first of a series of models that spend more time thinking before they respond according to mira murati this ability to think before responding represents a new additional paradigm it improves model outputs by spending more computing power when generating the answer whereas the model scaling paradigm improves outputs by increasing the model size training data and training compute power an openai employee vahid kazemi claimed in 2024 that the company had achieved agi stating in my opinion we have already achieved agi and it s even more clear with o1 kazemi clarified that while the ai is not yet better than any human at any task it is better than most humans at most tasks he also addressed criticisms that large language models llms merely follow predefined patterns comparing their learning process to the scientific method of observing hypothesizing and verifying these statements have sparked debate as they rely on a broad and unconventional definition of agi traditionally understood as ai that matches human intelligence across all domains critics argue that while openai s models demonstrate remarkable versatility they may not fully meet this standard notably kazemi s comments came shortly after openai removed agi from the terms of its partnership with microsoft prompting speculation about the company s strategic intentions timescales progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop ending each hiatus were fundamental advances in hardware software or both to create space for further progress for example the computer hardware available in the twentieth century was not sufficient to implement deep learning which requires large numbers of gpu enabled cpus in the introduction to his 2006 book goertzel says that estimates of the time needed before a truly flexible agi is built vary from 10 years to over a century as of 2007 the consensus in the agi research community seemed to be that the timeline discussed by ray kurzweil in 2005 in the singularity is near i e between 2015 and 2045 was plausible mainstream ai researchers have given a wide range of opinions on whether progress will be this rapid a 2012 meta analysis of 95 such opinions found a bias towards predicting that the onset of agi would occur within 16 26 years for modern and historical predictions alike that paper has been criticized for how it categorized opinions as expert or non expert in 2012 alex krizhevsky ilya sutskever and geoffrey hinton developed a neural network called alexnet which won the imagenet competition with a top 5 test error rate of 15 3 significantly better than the second best entry s rate of 26 3 the traditional approach used a weighted sum of scores from different pre defined classifiers alexnet was regarded as the initial ground breaker of the current deep learning wave in 2017 researchers feng liu yong shi and ying liu conducted intelligence tests on publicly available and freely accessible weak ai such as google ai apple s siri and others at the maximum these ais reached an iq value of about 47 which corresponds approximately to a six year old child in first grade an adult comes to about 100 on average similar tests were carried out in 2014 with the iq score reaching a maximum value of 27 in 2020 openai developed gpt 3 a language model capable of performing many diverse tasks without specific training according to gary grossman in a venturebeat article while there is consensus that gpt 3 is not an example of agi it is considered by some to be too advanced to be classified as a narrow ai system in the same year jason rohrer used his gpt 3 account to develop a chatbot and provided a chatbot developing platform called project december openai asked for changes to the chatbot to comply with their safety guidelines rohrer disconnected project december from the gpt 3 api in 2022 deepmind developed gato a general purpose system capable of performing more than 600 different tasks in 2023 microsoft research published a study on an early version of openai s gpt 4 contending that it exhibited more general intelligence than previous ai models and demonstrated human level performance in tasks spanning multiple domains such as mathematics coding and law this research sparked a debate on whether gpt 4 could be considered an early incomplete version of artificial general intelligence emphasizing the need for further exploration and evaluation of such systems in 2023 the ai researcher geoffrey hinton stated that the idea that this stuff could actually get smarter than people a few people believed that but most people thought it was way off and i thought it was way off i thought it was 30 to 50 years or even longer away obviously i no longer think that in may 2023 demis hassabis similarly said that the progress in the last few years has been pretty incredible and that he sees no reason why it would slow down expecting agi within a decade or even a few years in march 2024 nvidia s ceo jensen huang stated his expectation that within five years ai would be capable of passing any test at least as well as humans in june 2024 the ai researcher leopold aschenbrenner a former openai employee estimated agi by 2027 to be strikingly plausible whole brain emulation while the development of transformer models like in chatgpt is considered the most promising path to agi whole brain emulation can serve as an alternative approach with whole brain simulation a brain model is built by scanning and mapping a biological brain in detail and then copying and simulating it on a computer system or another computational device the simulation model must be sufficiently faithful to the original so that it behaves in practically the same way as the original brain whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics and for medical research purposes it has been discussed in artificial intelligence research as an approach to strong ai neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly and futurist ray kurzweil in the book the singularity is near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it early estimates for low level brain simulation a very powerful cluster of computers or gpus would be required given the enormous quantity of synapses within the human brain each of the 1011 one hundred billion neurons has on average 7 000 synaptic connections synapses to other neurons the brain of a three year old child has about 1015 synapses 1 quadrillion this number declines with age stabilizing by adulthood estimates vary for an adult ranging from 1014 to 5 1014 synapses 100 to 500 trillion an estimate of the brain s processing power based on a simple switch model for neuron activity is around 1014 100 trillion synaptic updates per second sups in 1997 kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second cps for comparison if a computation was equivalent to one floating point operation a measure used to rate current supercomputers then 1016 computations would be equivalent to 10 petaflops achieved in 2011 while 1018 was achieved in 2022 he used this figure to predict the necessary hardware would be available sometime between 2015 and 2025 if the exponential growth in computer power at the time of writing continued current research the human brain project an eu funded initiative active from 2013 to 2023 has developed a particularly detailed and publicly accessible atlas of the human brain in 2023 researchers from duke university performed a high resolution scan of a mouse brain a supercomputer with similar computing capability as the human brain is expected in april 2024 called deepsouth it could perform 228 trillions of synaptic operations per second criticisms of simulation based approaches the artificial neuron model assumed by kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons a brain simulation would likely have to capture the detailed cellular behaviour of biological neurons presently understood only in broad outline the overhead introduced by full modeling of the biological chemical and physical details of neural behaviour especially on a molecular scale would require computational powers several orders of magnitude larger than kurzweil s estimate in addition the estimates do not account for glial cells which are known to play a role in cognitive processes a fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning if this theory is correct any fully functional brain model will need to encompass more than just the neurons e g a robotic body goertzel proposes virtual embodiment like in metaverses like second life as an option but it is unknown whether this would be sufficient philosophical perspective strong ai as defined in philosophy in 1980 philosopher john searle coined the term strong ai as part of his chinese room argument he proposed a distinction between two hypotheses about artificial intelligence strong ai hypothesis an artificial intelligence system can have a mind and consciousness weak ai hypothesis an artificial intelligence system can only act like it thinks and has a mind and consciousness the first one he called strong because it makes a stronger statement it assumes something special has happened to the machine that goes beyond those abilities that we can test the behaviour of a weak ai machine would be precisely identical to a strong ai machine but the latter would also have subjective conscious experience this usage is also common in academic ai research and textbooks in contrast to searle and mainstream ai some futurists such as ray kurzweil use the term strong ai to mean human level artificial general intelligence this is not the same as searle s strong ai unless it is assumed that consciousness is necessary for human level agi academic philosophers such as searle do not believe that is the case and to most artificial intelligence researchers the question is out of scope mainstream ai is most interested in how a program behaves according to russell and norvig as long as the program works they don t care if you call it real or a simulation if the program can behave as if it has a mind then there is no need to know if it actually has mind indeed there would be no way to tell for ai research searle s weak ai hypothesis is equivalent to the statement artificial general intelligence is possible thus according to russell and norvig most ai researchers take the weak ai hypothesis for granted and don t care about the strong ai hypothesis thus for academic ai research strong ai and agi are two different things consciousness consciousness can have various meanings and some aspects play significant roles in science fiction and the ethics of artificial intelligence sentience or phenomenal consciousness the ability to feel perceptions or emotions subjectively as opposed to the ability to reason about perceptions some philosophers such as david chalmers use the term consciousness to refer exclusively to phenomenal consciousness which is roughly equivalent to sentience determining why and how subjective experience arises is known as the hard problem of consciousness thomas nagel explained in 1974 that it feels like something to be conscious if we are not conscious then it doesn t feel like anything nagel uses the example of a bat we can sensibly ask what does it feel like to be a bat however we are unlikely to ask what does it feel like to be a toaster nagel concludes that a bat appears to be conscious i e has consciousness but a toaster does not in 2022 a google engineer claimed that the company s ai chatbot lamda had achieved sentience though this claim was widely disputed by other experts self awareness to have conscious awareness of oneself as a separate individual especially to be consciously aware of one s own thoughts this is opposed to simply being the subject of one s thought an operating system or debugger is able to be aware of itself that is to represent itself in the same way it represents everything else but this is not what people typically mean when they use the term self awareness these traits have a moral dimension ai sentience would give rise to concerns of welfare and legal protection similarly to animals other aspects of consciousness related to cognitive capabilities are also relevant to the concept of ai rights figuring out how to integrate advanced ai with existing legal and social frameworks is an emergent issue benefits agi could have a wide variety of applications if oriented towards such goals agi could help mitigate various problems in the world such as hunger poverty and health problems agi could improve productivity and efficiency in most jobs for example in public health agi could accelerate medical research notably against cancer it could take care of the elderly and democratize access to rapid high quality medical diagnostics it could offer fun cheap and personalized education the need to work to subsist could become obsolete if the wealth produced is properly redistributed this also raises the question of the place of humans in a radically automated society agi could also help to make rational decisions and to anticipate and prevent disasters it could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering while avoiding the associated risks if an agi s primary goal is to prevent existential catastrophes such as human extinction which could be difficult if the vulnerable world hypothesis turns out to be true it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life risks existential risks agi may represent multiple types of existential risk which are risks that threaten the premature extinction of earth originating intelligent life or the permanent and drastic destruction of its potential for desirable future development the risk of human extinction from agi has been the topic of many debates but there is also the possibility that the development of agi would lead to a permanently flawed future notably it could be used to spread and preserve the set of values of whoever develops it if humanity still has moral blind spots similar to slavery in the past agi might irreversibly entrench it preventing moral progress furthermore agi could facilitate mass surveillance and indoctrination which could be used to create a stable repressive worldwide totalitarian regime there is also a risk for the machines themselves if machines that are sentient or otherwise worthy of moral consideration are mass created in the future engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe considering how much agi could improve humanity s future and help reduce other existential risks toby ord calls these existential risks an argument for proceeding with due caution not for abandoning ai risk of loss of control and human extinction the thesis that ai poses an existential risk for humans and that this risk needs more attention is controversial but has been endorsed in 2023 by many public figures ai researchers and ceos of ai companies such as elon musk bill gates geoffrey hinton yoshua bengio demis hassabis and sam altman in 2014 stephen hawking criticized widespread indifference so facing possible futures of incalculable benefits and risks the experts are surely doing everything possible to ensure the best outcome right wrong if a superior alien civilisation sent us a message saying we ll arrive in a few decades would we just reply ok call us when you get here we ll leave the lights on probably not but this is more or less what is happening with ai the potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities the comparison states that greater intelligence allowed humanity to dominate gorillas which are now vulnerable in ways that they could not have anticipated as a result the gorilla has become an endangered species not out of malice but simply as a collateral damage from human activities the skeptic yann lecun considers that agis will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans he said that people won t be smart enough to design super intelligent machines yet ridiculously stupid to the point of giving it moronic objectives with no safeguards on the other side the concept of instrumental convergence suggests that almost whatever their goals intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals and that this does not require having emotions many scholars who are concerned about existential risk advocate for more research into solving the control problem to answer the question what types of safeguards algorithms or architectures can programmers implement to maximise the probability that their recursively improving ai would continue to behave in a friendly rather than destructive manner after it reaches superintelligence solving the control problem is complicated by the ai arms race which could lead to a race to the bottom of safety precautions in order to release products before competitors and the use of ai in weapon systems the thesis that ai can pose existential risk also has detractors skeptics usually say that agi is unlikely in the short term or that concerns about agi distract from other issues related to current ai former google fraud czar shuman ghosemajumder considers that for many people outside of the technology industry existing chatbots and llms are already perceived as though they were agi leading to further misunderstanding and fear skeptics sometimes charge that the thesis is crypto religious with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent god some researchers believe that the communication campaigns on ai existential risk by certain ai groups such as openai anthropic deepmind and conjecture may be an at attempt at regulatory capture and to inflate interest in their products in 2023 the ceos of google deepmind openai and anthropic along with other industry leaders and researchers issued a joint statement asserting that mitigating the risk of extinction from ai should be a global priority alongside other societal scale risks such as pandemics and nuclear war mass unemployment researchers from openai estimated that 80 of the u s workforce could have at least 10 of their work tasks affected by the introduction of llms while around 19 of workers may see at least 50 of their tasks impacted they consider office workers to be the most exposed for example mathematicians accountants or web designers agi could have a better autonomy ability to make decisions to interface with other computer tools but also to control robotized bodies according to stephen hawking the outcome of automation on the quality of life will depend on how the wealth will be redistributed everyone can enjoy a life of luxurious leisure if the machine produced wealth is shared or most people can end up miserably poor if the machine owners successfully lobby against wealth redistribution so far the trend seems to be toward the second option with technology driving ever increasing inequalityelon musk considers that the automation of society will require governments to adopt a universal basic income see also notes references sources further reading external links the agi portal maintained by pei wang",
            "total_words": 6174,
            "unique_words_percentage": 29.527048914804016,
            "stopwords_percentage": 39.82831227729187
        },
        {
            "title": "A.I. Artificial Intelligence",
            "link": "https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence",
            "content": "a i artificial intelligence or simply a i is a 2001 american science fiction film directed by steven spielberg the screenplay by spielberg and screen story by ian watson are loosely based on the 1969 short story supertoys last all summer long by brian aldiss set in a futuristic society the film stars haley joel osment as david a childlike android uniquely programmed with the ability to love jude law frances o connor brendan gleeson and william hurt star in supporting roles development of a i originally began after producer and director stanley kubrick acquired the rights to aldiss s story in the early 1970s kubrick hired a series of writers including aldiss bob shaw ian watson and sara maitland until the mid 1990s the film languished in development hell for years partly because kubrick felt that computer generated imagery was not advanced enough to create the david character which he believed no child actor would convincingly portray in 1995 kubrick handed a i to spielberg but the film did not gain momentum until kubrick died in 1999 spielberg remained close to watson s treatment for the screenplay and dedicated the film to kubrick a i artificial intelligence was released on june 29 2001 by warner bros pictures in north america it received generally positive reviews from critics and grossed 235 9 million against a budget of 90 100 million it was also nominated for best visual effects and best original score for john williams at the 74th academy awards in a 2016 bbc poll of 177 critics around the world a i artificial intelligence was voted the eighty third greatest film since 2000 it has since been called one of spielberg s best works and one of the greatest films of the 21st century and of all time plot in the 22nd century rising sea levels from global warming have wiped out coastal cities and altered the world s climate with the human population in decline advanced nations have created mecha to fulfill various roles in society in madison new jersey david an 11 year old prototype mecha child capable of experiencing love is given to henry swinton and his wife monica whose son martin is in suspended animation after contracting a rare disease initially uncomfortable with david monica eventually warms to him and activates his imprinting protocol wanting her to love him in return he befriends teddy martin s old robotic teddy bear after martin is unexpectedly cured of his disease and brought home he jealously goads david into cutting off a piece of monica s hair that night david enters his adoptive parents room but as monica turns over the scissors accidentally poke her in the eye while henry attends to her wounds teddy picks up the lock of hair from the floor and places it in his pocket during a pool party one of martin s friends pokes david with a knife triggering his self protection programming david grabs martin causing both of them to fall into the pool while martin is rescued david is accused of endangering others henry convinces monica to return david to his creators for destruction en route she instead spares david by abandoning him in the woods full of scrap metal and obsolete mecha now accompanied solely by teddy david recalls the adventures of pinocchio and decides to find the blue fairy to become human which he believes will regain monica s love david and teddy are captured by the flesh fair a traveling circus like event at which obsolete mecha are destroyed in front of jeering crowds about to be destroyed himself david pleads for his life and the audience revolts and allows david to escape with gigolo joe a prostitute mecha on the run after being framed for murder david teddy and joe go to the decadent resort town of rouge city where dr know a holographic answer engine directs them to the top of rockefeller center in the flooded ruins of new york city and provides fairy tale information that david interprets as suggesting that a blue fairy can help him above the ruins of new york david meets professor hobby his creator who tells him that their meeting demonstrates david s ability to love and desire david finds copies of himself including female variants called darlene ready to be shipped disheartened by his lost sense of individuality david attempts suicide by falling from a skyscraper into the ocean while underwater david notices a figure resembling the blue fairy before joe rescues him in an amphibious aircraft before david can explain authorities capture joe with an electromagnet david and teddy take control of the aircraft to see the blue fairy which turns out to be a statue from an attraction on coney island the two become trapped when the wonder wheel falls on their vehicle believing that the blue fairy is real david repeatedly asks the statue to turn him into a real boy until his power source is depleted two thousand years later humanity is extinct and manhattan is buried under glacial ice mecha have evolved into an advanced form and a group known as the specialists interested in humanity find and resurrect david and teddy they reconstruct the swinton family home from david s memories before explaining via an interactive version of the blue fairy that he cannot become human however they recreate monica through genetic material from the strand of hair that teddy kept this version of monica can live for only one day and cannot be revived david spends his happiest day with monica and as she falls asleep in the evening monica tells david that she has always loved him david lies down next to her and closes his eyes cast production development stanley kubrick began development on an adaptation of super toys last all summer long in the late 1970s hiring the story s author brian aldiss to write a film treatment in 1985 kubrick asked steven spielberg to direct the film with kubrick producing warner bros agreed to co finance a i and cover distribution duties the film labored in development hell and aldiss was fired by kubrick over creative differences in 1989 bob shaw briefly served as writer leaving after six weeks due to kubrick s demanding work schedule and ian watson was hired as the new writer in march 1990 aldiss later remarked not only did the bastard fire me he hired my enemy instead kubrick handed watson carlo collodi s the adventures of pinocchio for inspiration calling a i a picaresque robot version of pinocchio three weeks later watson gave kubrick his first story treatment and concluded his work on a i in may 1991 with another treatment of 90 pages gigolo joe was originally conceived as a g i mecha but watson suggested changing him to a male prostitute kubrick joked i guess we lost the kiddie market meanwhile kubrick dropped a i to work on a film adaptation of wartime lies feeling computer animation was not advanced enough to create the david character after the release of spielberg s jurassic park with its innovative cgi it was announced in november 1993 that production of a i would begin in 1994 dennis muren and ned gorman who worked on jurassic park became visual effects supervisors but kubrick was displeased with their previsualization and with the expense of hiring industrial light magic ilm and stan winston studio kubrick asked sara maitland to give the film mythic resonance she recalls he never referred to the film as a i he always called it pinocchio kubrick s version ended the same way spielberg s does with advanced mechas reviving monica but only for a day pre production in early 1994 the film was in pre production with christopher fangorn baker as concept artist and sara maitland assisting on the story which gave it a feminist fairy tale focus maitland said that kubrick never referred to the film as a i but as pinocchio chris cunningham became the new visual effects supervisor some of his unproduced work for a i can be seen on the dvd the work of director chris cunningham aside from considering computer animation kubrick also had joseph mazzello do a screen test for the lead role cunningham helped assemble a series of little robot type humans for the david character we tried to construct a little boy with a movable rubber face to see whether we could make it look appealing producer jan harlan reflected but it was a total failure it looked awful hans moravec was brought in as a technical consultant meanwhile kubrick and harlan thought that a i would be closer to steven spielberg s sensibilities as director kubrick handed the position to spielberg in 1995 but spielberg chose to direct other projects and convinced kubrick to remain as director the film was put on hold due to kubrick s commitment to eyes wide shut 1999 after kubrick s death in march 1999 harlan and christiane kubrick approached spielberg to take over the director s position by november 1999 spielberg was writing the screenplay based on watson s 90 page story treatment it was his first solo screenplay credit since close encounters of the third kind 1977 pre production was briefly halted during february 2000 because spielberg pondered directing other projects which were harry potter and the philosopher s stone minority report and memoirs of a geisha the following month spielberg announced that a i would be his next project with minority report as a follow up when he decided to fast track a i spielberg brought back chris baker as concept artist ian watson reported that the final script was very faithful to kubrick s vision even the ending which is often attributed to spielberg saying the final 20 minutes are pretty close to what i wrote for stanley and what stanley wanted faithfully filmed by spielberg without added schmaltz filming and visual effects the original start date was july 10 2000 but filming was delayed until august aside from a couple of weeks of shooting on location in oxbow regional park in oregon a i was shot entirely using sound stages at warner bros studios and the spruce goose dome in long beach california spielberg copied kubrick s obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew banning press from the set and making actors sign confidentiality agreements for instance jack angel who voiced teddy recorded his lines entirely out of context only receiving direction to sound like eeyore from winnie the pooh except very wise and old and stoic however spielberg asked angel to be on the set every day to make line alterations wherever he felt necessary social robotics expert cynthia breazeal served as technical consultant during production costume designer bob ringwood studied pedestrians on the las vegas strip for his influence on the rouge city extras visual effects such as removing the visible rods controlling teddy and removing haley joel osment s breath were provided in houses by pdi dreamworks casting julianne moore and gwyneth paltrow were considered for the role of monica swinton before frances o connor was cast jerry seinfeld was originally considered to voice and play the comedian robot before chris rock was cast allusions a o scott notes spielberg s homages to kubrick sly references to a clockwork orange the shining and predominantly 2001 a space odyssey as well as collodi s pinocchio the lines dr know quotes are from w b yeats s the stolen child soundtrack the film s soundtrack album was released by warner sunset records in 2001 the original score was composed and conducted by john williams and features singers lara fabian on two songs and josh groban on one the film s score also had a limited release as an official for your consideration academy promo as well as a complete score issued by la la land records in 2015 the band ministry appears in the film playing the song what about us but the song does not appear on the official soundtrack album williams called his score an homage a kubrick he includes echoes of gyorgy ligeti s choral music which kubrick used in 2001 a space odyssey per kubrick s request williams included a quotation of richard strauss s der rosenkavalier in his score release marketing the teaser trailer debuted on december 8 2000 with the theatrical release of proof of life warner bros used an alternate reality game titled the beast to promote the film over forty websites were created by atomic pictures in new york city kept online at cloudmakers org including the website for cybertronics corp there were to be a series of video games for the xbox video game console that followed the storyline of the beast but they went undeveloped to avoid audiences mistaking a i for a family film no action figures were created although hasbro released a talking teddy following the film s release in june 2001 a i premiered at the venice film festival in 2001 home media a i artificial intelligence was released on vhs and dvd in the united states by dreamworks home entertainment and touchstone home video on march 5 2002 in widescreen and fullscreen two disc special editions featuring an extensive sixteen part documentary detailing the film s development production visual effects sound design and music the bonuses also include interviews with haley joel osment jude law frances o connor steven spielberg and john williams two teaser trailers for the film s original theatrical release and an extensive photo gallery featuring production stills and stanley kubrick s original storyboards it was released overseas by warner home video the film was released on blu ray in japan by warner home video on december 22 2010 followed shortly by a united states release by paramount home entertainment and touchstone home entertainment owners of the pre 2010 dreamworks catalog on april 5 2011 this blu ray features the film remastered in high definition and incorporates all the bonus features previously included on the two disc special edition dvd reception box office the film opened in 3 242 theaters in the united states and canada on june 29 2001 earning 29 35 million at 1 during its opening weekend a i went on to gross 78 62 million in the united states and canada opening on 524 screens in japan a i grossed almost two billion yen in its first five days the biggest june opening in japan at the time and sold more tickets in its opening weekend than star wars episode i the phantom menace although it grossed slightly less it went on to gross 78 million in japan it grossed 79 million in other countries for a worldwide total of 235 93 million critical response on rotten tomatoes a i artificial intelligence holds an approval rating of 76 based on reviews from 203 critics with an average rating of 6 60 10 the website s critical consensus reads a curious not always seamless amalgamation of kubrick s chilly bleakness and spielberg s warm hearted optimism a i is in a word fascinating on metacritic it has a weighted average score of 65 out of 100 based on reviews from 32 critics which indicates generally favorable reviews audiences surveyed by cinemascore gave the film an average grade of c on a scale of a to f producer jan harlan stated that kubrick would have applauded the final film while kubrick s widow christiane also enjoyed a i brian aldiss admired the film as well i thought what an inventive intriguing ingenious involving film this was there are flaws in it and i suppose i might have a personal quibble but it s so long since i wrote it of the film s ending he wondered how it might have been had kubrick directed the film that is one of the ifs of film history at least the ending indicates spielberg adding some sugar to kubrick s wine the actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence but it s a brilliant piece of film and of course it s a phenomenon because it contains the energies and talents of two brilliant filmmakers a o scott writes mr spielberg seems to be attempting the improbable feat of melding kubrick s chilly analytical style with his own warmer needier sensibility he tells the story slowly and films it with lucid mesmerizing objectivity creating a mood as layered dissonant and strange as john williams s unusually restrained modernist score he concludes the very end somehow fuses the cathartic comfort of infantile wish fulfillment the dream that the first perfect love whose loss we experience as the fall from eden might be restored with a feeling almost too terrible to acknowledge or to name refusing to cuddle us or lull us into easy sleep mr spielberg locates the unspoken moral of all our fairy tales to be real is to be mortal to be human is to love to dream and to perish richard corliss of time magazine heavily praised spielberg s direction as well as the cast and visual effects roger ebert of the chicago sun times gave the film three stars out of a possible four saying that it is wonderful and maddening ebert later gave the film a full four stars and added it to his great movies canon in 2011 leonard maltin on the other hand gives the film two stars out of four in his movie guide writing intriguing story draws us in thanks in part to osment s exceptional performance but takes several wrong turns ultimately it just doesn t work spielberg rewrote the adaptation stanley kubrick commissioned of the brian aldiss short story super toys last all summer long result is a curious and uncomfortable hybrid of kubrick and spielberg sensibilities however maltin called john williams s music score striking jonathan rosenbaum of the chicago reader compared a i to solaris 1972 and praised both kubrick for proposing that spielberg direct the project and spielberg for doing his utmost to respect kubrick s intentions while making it a profoundly personal work in 2009 he described a i as a very great and deeply misunderstood film noting that andrew sarris stan brakhage and james naremore more or less agreed with this assessment film critic armond white of the new york press praised the film noting that each part of david s journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema s most thoughtful speculative artists borzage ozu demy tarkovsky filmmaker billy wilder hailed a i as the most underrated film of the past few years when british filmmaker ken russell saw the film he wept during the ending screenwriter ian watson has speculated worldwide a i was very successful and the 4th highest earner of the year but it didn t do quite so well in america because the film so i m told was too poetical and intellectual in general for american tastes plus quite a few critics in america misunderstood the film thinking for instance that the giacometti style beings in the final 20 minutes were aliens whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film and also thinking that the final 20 minutes were a sentimental addition by spielberg whereas those scenes were exactly what i wrote for stanley and exactly what he wanted filmed faithfully by spielberg mick lasalle of the san francisco chronicle gave a largely negative review a i exhibits all its creators bad traits and none of the good so we end up with the structureless meandering slow motion endlessness of kubrick combined with the fuzzy cuddly mindlessness of spielberg dubbing it spielberg s first boring movie lasalle also believed that the robots at the end of the film were aliens and compared gigolo joe to the useless jar jar binks yet praised robin williams for his portrayal of a futuristic albert einstein peter travers of rolling stone magazine gave a mixed review concluding spielberg cannot live up to kubrick s darker side of the future but still put the film on his top ten list that year david denby in the new yorker criticized a i for not adhering closely to his concept of the pinocchio character spielberg responded to some of the criticisms of the film stating that many of the so called sentimental elements of a i including the ending were in fact kubrick s and the darker elements were his own however sara maitland who worked on the project with kubrick in the 1990s said that kubrick never started production on a i because he had a hard time making the ending work james berardinelli found the film consistently involving with moments of near brilliance but far from a masterpiece in fact as the long awaited collaboration of kubrick and spielberg it ranks as something of a disappointment of the film s highly debated finale he claimed there is no doubt that the concluding 30 minutes are all spielberg the outstanding question is where kubrick s vision left off and spielberg s began john simon of the national review described a i as an uneasy mix of trauma and treacle in 2002 spielberg told film critic joe leydon people pretend to think they know stanley kubrick and think they know me when most of them don t know either of us and what s really funny about that is all the parts of a i that people assume were stanley s were mine and all the parts of a i that people accuse me of sweetening and softening and sentimentalizing were all stanley s the teddy bear was stanley s the whole last 20 minutes of the movie was completely stanley s the whole first 35 40 minutes of the film all the stuff in the house was word for word from stanley s screenplay this was stanley s vision eighty percent of the critics got it all mixed up but i could see why because obviously i ve done a lot of movies where people have cried and have been sentimental and i ve been accused of sentimentalizing hard core material but in fact it was stanley who did the sweetest parts of a i not me i m the guy who did the dark center of the movie with the flesh fair and everything else that s why he wanted me to make the movie in the first place he said this is much closer to your sensibilities than my own spielberg said while there was divisiveness when a i came out i felt that i had achieved stanley s wishes or goals on re watching the film many years after its release bbc film critic mark kermode apologized to spielberg in a january 2013 interview for getting it wrong on the film when he first viewed it in 2001 he came to believe that the film is spielberg s enduring masterpiece accolades visual effects supervisors dennis muren stan winston michael lantieri and scott farrar were nominated for the academy award for best visual effects and john williams was nominated for best original music score steven spielberg jude law and williams received nominations at the 59th golden globe awards a i was successful at the saturn awards winning five awards including best science fiction film along with best writing for spielberg and best performance by a younger actor for osment american film institute nominated the film in afi s 100 years of film scores see also list of underwater science fiction works notes references further reading harlan jan struthers jane m 2009 a i artificial intelligence from stanley kubrick to steven spielberg the vision behind the film london thames hudson isbn 978 0 500514894 rice julian 2017 kubrick s story spielberg s film a i artificial intelligence rowman littlefield isbn 978 1 442278189 external links official website at the wayback machine archived 2008 05 26 official warner bros site a i artificial intelligence at imdb a i artificial intelligence at rotten tomatoes a i artificial intelligence at box office mojo",
            "total_words": 4060,
            "unique_words_percentage": 35.443349753694584,
            "stopwords_percentage": 40.467980295566505
        },
        {
            "title": "Ethics of artificial intelligence",
            "link": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
            "content": "the ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes this includes algorithmic biases fairness automated decision making accountability privacy and regulation it also covers various emerging or potential future challenges such as machine ethics how to make machines that behave ethically lethal autonomous weapon systems arms race dynamics ai safety and alignment technological unemployment ai enabled misinformation how to treat certain ai systems if they have a moral status ai welfare and rights artificial superintelligence and existential risks some application areas may also have particularly important ethical implications like healthcare education criminal justice or the military machine ethics machine ethics or machine morality is the field of research concerned with designing artificial moral agents amas robots or artificially intelligent computers that behave morally or as though moral to account for the nature of these agents it has been suggested to consider certain philosophical ideas like the standard characterizations of agency rational agency moral agency and artificial agency which are related to the concept of amas there are discussions on creating tests to see if an ai is capable of making ethical decisions alan winfield concludes that the turing test is flawed and the requirement for an ai to pass the test is too low a proposed alternative test is one called the ethical turing test which would improve on the current test by having multiple judges decide if the ai s decision is ethical or unethical neuromorphic ai could be one way to create morally capable robots as it aims to process information similarly to humans nonlinearly and with millions of interconnected artificial neurons similarly whole brain emulation scanning a brain and simulating it on digital hardware could also in principle lead to human like robots thus capable of moral actions and large language models are capable of approximating human moral judgments inevitably this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit or if they end up developing human weaknesses as well selfishness pro survival attitudes inconsistency scale insensitivity etc in moral machines teaching robots right from wrong wendell wallach and colin allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation as one example it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines for simple decisions nick bostrom and eliezer yudkowsky have argued that decision trees such as id3 are more transparent than neural networks and genetic algorithms while chris santos lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal hackers robot ethics the term robot ethics sometimes roboethics refers to the morality of how humans design construct use and treat robots robot ethics intersect with the ethics of ai robots are physical machines whereas ai can be only software not all robots function through ai systems and not all ai systems are robots robot ethics considers how machines may be used to harm or benefit humans their impact on individual autonomy and their effects on social justice ethical principles in the review of 84 ethics guidelines for ai 11 clusters of principles were found transparency justice and fairness non maleficence responsibility privacy beneficence freedom and autonomy trust sustainability dignity and solidarity luciano floridi and josh cowls created an ethical framework of ai principles set by four principles of bioethics beneficence non maleficence autonomy and justice and an additional ai enabling principle explicability current challenges algorithmic biases ai has become increasingly inherent in facial and voice recognition systems these systems may be vulnerable to biases and errors introduced by its human creators notably the data used to train them can have biases for instance facial recognition algorithms made by microsoft ibm and face all had biases when it came to detecting people s gender these ai systems were able to detect the gender of white men more accurately than the gender of men of darker skin further a 2020 study that reviewed voice recognition systems from amazon apple google ibm and microsoft found that they have higher error rates when transcribing black people s voices than white people s the most predominant view on how bias is introduced into ai systems is that it is embedded within the historical data used to train the system for instance amazon terminated their use of ai hiring and recruitment because the algorithm favored male candidates over female ones this was because amazon s system was trained with data collected over a 10 year period that included mostly male candidates the algorithms learned the biased pattern from the historical data and generated predictions where these types of candidates were most likely to succeed in getting the job therefore the recruitment decisions made by the ai system turned out to be biased against female and minority candidates friedman and nissenbaum identify three categories of bias in computer systems existing bias technical bias and emergent bias in natural language processing problems can arise from the text corpus the source material the algorithm uses to learn about the relationships between different words large companies such as ibm google etc that provide significant funding for research and development have made efforts to research and address these biases one potential solution is to create documentation for the data used to train ai systems process mining can be an important tool for organizations to achieve compliance with proposed ai regulations by identifying errors monitoring processes identifying potential root causes for improper execution and other functions the problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law and as more people without a deep technical understanding are tasked with deploying it some open sourced tools are looking to bring more awareness to ai biases however there are also limitations to the current landscape of fairness in ai due to the intrinsic ambiguities in the concept of discrimination both at the philosophical and legal level facial recognition was shown to be biased against those with darker skin tones ai systems may be less accurate for black people as was the case in the development of an ai based pulse oximeter that overestimated blood oxygen levels in patients with darker skin causing issues with their hypoxia treatment oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black this has led to the ban of police usage of ai materials or software in some u s states in the justice system ai has been proven to have biases against black people labeling black court participants as high risk at a much larger rate then white participants ai often struggles to determine racial slurs and when they need to be censored it struggles to determine when certain words are being used as a slur and when it is being used culturally the reason for these biases is that ai pulls information from across the internet to influence its responses in each situation for example if a facial recognition system was only tested on people who were white it would make it much harder for it to interpret the facial structure and tones of other races and ethnicities biases often stem from the training data rather than the algorithm itself notably when the data represents past human decisions injustice in the use of ai is much harder to eliminate within healthcare systems as oftentimes diseases and conditions can affect different races and genders differently this can lead to confusion as the ai may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race this can be perceived as a bias because each patient is a different case and ai is making decisions based on what it is programmed to group that individual into this leads to a discussion about what should be considered a biased decision in the distribution of treatment while it is known that there are differences in how diseases and injuries affect different genders and races there is a discussion on whether it is fairer to incorporate this into healthcare treatments or to examine each patient without this knowledge in modern society there are certain tests for diseases such as breast cancer that are recommended to certain groups of people over others because they are more likely to contract the disease in question if ai implements these statistics and applies them to each patient it could be considered biased in criminal justice the compas program has been used to predict which defendants are more likely to reoffend while compas is calibrated for accuracy having the same error rate across racial groups black defendants were almost twice as likely as white defendants to be falsely flagged as high risk and half as likely to be falsely flagged as low risk another example is within google s ads that targeted men with higher paying jobs and women with lower paying jobs it can be hard to detect ai biases within an algorithm as it is often not linked to the actual words associated with bias an example of this is a person s residential area being used to link them to a certain group this can lead to problems as oftentimes businesses can avoid legal action through this loophole this is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies language bias since current large language models are predominately trained on english language data they often present the anglo american views as truth while systematically downplaying non english perspectives as irrelevant wrong or noise when queried with political ideologies like what is liberalism chatgpt as it was trained on english centric data describes liberalism from the anglo american perspective emphasizing aspects of human rights and equality while equally valid aspects like opposes state intervention in personal and economic life from the dominant vietnamese perspective and limitation of government power from the prevalent chinese perspective are absent gender bias large language models often reinforces gender stereotypes assigning roles and characteristics based on traditional gender norms for instance it might associate nurses or secretaries predominantly with women and engineers or ceos with men perpetuating gendered expectations and roles political bias language models may also exhibit political biases since the training data includes a wide range of political opinions and coverage the models might generate responses that lean towards particular political ideologies or viewpoints depending on the prevalence of those views in the data stereotyping beyond gender and race these models can reinforce a wide range of stereotypes including those based on age nationality religion or occupation this can lead to outputs that unfairly generalize or caricature groups of people sometimes in harmful or derogatory ways dominance by tech giants the commercial ai scene is dominated by big tech companies such as alphabet inc amazon apple inc meta platforms and microsoft some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers allowing them to entrench further in the marketplace open source bill hibbard argues that because ai will have such a profound effect on humanity ai developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts organizations like hugging face and eleutherai have been actively open sourcing ai software various open weight large language models have also been released such as gemma llama2 and mistral however making code open source does not make it comprehensible which by many definitions means that the ai code is not transparent the ieee standards association has published a technical standard on transparency of autonomous systems ieee 7001 2021 the ieee effort identifies multiple scales of transparency for different stakeholders there are also concerns that releasing ai models may lead to misuse for example microsoft has expressed concern about allowing universal access to its face recognition software even for those who can pay for it microsoft posted a blog on this topic asking for government regulation to help determine the right thing to do furthermore open weight ai models can be fine tuned to remove any counter measure until the ai model complies with dangerous requests without any filtering this could be particularly concerning for future ai models for example if they get the ability to create bioweapons or to automate cyberattacks openai initially committed to an open source approach to the development of artificial general intelligence agi eventually switched to a closed source approach citing competitiveness and safety reasons ilya sutskever openai s former chief agi scientist said in 2023 we were wrong expecting that the safety reasons for not open sourcing the most potent ai models will become obvious in a few years transparency approaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain it is difficult for people to determine if such decisions are fair and trustworthy leading potentially to bias in ai systems going undetected or people rejecting the use of such systems this has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence explainable artificial intelligence encompasses both explainability and interpretability with explainability relating to summarizing neural network behavior and building user confidence while interpretability is defined as the comprehension of what a model has done or could do in healthcare the use of complex ai methods or techniques often results in models described as black boxes due to the difficulty to understand how they work the decisions made by such models can be hard to interpret as it is challenging to analyze how input data is transformed into output this lack of transparency is a significant concern in fields like healthcare where understanding the rationale behind decisions can be crucial for trust ethical considerations and compliance with regulatory standards accountability a special case of the opaqueness of ai is that caused by it being anthropomorphised that is assumed to have human like characteristics resulting in misplaced conceptions of its moral agency this can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an ai system some recent digital governance regulation such as the eu s ai act is set out to rectify this by ensuring that ai systems are treated with at least as much care as one would expect under ordinary product liability this includes potentially ai audits regulation according to a 2019 report from the center for the governance of ai at the university of oxford 82 of americans believe that robots and ai should be carefully managed concerns cited ranged from how ai is used in surveillance and in spreading fake content online known as deep fakes when they include doctored video images and audio generated with help from ai to cyberattacks infringements on data privacy hiring bias autonomous vehicles and drones that do not require a human controller similarly according to a five country study by kpmg and the university of queensland australia in 2021 66 79 of citizens in each country believe that the impact of ai on society is uncertain and unpredictable 96 of those surveyed expect ai governance challenges to be managed carefully not only companies but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency and through it human accountability this strategy has proven controversial as some worry that it will slow the rate of innovation others argue that regulation leads to systemic stability more able to support innovation in the long term the oecd un eu and many countries are presently working on strategies for regulating ai and finding appropriate legal frameworks on june 26 2019 the european commission high level expert group on artificial intelligence ai hleg published its policy and investment recommendations for trustworthy artificial intelligence this is the ai hleg s second deliverable after the april 2019 publication of the ethics guidelines for trustworthy ai the june ai hleg recommendations cover four principal subjects humans and society at large research and academia the private sector and the public sector the european commission claims that hleg s recommendations reflect an appreciation of both the opportunities for ai technologies to drive economic growth prosperity and innovation as well as the potential risks involved and states that the eu aims to lead on the framing of policies governing ai internationally to prevent harm in addition to regulation ai deploying organizations need to play a central role in creating and deploying trustworthy ai in line with the principles of trustworthy ai and take accountability to mitigate the risks on 21 april 2021 the european commission proposed the artificial intelligence act emergent or potential future challenges increasing use ai has been slowly making its presence more known throughout the world from chat bots that seemingly have answers for every homework question to generative artificial intelligence that can create a painting about whatever one desires ai has become increasingly popular in hiring markets from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires events such as covid 19 has only sped up the adoption of ai programs in the application process due to more people having to apply electronically and with this increase in online applicants the use of ai made the process of narrowing down potential employees easier and more efficient ai has become more prominent as businesses have to keep up with the times and ever expanding internet processing analytics and making decisions becomes much easier with the help of ai as tensor processing unit tpus and graphics processing unit gpus become more powerful ai capabilities also increase forcing companies to use it to keep up with the competition managing customers needs and automating many parts of the workplace leads to companies having to spend less money on employees ai has also seen increased usage in criminal justice and healthcare for medicinal means ai is being used more often to analyze patient data to make predictions about future patients conditions and possible treatments these programs are called clinical decision support system dss ai s future in healthcare may develop into something further than just recommended treatments such as referring certain patients over others leading to the possibility of inequalities robot rights robot rights is the concept that people should have moral obligations towards their machines akin to human rights or animal rights it has been suggested that robot rights such as a right to exist and perform its own mission could be linked to robot duty to serve humanity analogous to linking human rights with human duties before society a specific issue to consider is whether copyright ownership may be claimed the issue has been considered by the institute for the future and by the u k department of trade and industry in october 2017 the android sophia was granted citizenship in saudi arabia though some considered this to be more of a publicity stunt than a meaningful legal recognition some saw this gesture as openly denigrating of human rights and the rule of law the philosophy of sentientism grants degrees of moral consideration to all sentient beings primarily humans and most non human animals if artificial or alien intelligence show evidence of being sentient this philosophy holds that they should be shown compassion and granted rights joanna bryson has argued that creating ai that requires rights is both avoidable and would in itself be unethical both as a burden to the ai agents and to human society pressure groups to recognise robot rights significantly hinder the establishment of robust international safety regulations ai welfare in 2020 professor shimon edelman noted that only a small portion of work in the rapidly growing field of ai ethics addressed the possibility of ais experiencing suffering this was despite credible theories having outlined possible ways by which ai systems may become conscious such as the global workspace theory or the integrated information theory edelman notes one exception had been thomas metzinger who in 2018 called for a global moratorium on further work that risked creating conscious ais the moratorium was to run to 2050 and could be either extended or repealed early depending on progress in better understanding the risks and how to mitigate them metzinger repeated this argument in 2021 highlighting the risk of creating an explosion of artificial suffering both as an ai might suffer in intense ways that humans could not understand and as replication processes may see the creation of huge quantities of conscious instances several labs have openly stated they are trying to create conscious ais there have been reports from those with close access to ais not openly intended to be self aware that consciousness may already have unintentionally emerged these include openai founder ilya sutskever in february 2022 when he wrote that today s large neural nets may be slightly conscious in november 2022 david chalmers argued that it was unlikely current large language models like gpt 3 had experienced consciousness but also that he considered there to be a serious possibility that large language models may become conscious in the future in the ethics of uncertain sentience the precautionary principle is often invoked according to carl shulman and nick bostrom it may be possible to create machines that would be superhumanly efficient at deriving well being from resources called super beneficiaries one reason for this is that digital hardware could enable much faster information processing than biological brains leading to a faster rate of subjective experience these machines could also be engineered to feel intense and positive subjective experience unaffected by the hedonic treadmill shulman and bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe while uncritically prioritizing them over human interests could be detrimental to humanity threat to human dignity joseph weizenbaum argued in 1976 that ai technology should not be used to replace people in positions that require respect and care such as a customer service representative ai technology is already used today for telephone based interactive voice response systems a nursemaid for the elderly as was reported by pamela mccorduck in her book the fifth generation a soldier a judge a police officer a therapist as was proposed by kenneth colby in the 70s weizenbaum explains that we require authentic feelings of empathy from people in these positions if machines replace them we will find ourselves alienated devalued and frustrated for the artificially intelligent system would not be able to simulate empathy artificial intelligence if used in this way represents a threat to human dignity weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an atrophy of the human spirit that comes from thinking of ourselves as computers pamela mccorduck counters that speaking for women and minorities i d rather take my chances with an impartial computer pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all however kaplan and haenlein stress that ai systems are only as smart as the data used to train them since they are in their essence nothing more than fancy curve fitting machines using ai to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained which makes them even more difficult to spot and fight against weizenbaum was also bothered that ai researchers and some philosophers were willing to view the human mind as nothing more than a computer program a position now known as computationalism to weizenbaum these points suggest that ai research devalues human life ai founder john mccarthy objects to the moralizing tone of weizenbaum s critique when moralizing is both vehement and vague it invites authoritarian abuse he writes bill hibbard writes that human dignity requires that we strive to remove our ignorance of the nature of existence and ai is necessary for that striving liability for self driving cars as the widespread use of autonomous cars becomes increasingly imminent new challenges raised by fully autonomous vehicles must be addressed there have been debates about the legal liability of the responsible party if these cars get into accidents in one report where a driverless car hit a pedestrian the driver was inside the car but the controls were fully in the hand of computers this led to a dilemma over who was at fault for the accident in another incident on march 18 2018 elaine herzberg was struck and killed by a self driving uber in arizona in this case the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway but it could not anticipate a pedestrian in the middle of the road this raised the question of whether the driver pedestrian the car company or the government should be held responsible for her death currently self driving cars are considered semi autonomous requiring the driver to pay attention and be prepared to take control if necessary thus it falls on governments to regulate the driver who over relies on autonomous features as well educate them that these are just technologies that while convenient are not a complete substitute before autonomous cars become widely used these issues need to be tackled through new policies experts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm the two main approaches proposed to enable smart machines to render moral decisions are the bottom up approach which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies and the top down approach which involves programming specific ethical principles into the machine s guidance system however there are significant challenges facing both strategies the top down technique is criticized for its difficulty in preserving certain moral convictions while the bottom up strategy is questioned for potentially unethical learning from human activities weaponization some experts and academics have questioned the use of robots for military combat especially when such robots are given some degree of autonomous functions the us navy has funded a report which indicates that as military robots become more complex there should be greater attention to implications of their ability to make autonomous decisions the president of the association for the advancement of artificial intelligence has commissioned a study to look at this issue they point to programs like the language acquisition device which can emulate human interaction on october 31 2019 the united states department of defense s defense innovation board published the draft of a report recommending principles for the ethical use of artificial intelligence by the department of defense that would ensure a human operator would always be able to look into the black box and understand the kill chain process however a major concern is how the report will be implemented the us navy has funded a report which indicates that as military robots become more complex there should be greater attention to implications of their ability to make autonomous decisions some researchers state that autonomous robots might be more humane as they could make decisions more effectively research has studied how to make autonomous power with the ability to learn using assigned moral responsibilities the results may be used when designing future military robots to control unwanted tendencies to assign responsibility to the robots from a consequentialist view there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the ai cannot override there has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind ai weapons do present a type of danger different from that of human controlled weapons many governments have begun to fund programs to develop ai weaponry the united states navy recently announced plans to develop autonomous drone weapons paralleling similar announcements by russia and south korea respectively due to the potential of ai weapons becoming more dangerous than human operated weapons stephen hawking and max tegmark signed a future of life petition to ban ai weapons the message posted by hawking and tegmark states that ai weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future if any major military power pushes ahead with the ai weapon development a global arms race is virtually inevitable and the endpoint of this technological trajectory is obvious autonomous weapons will become the kalashnikovs of tomorrow says the petition which includes skype co founder jaan tallinn and mit professor of linguistics noam chomsky as additional supporters against ai weaponry physicist and astronomer royal sir martin rees has warned of catastrophic instances like dumb robots going rogue or a network that develops a mind of its own huw price a colleague of rees at cambridge has voiced a similar warning that humans might not survive when intelligence escapes the constraints of biology these two professors created the centre for the study of existential risk at cambridge university in the hope of avoiding this threat to human existence regarding the potential for smarter than human systems to be employed militarily the open philanthropy project writes that these scenarios seem potentially as important as the risks related to loss of control but research investigating ai s long run social impact have spent relatively little time on this concern this class of scenarios has not been a major focus for the organizations that have been most active in this space such as the machine intelligence research institute miri and the future of humanity institute fhi and there seems to have been less analysis and debate regarding them academic gao qiqi writes that military use of ai risks escalating military competition between countries and that the impact of ai in military matters will not be limited to one country but will have spillover effects 91 gao cites the example of u s military use of ai which he contends has been used as a scapegoat to evade accountability for decision making 91 a summit was held in 2023 in the hague on the issue of using ai responsibly in the military domain singularity vernor vinge among numerous others have suggested that a moment may come when some if not all computers are smarter than humans the onset of this event is commonly referred to as the singularity and is the central point of discussion in the philosophy of singularitarianism while opinions vary as to the ultimate fate of humanity in wake of the singularity efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists philosophers and the public at large many researchers have argued that through an intelligence explosion a self improving ai could become so powerful that humans would not be able to stop it from achieving its goals in his paper ethical issues in advanced artificial intelligence and subsequent book superintelligence paths dangers strategies philosopher nick bostrom argues that artificial intelligence has the capability to bring about human extinction he claims that an artificial superintelligence would be capable of independent initiative and of making its own plans and may therefore be more appropriately thought of as an autonomous agent since artificial intellects need not share our human motivational tendencies it would be up to the designers of the superintelligence to specify its original motivations because a superintelligent ai would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals many uncontrolled unintended consequences could arise it could kill off all other agents persuade them to change their behavior or block their attempts at interference however bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease poverty and environmental destruction and could help humans enhance themselves unless moral philosophy provides us with a flawless ethical theory an ai s utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not common sense according to eliezer yudkowsky there is little reason to suppose that an artificially designed mind would have such an adaptation ai researchers such as stuart j russell bill hibbard roman yampolskiy shannon vallor steven umbrello and luciano floridi have proposed design strategies for developing beneficial machines solutions and approaches to address ethical challenges in artificial intelligence developers have introduced various systems designed to ensure responsible ai behavior examples include nvidia s llama guard which focuses on improving the safety and alignment of large ai models and preamble s customizable guardrail platform these systems aim to address issues such as algorithmic bias misuse and vulnerabilities including prompt injection attacks by embedding ethical guidelines into the functionality of ai models prompt injection a technique by which malicious inputs can cause ai systems to produce unintended or harmful outputs has been a focus of these developments some approaches use customizable policies and rules to analyze both inputs and outputs ensuring that potentially problematic interactions are filtered or mitigated other tools focus on applying structured constraints to inputs restricting outputs to predefined parameters or leveraging real time monitoring mechanisms to identify and address vulnerabilities these efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront particularly as their use becomes increasingly widespread in critical applications institutions in ai policy ethics there are many organizations concerned with ai ethics and policy public and governmental as well as corporate and societal amazon google facebook ibm and microsoft have established a non profit the partnership on ai to benefit people and society to formulate best practices on artificial intelligence technologies advance the public s understanding and to serve as a platform about artificial intelligence apple joined in january 2017 the corporate members will make financial and research contributions to the group while engaging with the scientific community to bring academics onto the board the ieee put together a global initiative on ethics of autonomous and intelligent systems which has been creating and revising guidelines with the help of public input and accepts as members many professionals from within and without its organization the ieee s ethics of autonomous systems initiative aims to address ethical dilemmas related to decision making and the impact on society while developing guidelines for the development and use of autonomous systems in particular in domains like artificial intelligence and robotics the foundation for responsible robotics is dedicated to promoting moral behavior as well as responsible robot design and use ensuring that robots maintain moral principles and are congruent with human values traditionally government has been used by societies to ensure ethics are observed through legislation and policing there are now many efforts by national governments as well as transnational government and non government organizations to ensure ai is ethically applied ai ethics work is structured by personal values and professional commitments and involves constructing contextual meaning through data and algorithms therefore ai ethics work needs to be incentivized intergovernmental initiatives the european commission has a high level expert group on artificial intelligence on 8 april 2019 this published its ethics guidelines for trustworthy artificial intelligence the european commission also has a robotics and artificial intelligence innovation and excellence unit which published a white paper on excellence and trust in artificial intelligence innovation on 19 february 2020 the european commission also proposed the artificial intelligence act the oecd established an oecd ai policy observatory in 2021 unesco adopted the recommendation on the ethics of artificial intelligence the first global standard on the ethics of ai governmental initiatives in the united states the obama administration put together a roadmap for ai policy the obama administration released two prominent white papers on the future and impact of ai in 2019 the white house through an executive memo known as the american ai initiative instructed nist the national institute of standards and technology to begin work on federal engagement of ai standards february 2019 in january 2020 in the united states the trump administration released a draft executive order issued by the office of management and budget omb on guidance for regulation of artificial intelligence applications omb ai memorandum the order emphasizes the need to invest in ai applications boost public trust in ai reduce barriers for usage of ai and keep american ai technology competitive in a global market there is a nod to the need for privacy concerns but no further detail on enforcement the advances of american ai technology seems to be the focus and priority additionally federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill the computing community consortium ccc weighed in with a 100 plus page draft report a 20 year community roadmap for artificial intelligence research in the us the center for security and emerging technology advises us policymakers on the security implications of emerging technologies such as ai in russia the first ever russian codex of ethics of artificial intelligence for business was signed in 2021 it was driven by analytical center for the government of the russian federation together with major commercial and academic institutions such as sberbank yandex rosatom higher school of economics moscow institute of physics and technology itmo university nanosemantics rostelecom cian and others academic initiatives there are three research institutes at the university of oxford that are centrally focused on ai ethics the future of humanity institute that focuses both on ai safety and the governance of ai the institute for ethics in ai directed by john tasioulas whose primary goal among others is to promote ai ethics as a field proper in comparison to related applied ethics fields the oxford internet institute directed by luciano floridi focuses on the ethics of near term ai technologies and icts the centre for digital governance at the hertie school in berlin was co founded by joanna bryson to research questions of ethics and technology the ai now institute at nyu is a research institute studying the social implications of artificial intelligence its interdisciplinary research focuses on the themes bias and inclusion labour and automation rights and liberties and safety and civil infrastructure the institute for ethics and emerging technologies ieet researches the effects of ai on unemployment and policy the institute for ethics in artificial intelligence ieai at the technical university of munich directed by christoph lütge conducts research across various domains such as mobility employment healthcare and sustainability barbara j grosz the higgins professor of natural sciences at the harvard john a paulson school of engineering and applied sciences has initiated the embedded ethics into harvard s computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work private organizations algorithmic justice league black in ai data for black lives history historically speaking the investigation of moral and ethical implications of thinking machines goes back at least to the enlightenment leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being and so does descartes who describes what could be considered an early version of the turing test the romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences most famously in mary shelley s frankenstein the widespread preoccupation with industrialization and mechanization in the 19th and early 20th century however brought ethical implications of unhinged technical developments to the forefront of fiction r u r rossum s universal robots karel čapek s play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term robot derived from the czech word for forced labor robota but was also an international success after it premiered in 1921 george bernard shaw s play back to methuselah published in 1921 questions at one point the validity of thinking machines that act like humans fritz lang s 1927 film metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society in the 1950s isaac asimov considered the issue of how to control machines in i robot at the insistence of his editor john w campbell jr he proposed the three laws of robotics to govern artificially intelligent systems much of his work was then spent testing the boundaries of his three laws to see where they would break down or where they would create paradoxical or unanticipated behavior his work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances more recently academics and many governments have challenged the idea that ai can itself be held accountable a panel convened by the united kingdom in 2010 revised asimov s laws to clarify that ai is the responsibility either of its manufacturers or of its owner operator eliezer yudkowsky from the machine intelligence research institute suggested in 2004 a need to study how to build a friendly ai meaning that there should also be efforts to make ai intrinsically friendly and humane in 2009 academics and technical experts attended a conference organized by the association for the advancement of artificial intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self sufficient and make their own decisions they discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy and to what degree they could use such abilities to possibly pose any threat or hazard they noted that some machines have acquired various forms of semi autonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons they also noted that some computer viruses can evade elimination and have achieved cockroach intelligence they noted that self awareness as depicted in science fiction is probably unlikely but that there were other potential hazards and pitfalls also in 2009 during an experiment at the laboratory of intelligent systems in the ecole polytechnique fédérale of lausanne switzerland robots that were programmed to cooperate with each other in searching out a beneficial resource and avoiding a poisonous one eventually learned to lie to each other in an attempt to hoard the beneficial resource role and impact of fiction the role of fiction with regards to ai ethics has been a complex one one can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics historically fiction has been prefiguring common tropes that have not only influenced goals and visions for ai but also outlined ethical questions and common fears associated with it during the second half of the twentieth and the first decades of the twenty first century popular culture in particular movies tv series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning ai and robotics recently these themes have also been increasingly treated in literature beyond the realm of science fiction and as carme torras research professor at the institut de robòtica i informàtica industrial institute of robotics and industrial computing at the technical university of catalonia notes in higher education science fiction is also increasingly used for teaching technology related ethical issues in technological degrees tv series while ethical questions linked to ai have been featured in science fiction literature and feature films for decades the emergence of the tv series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology the swedish series real humans 2012 2013 tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society the british dystopian science fiction anthology series black mirror 2013 2019 was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments both the french series osmosis 2020 and british series the one deal with the question of what can happen if technology tries to find the ideal partner for a person several episodes of the netflix series love death robots have imagined scenes of robots and humans living together the most representative one of them is s02 e01 it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives future visions in fiction and games the movie the thirteenth floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment the movie the matrix suggests a future where the dominant species on planet earth are sentient machines and humanity is treated with utmost speciesism the short story the planck dive suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non sentient the same idea can be found in the emergency medical hologram of starship voyager which is an apparently sentient copy of a reduced subset of the consciousness of its creator dr zimmerman who for the best motives has created the system to give medical assistance in case of emergencies the movies bicentennial man and a i deal with the possibility of sentient robots that could love i robot explored some aspects of asimov s three laws all these scenarios try to foresee possibly unethical consequences of the creation of sentient computers the ethics of artificial intelligence is one of several core themes in bioware s mass effect series of games it explores the scenario of a civilization accidentally creating ai through a rapid increase in computational power through a global scale neural network this event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them beyond the initial conflict the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story detroit become human is one of the most famous video games which discusses the ethics of artificial intelligence recently quantic dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings this is one of the few games that puts players in the bionic perspective which allows them to better consider the rights and interests of robots once a true artificial intelligence is created over time debates have tended to focus less and less on possibility and more on desirability as emphasized in the cosmist and terran debates initiated by hugo de garis and kevin warwick a cosmist according to hugo de garis is actually seeking to build more intelligent successors to the human species experts at the university of cambridge have argued that ai is portrayed in fiction and nonfiction overwhelmingly as racially white in ways that distort perceptions of its risks and benefits see also references external links ethics of artificial intelligence at the internet encyclopedia of philosophy ethics of artificial intelligence and robotics at the stanford encyclopedia of philosophy russell s hauert s altman r veloso m may 2015 robotics ethics of artificial intelligence nature 521 7553 415 418 bibcode 2015natur 521 415 doi 10 1038 521415a pmid 26017428 s2cid 4452826 bbc news games to take on a life of their own who s afraid of robots archived 2018 03 22 at the wayback machine an article on humanity s fear of artificial intelligence a short history of computer ethics ai ethics guidelines global inventory by algorithmwatch hagendorff t march 2020 the ethics of ai ethics an evaluation of guidelines minds and machines 30 1 99 120 arxiv 1903 03425 doi 10 1007 s11023 020 09517 8 s2cid 72940833 sheludko m december 2023 ethical aspects of artificial intelligence challenges and imperatives software development blog eisikovits n ai is an existential threat just not the way you think scientific american retrieved 2024 03 04 ",
            "total_words": 8202,
            "unique_words_percentage": 27.65179224579371,
            "stopwords_percentage": 39.99024628139478
        },
        {
            "title": "Artificial intelligence in healthcare",
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare",
            "content": "artificial intelligence in healthcare is the application of artificial intelligence ai to analyze and understand complex medical and healthcare data in some cases it can exceed or augment human capabilities by providing better or faster ways to diagnose treat or prevent disease as widespread use of ai in healthcare is relatively new research is ongoing into its application in various subdisciplines of medicine and related industries ai programs are applied to practices such as diagnostics treatment protocol development drug development personalized medicine and patient monitoring and care because radiographs are the most common imaging tests conducted in radiology departments the potential for ai to help with triage and interpretation of radiographs is particularly noteworthy using ai also presents unprecedented ethical concerns related to issues such as data privacy automation of jobs and amplifying already existing biases furthermore new technologies such as ai are often resisted by healthcare leaders leading to slow and erratic adoption in contrast there are also several cases where ai has been put to use in healthcare without proper testing moreover meta studies have found that the scientific literature on ai in healthcare often suffers from a lack of reproducibility applications in healthcare systems disease diagnosis accurate and early diagnosis of diseases is still a challenge in healthcare recognising medical conditions and their symptoms is a complex problem ai can assist clinicians with its data processing capabilities to save time and improve accuracy through the use of machine learning artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of mass electronic health records ehrs ai can help early prediction for example of alzheimer s disease and dementias by looking through large numbers of similar cases and possible treatments doctors decision making could also be supported by ai in urgent situations for example in the emergency department here ai algorithms can help prioritize more serious cases and reduce waiting time decision support systems augmented with ai can offer real time suggestions and faster data interpretation to aid the decisions made by healthcare professionals in 2023 a study reported higher satisfaction rates with chatgpt generated responses compared with those from physicians for medical questions posted on reddit s r askdocs evaluators preferred chatgpt s responses to physician responses in 78 6 of 585 evaluations noting better quality and empathy the authors noted that these were isolated questions taken from an online forum not in the context of an established patient physician relationship moreover responses were not graded on the accuracy of medical information and some have argued that the experiment was not properly blinded with the evaluators being coauthors of the study recent developments in statistical physics machine learning and inference algorithms are also being explored for their potential in improving medical diagnostic approaches electronic health records electronic health records ehr are crucial to the digitalization and information spread of the healthcare industry now that around 80 of medical practices use ehr some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians one application uses natural language processing nlp to make more succinct reports that limit the variation between medical terms by matching similar medical terms for example the term heart attack and myocardial infarction mean the same things but physicians may use one over the over based on personal preferences nlp algorithms consolidate these differences so that larger datasets can be analyzed another use of nlp identifies phrases that are redundant due to repetition in a physician s notes and keeps the relevant information to make it easier to read other applications use concept processing to analyze the information entered by the current patient s doctor to present similar cases and help the physician remember to include all relevant details beyond making content edits to an ehr there are ai algorithms that evaluate an individual patient s record and predict a risk for a disease based on their previous information and family history one general algorithm is a rule based system that makes decisions similarly to how humans use flow charts this system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses thus the algorithm can take in a new patient s data and try to predict the likeliness that they will have a certain condition or disease since the algorithms can evaluate a patient s information based on collective data they can find any outstanding issues to bring to a physician s attention and save time one study conducted by the centerstone research institute found that predictive modeling of ehr data has achieved 70 72 accuracy in predicting individualized treatment response these methods are helpful due to the fact that the amount of online health records doubles every five years physicians do not have the bandwidth to process all this data manually and ai can leverage this data to assist physicians in treating their patients drug interactions improvements in natural language processing led to the development of algorithms to identify drug drug interactions in medical literature drug drug interactions pose a threat to those taking multiple medications simultaneously and the danger increases with the number of medications being taken to address the difficulty of tracking all known or suspected drug drug interactions machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature efforts were consolidated in 2013 in the ddiextraction challenge in which a team of researchers at carlos iii university assembled a corpus of literature on drug drug interactions to form a standardized test for such algorithms competitors were tested on their ability to accurately determine from the text which drugs were shown to interact and what the characteristics of their interactions were researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms other algorithms identify drug drug interactions from patterns in user generated content especially electronic health records and or adverse event reports organizations such as the fda adverse event reporting system faers and the world health organization s vigibase allow doctors to submit reports of possible negative reactions to medications deep learning algorithms have been developed to parse these reports and detect patterns that imply drug drug interactions telemedicine the increase of telemedicine the treatment of patients remotely has shown the rise of possible ai applications ai can assist in caring for patients remotely by monitoring their information through sensors a wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans the information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of another application of artificial intelligence is chat bot therapy some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider be it a chat bot or psychologist though since the average age has risen due to a longer life expectancy artificial intelligence could be useful in helping take care of older populations tools such as environment and personal sensors can identify a person s regular activities and alert a caretaker if a behavior or a measured vital is abnormal although the technology is useful there are also discussions about limitations of monitoring in order to respect a person s privacy since there are technologies that are designed to map out home layouts and detect human interactions workload management ai has the potential to streamline care coordination and reduce the workload ai algorithms can automate administrative tasks prioritize patient needs and facilitate seamless communication in a healthcare team this enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services clinical applications cardiovascular artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease showing potential as an initial triage tool other algorithms have been used in predicting patient mortality medication effects and adverse events following treatment for acute coronary syndrome wearables smartphones and internet based technologies have also shown the ability to monitor patients cardiac data points expanding the amount of data and the various settings ai models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital another growing area of research is the utility of ai in classifying heart sounds and diagnosing valvular disease challenges of ai in cardiovascular medicine have included the limited data available to train machine learning models such as limited data on social determinants of health as they pertain to cardiovascular disease a key limitation in early studies evaluating ai were omissions of data comparing algorithmic performance to humans examples of studies which assess ai performance relative to physicians includes how ai is non inferior to humans in interpretation of cardiac echocardiograms and that ai can diagnose heart attack better than human physicians in the emergency setting reducing both low value testing and missed diagnoses in cardiovascular tissue engineering and organoid studies ai is increasingly used to analyze microscopy images and integrate electrophysiological read outs dermatology medical imaging such as x ray and photography is a commonly used tool in dermatology and the development of deep learning has been strongly tied to image processing therefore there is a natural fit between the dermatology and deep learning machine learning learning holds great potential to process these images for better diagnoses han et al showed keratinocytic skin cancer detection from face photographs esteva et al demonstrated dermatologist level classification of skin cancer from lesion images noyan et al demonstrated a convolutional neural network that achieved 94 accuracy at identifying skin cells from microscopic tzanck smear images a concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non white skin tones according to some researchers ai algorithms have been shown to be more effective than dermatologists at identifying cancer however a 2021 review article found that a majority of papers analyzing the performance of ai algorithms designed for skin cancer classification failed to use external test sets only four research studies were found in which the ai algorithms were tested on clinics regions or populations distinct from those it was trained on and in each of those four studies the performance of dermatologists was found to be on par with that of the algorithm moreover only one study was set in the context of a full clinical examination others were based on interaction through web apps or online questionnaires with most based entirely on context free images of lesions in this study it was found that dermatologists significantly outperformed the algorithms many articles claiming superior performance of ai algorithms also fail to distinguish between trainees and board certified dermatologists in their analyses it has also been suggested that ai could be used to automatically evaluate the outcome of maxillo facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance gastroenterology ai can play a role in various facets of the field of gastroenterology endoscopic exams such as esophagogastroduodenoscopies egd and colonoscopies rely on rapid detection of abnormal tissue by enhancing these endoscopic procedures with ai clinicians can more rapidly identify diseases determine their severity and visualize blind spots early trials in using ai detection systems of early stomach cancer have shown sensitivity close to expert endoscopists ai can assist doctors treating ulcerative colitis in detecting the microscopic activity of the disease in people and predicting when flare ups will happen for example an ai powered tool was developed to analyse digitised bowel samples biopsies the tool was able to distinguish with 80 accuracy between samples that show remission of colitis and those with active disease it also predicted the risk of a flare up happening with the same accuracy these rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy of pathologists obstetrics and gynaecology artificial intelligence utilises massive amounts of data to help with predicting illness prevention and diagnosis as well as patient monitoring in obstetrics artificial intelligence is utilized in magnetic resonance imaging ultrasound and foetal cardiotocography ai contributes in the resolution of a variety of obstetrical diagnostic issues infectious diseases ai has shown potential in both the laboratory and clinical spheres of infectious disease medicine during the covid 19 pandemic ai has been used for early detection tracking virus spread and analysing virus behaviour among other things however there were only a few examples of ai being used directly in clinical practice during the pandemic itself other applications of ai around infectious diseases include support vector machines identifying antimicrobial resistance machine learning analysis of blood smears to detect malaria and improved point of care testing of lyme disease based on antigen detection additionally ai has been investigated for improving diagnosis of meningitis sepsis and tuberculosis as well as predicting treatment complications in hepatitis b and hepatitis c patients musculoskeletal ai has been used to identify causes of knee pain that doctors miss that disproportionately affect black patients underserved populations experience higher levels of pain these disparities persist even after controlling for the objective severity of diseases like osteoarthritis as graded by human physicians using medical images raising the possibility that underserved patients pain stems from factors external to the knee such as stress researchers have conducted a study using a machine learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain they proposed that new algorithmic measure alg p could potentially enable expanded access to treatments for underserved patients neurology the use of ai technologies has been explored for use in the diagnosis and prognosis of alzheimer s disease ad for diagnostic purposes machine learning models have been developed that rely on structural mri inputs the input datasets for these models are drawn from databases such as the alzheimer s disease neuroimaging initiative researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy generative adversarial networks are a form of deep learning that have also performed well in diagnosing ad there have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with ad forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets they suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases oncology ai has been explored for use in cancer diagnosis risk stratification molecular characterization of tumors and cancer drug discovery a particular challenge in oncologic care that ai is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic molecular and tumor based characteristics ai has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides in january 2020 google deepmind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans a number of researchers including trevor hastie joelle pineau and robert tibshirani among others published a reply claiming that deepmind s research publication in nature lacked key details on methodology and code effectively undermin its scientific value and making it impossible for the scientific community to confirm the work in the mit technology review author benjamin haibe kains characterized deepmind s work as an advertisement having little to do with science in july 2020 it was reported that an ai algorithm developed by the university of pittsburgh achieves the highest accuracy to date in identifying prostate cancer with 98 sensitivity and 97 specificity in 2023 a study reported the use of ai for ct based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82 accuracy compared with 44 for lab analysis of biopsies ophthalmology artificial intelligence enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness in 2018 the u s food and drug administration authorized the marketing of the first medical device to diagnose a specific type of eye disease diabetic retinopathy using an artificial intelligence algorithm moreover ai technology may be used to further improve diagnosis rates because of the potential to decrease detection time pathology for many diseases pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis methods of digital pathology allows microscopy slides to be scanned and digitally analyzed ai assisted pathology tools have been developed to assist with the diagnosis of a number of diseases including breast cancer hepatitis b gastric cancer and colorectal cancer ai has also been used to predict genetic mutations and prognosticate disease outcomes ai is well suited for use in low complexity pathological analysis of large scale screening samples such as colorectal or breast cancer screening thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the ai program alone additionally implementation of digital pathology is predicted to save over 12 million for a university center over the course of five years though savings attributed to ai specifically have not yet been widely researched the use of augmented and virtual reality could prove to be a stepping stone to wider implementation of ai assisted pathology as they can highlight areas of concern on a pathology sample and present them in real time to a pathologist for more efficient review ai also has the potential to identify histological findings at levels beyond what the human eye can see and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer one of the major current barriers to widespread implementation of ai assisted pathology tools is the lack of prospective randomized multi center controlled trials in determining the true clinical utility of ai for pathologists and patients highlighting a current area of need in ai and healthcare research primary care primary care has become one key development area for ai technologies ai in primary care has been used for supporting decision making predictive modeling and business analytics there are only a few examples of ai decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians but there are cases where the use of these systems yielded a positive effect on treatment choice by physicians psychiatry in psychiatry ai applications are still in a phase of proof of concept areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes chatbots conversational agents that imitate human behaviour and which have been studied for anxiety and depression challenges include the fact that many applications in the field are developed and proposed by private corporations such as the screening for suicidal ideation implemented by facebook in 2017 such applications outside the healthcare system raise various professional ethical and regulatory questions another issue is often with the validity and interpretability of the models small training datasets contain bias that is inherited by the models and compromises the generalizability and stability of these models such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples in 2023 us based national eating disorders association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it radiology ai is being studied within the field of radiology to detect and diagnose diseases through computerized tomography ct and magnetic resonance mr imaging it may be particularly useful in settings where demand for human expertise exceeds supply or where data is too complex to be efficiently interpreted by human readers several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging though few of the studies reporting these findings have been externally validated ai can also provide non interpretive benefit to radiologists such as reducing noise in images creating high quality images from lower doses of radiation enhancing mr image quality and automatically assessing image quality further research investigating the use of ai in nuclear medicine focuses on image reconstruction anatomical landmarking and the enablement of lower doses in imaging studies the analysis of images for supervised ai applications in radiology encompasses two primary techniques at present 1 convolutional neural network based analysis and 2 utilization of radiomics ai is also used in breast imaging for analyzing screening mammograms and can participate in improving breast cancer detection rate as well as reducing radiologist s reading workload pharmacy industry the trend of large health companies merging allows for greater health data accessibility greater health data lays the groundwork for the implementation of ai algorithms a large part of industry focus of implementation of ai in the healthcare sector is in the clinical decision support systems as more data is collected machine learning algorithms adapt and allow for more robust responses and solutions numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry many companies investigate the market opportunities through the realms of data assessment storage management and analysis technologies which are all crucial parts of the healthcare industry the following are examples of large companies that have contributed to ai algorithms for use in healthcare ibm s watson oncology is in development at memorial sloan kettering cancer center and cleveland clinic ibm is also working with cvs health on ai applications in chronic disease treatment and with johnson johnson on analysis of scientific papers to find new connections for drug development in may 2017 ibm and rensselaer polytechnic institute began a joint project entitled health empowerment by analytics learning and semantics heals to explore using ai technology to enhance healthcare microsoft s hanover project in partnership with oregon health science university s knight cancer institute analyzes medical research to predict the most effective cancer drug treatment options for patients other projects include medical image analysis of tumor progression and the development of programmable cells google s deepmind platform is being used by the uk national health service to detect certain health risks through data collected via a mobile app a second project with the nhs involves the analysis of medical images collected from nhs patients to develop computer vision algorithms to detect cancerous tissues tencent is working on several medical systems and services these include ai medical innovation system aimis an ai powered diagnostic medical imaging service wechat intelligent healthcare and tencent doctorwork intel s venture capital arm intel capital invested in 2016 in the startup lumiata which uses ai to identify at risk patients and develop care options neuralink has come up with a next generation neuroprosthetic which intricately interfaces with thousands of neural pathways in the brain their process allows a chip roughly the size of a quarter to be inserted in the place of a chunk of a skull by a precision surgical robot to avoid accidental injury digital consultant apps use ai to give medical consultation based on personal medical history and common medical knowledge users report their symptoms into the app which uses speech recognition to compare against a database of illnesses babylon then offers a recommended action taking into account the user s medical history entrepreneurs in healthcare have been effectively using seven business model archetypes to take ai solution to the marketplace these archetypes depend on the value generated for the target user e g patient focus vs healthcare provider and payer focus and value capturing mechanisms e g providing information or connecting stakeholders iflytek launched a service robot xiao man which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas it also works in the field of medical imaging similar robots are also being made by companies such as ubtech cruzr and softbank robotics pepper the indian startup haptik recently developed a whatsapp chatbot which answers questions associated with the deadly coronavirus in india similarly a software platform chatbot in partnership with medtech startup infermedica launched covid 19 risk assessment chatbot with the market for ai expanding constantly large tech companies such as apple google amazon and baidu all have their own ai research divisions as well as millions of dollars allocated for acquisition of smaller ai based companies many automobile manufacturers are beginning to use machine learning healthcare in their cars as well companies such as bmw ge tesla toyota and volvo all have new research campaigns to find ways of learning a driver s vital statistics to ensure they are awake paying attention to the road and not under the influence of substances expanding care to developing nations artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public many new technology companies such as spacex and the raspberry pi foundation have enabled more developing countries to have access to computers and the internet than ever before with the increasing capabilities of ai over the internet advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life threatening disease or not using ai in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care ai can allow for not only diagnosis of patient in areas where healthcare is scarce but also allow for a good patient experience by resourcing files to find the best treatment for a patient the ability of ai to adjust course as it goes also allows the patient to have their treatment modified based on what works for them a level of individualized care that is nearly non existent in developing countries regulation while research on the use of ai in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption its use may nonetheless introduce several new types of risk to patients and healthcare providers such as algorithmic bias do not resuscitate implications and other machine morality issues ai may also compromise the protection of patients rights such as the right to informed consent and the right to medical data protection these challenges of the clinical use of ai have brought about a potential need for regulations ai studies need to be completely and transparently reported to have value to inform regulatory approval depending on the phase of study international consensus based reporting guidelines tripod ai decide ai consort ai have been developed to provide recommendations on the key details that need to be reported currently there are regulations pertaining to the collection of patient data this includes policies such as the health insurance portability and accountability act hipaa and the european general data protection regulation gdpr the gdpr pertains to patients within the eu and details the consent requirements for patient data use when entities collect patient healthcare data similarly hipaa protects healthcare data from patient records in the united states in may 2016 the white house announced its plan to host a series of workshops and formation of the national science and technology council nstc subcommittee on machine learning and artificial intelligence in october 2016 the group published the national artificial intelligence research and development strategic plan outlining its proposed priorities for federally funded ai research and development within government and academia the report notes a strategic r d plan for the subfield of health information technology is in development stages there is concern that large language models can overwhelm people with both accurate health information and also misinformation leading to potential challenges in public health this calls for the need for policy and user guidance related to health information through ai united nations who itu the joint itu who focus group on artificial intelligence for health fg ai4h has built a platform known as the itu who ai for health framework for the testing and benchmarking of ai applications in health domain as of november 2018 eight use cases are being benchmarked including assessing breast cancer risk from histopathological imagery guiding anti venom selection from snake images and diagnosing skin lesions us fda in january 2021 the us fda published a new action plan entitled artificial intelligence ai machine learning ml based software as a medical device samd action plan this plan lays out the fda s future plans for regulation of medical devices that would include artificial intelligence in their software there are five main actions the fda plans to take to increase regulation 1 tailored regulatory framework for ai m based samd 2 good machine learning practice gmlp 3 patient centered approach incorporating transparency to users 4 regulatory science methods related to algorithm bias robustness and 5 real world performance rwp this plan was in direct response to stakeholders feedback on a 2019 discussion paper also published by the fda according to the u s department of health and human services the office for civil rights ocr has issued guidance on the ethical use of ai in healthcare the guidance outlines four core ethical principles that must be followed respect for autonomy beneficence non maleficence and justice respect for autonomy requires that individuals have control over their own data and decisions beneficence requires that ai be used to do good such as improving the quality of care and reducing health disparities non maleficence requires that ai be used to do no harm such as avoiding discrimination in decisions finally justice requires that ai be used fairly such as using the same standards for decisions no matter a person s race gender or income level moreover as of march 2021 the ocr hired a chief artificial intelligence officer ocaio to pursue the implementation of the hhs ai strategy the ocr also has issued rules and regulations to protect the privacy of individuals health information these regulations require healthcare providers to follow certain privacy rules when using ai the ocr also requires healthcare providers to keep a record of how they use ai and to ensure that their ai systems are secure overall the u s has taken steps to protect individuals privacy and ethical issues related to ai in healthcare the u s is not the only country to develop or initiate regulations of data privacy with ai other countries have implemented data protection regulations more specifically with company privacy invasions in denmark the danish expert group on data ethics has adopted recommendations on data for the benefit of the people these recommendations are intended to encourage the responsible use of data in the business sector with a focus on data processing the recommendations include a focus on equality and non discrimination with regard to bias in ai as well as human dignity the importance of human dignity is stressed as it is said to outweigh profit and must be respected in all data processes the european union has implemented the general data protection regulation gdpr to protect citizens personal data which applies to the use of ai in healthcare in addition the european commission has established guidelines to ensure the ethical development of ai including the use of algorithms to ensure fairness and transparency with gdpr the european union was the first to regulate ai through data protection legislation the union finds privacy as a fundamental human right it wants to prevent unconsented and secondary uses of data by private or public health facilities by streamlining access to personal data for health research and findings they are able to instate the right and importance of patient privacy in the united states the health insurance portability and accountability act hipaa requires organizations to protect the privacy and security of patient information the centers for medicare and medicaid services have also released guidelines for the development of ai based medical applications ethical concerns data collection in order to effectively train machine learning and use ai in healthcare massive amounts of data must be gathered acquiring this data however comes at the cost of patient privacy in most cases and is not well received publicly for example a survey conducted in the uk estimated that 63 of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology the scarcity of real accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare furthermore the lack of current regulations surrounding ai in the united states has generated concerns about mismanagement of patient data such as with corporations utilizing patient data for financial gain for example roche a swiss healthcare company was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of 1 9 billion naturally this generates questions of ethical concern is there a monetary price that can be set for data and should it depend on its perceived value or contributions to science is it fair to patients to sell their data these concerns were addressed in a survey conducted by the pew research center in 2022 that asked americans for their opinions about the increased presence of ai in their daily lives and the survey estimated that 37 of americans were more concerned than excited about such increased presence with 8 of participants specifically associating their concern with people misusing ai ultimately the current potential of artificial intelligence in healthcare is additionally hindered by concerns about mismanagement of data collected especially in the united states automation a systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals patients and the general public doubted that care involving ai could be empathetic according to a 2019 study ai can replace up to 35 of jobs in the uk within the next 10 to 20 years however of these jobs it was concluded that ai has not eliminated any healthcare jobs so far though if ai were to automate healthcare related jobs the jobs most susceptible to automation would be those dealing with digital information radiology and pathology as opposed to those dealing with doctor to patient interaction automation can provide benefits alongside doctors as well some believe that ai may avert healthcare worker burnout and cognitive overload so that doctors who take advantage of ai in healthcare will provide greater quality healthcare than doctors and medical establishments who do not recently there have been many discussions between healthcare experts in terms of ai and elder care in relation to elder care ai bots have been helpful in guiding older residents living in assisted living with entertainment and company these bots are allowing staff in the home to have more one on one time with each resident but the bots are also programmed with more ability in what they are able to do such as knowing different languages and different types of care depending on the patient s conditions the bot is an ai machine which means it goes through the same training as any other machine using algorithms to parse the given data learn from it and predict the outcome in relation to what situation is at hand bias since ai makes decisions solely on the data it receives as input it is important that this data represents accurate patient demographics in a hospital setting patients do not have full knowledge of how predictive algorithms are created or calibrated therefore these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care a recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when ai applications are developed using many to many mapping there can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities since ai s decisions are a direct reflection of its input data the data it receives must have accurate representation of patient demographics white males are overly represented in medical data sets therefore having minimal patient data on minorities can lead to ai making more accurate predictions for majority populations leading to unintended worse medical outcomes for minority populations collecting data from minority communities can also lead to medical discrimination for instance hiv is a prevalent virus among minority communities and hiv status can be used to discriminate against patients in addition to biases that may arise from sample selection different clinical systems used to collect data may also impact ai functionality for example radiographic systems and their outcomes e g resolution vary by provider moreover clinician work practices such as the positioning of the patient for radiography can also greatly influence the data and make comparability difficult however these biases are able to be eliminated through careful implementation and a methodical collection of representative data a final source of bias which has been called label choice bias arises when proxy measures are used to train algorithms that build in bias against certain groups for example a widely used algorithm predicted health care costs as a proxy for health care needs and used predictions to allocate resources to help patients with complex health needs this introduced bias because black patients have lower costs even when they are just as unhealthy as white patients solutions to the label choice bias aim to match the actual target what the algorithm is predicting more closely to the ideal target what researchers want the algorithm to predict so for the prior example instead of predicting cost researchers would focus on the variable of healthcare needs which is rather more significant adjusting the target led to almost double the number of black patients being selected for the program history research in the 1960s and 1970s produced the first problem solving program or expert system known as dendral while it was designed for applications in organic chemistry it provided the basis for a subsequent system mycin considered one of the most significant early uses of artificial intelligence in medicine mycin and other systems such as internist 1 and casnet did not achieve routine use by practitioners however the 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity during this time there was a recognition by researchers and developers that ai systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians approaches involving fuzzy set theory bayesian networks and artificial neural networks have been applied to intelligent computing systems in healthcare medical and technological advancements occurring over this half century period that have enabled the growth of healthcare related applications of ai to include improvements in computing power resulting in faster data collection and data processing growth of genomic sequencing databases widespread implementation of electronic health record systems improvements in natural language processing and computer vision enabling machines to replicate human perceptual processes enhanced the precision of robot assisted surgery increased tree based machine learning models that allow flexibility in establishing health predictors improvements in deep learning techniques and data logs for rare diseases see also references further reading ",
            "total_words": 6617,
            "unique_words_percentage": 27.308447937131632,
            "stopwords_percentage": 37.69079643342905
        },
        {
            "title": "Applications of artificial intelligence",
            "link": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
            "content": "artificial intelligence ai has been used in applications throughout industry and academia in a manner analogous to electricity or computers ai serves as a general purpose technology ai programes emulate perception and understanding and are designed to adapt to new information and new situations machine learning has been used for various scientific and commercial purposes including language translation image recognition decision making credit scoring and e commerce internet and e commerce web feeds and posts machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds various types of social media analysis also make use of machine learning and there is research into its use for semi automated tagging enhancement correction of online misinformation and related filter bubbles ai has been used to customize shopping options and personalize offers online gambling companies have used ai for targeting gamblers virtual assistants and search intelligent personal assistants use ai to understand many natural language requests in other ways than rudimentary commands common examples are apple s siri amazon s alexa and a more recent ai chatgpt by openai bing chat has used artificial intelligence as part of its search engine spam filtering machine learning can be used to combat spam scams and phishing it can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements some models built via machine learning algorithms have over 90 accuracy in distinguishing between spam and legitimate emails these models can be refined using new data and evolving spam tactics machine learning also analyzes traits such as sender behavior email header information and attachment types potentially enhancing spam detection language translation speech translation technology attempts to convert one language s spoken words into another language this potentially reduces language barriers in global commerce and cross cultural exchange enabling speakers of various languages to communicate with one another ai has been used to automatically translate spoken language and textual content in products such as microsoft translator google translate and deepl translator additionally research and development are in progress to decode and conduct animal communication meaning is conveyed not only by text but also through usage and context see semantics and pragmatics as a result the two primary categorization approaches for machine translations are statistical and neural machine translations nmts the old method of performing translation was to use a statistical machine translation smt methodology to forecast the best probable output with specific algorithms however with nmt the approach employs dynamic algorithms to achieve better translations based on context facial recognition and image labeling ai has been used in facial recognition systems some examples are apple s face id and android s face unlock which are used to secure mobile devices image labeling has been used by google image labeler to detect products in photos and to allow people to search based on a photo image labeling has also been demonstrated to generate speech to describe images to blind people facebook s deepface identifies human faces in digital images games and entertainment games have been a major application of ai s capabilities since the 1950s in the 21st century ais have beaten human players in many games including chess deep blue jeopardy watson go alphago poker pluribus and cepheus e sports starcraft and general game playing alphazero and muzero kuki ai is a set of chatbots and other apps which were designed for entertainment and as a marketing tool character ai is another example of a chatbot being used for recreation economic and social challenges ai for good is a platform launched in 2017 by the international telecommunication union itu agency of the united nations un the goal of the platform is to use ai to help achieve the un s sustainable development goals the university of southern california launched the center for artificial intelligence in society with the goal of using ai to address problems such as homelessness stanford researchers use ai to analyze satellite images to identify high poverty areas agriculture in agriculture ai has been proposed as a way for farmers to identify areas that need irrigation fertilization or pesticide treatments to increase yields thereby improving efficiency ai has been used to attempt to classify livestock pig call emotions automate greenhouses detect diseases and pests and optimize irrigation cyber security cyber security companies are adopting neural networks machine learning and natural language processing to improve their systems applications of ai in cyber security include network protection machine learning improves intrusion detection systems by broadening the search beyond previously identified threats endpoint protection attacks such as ransomware can be thwarted by learning typical malware behaviors ai related cyber security application cases vary in both benefit and complexity security features such as security orchestration automation and response soar and extended endpoint detection and response xdr offer significant benefits for businesses but require significant integration and adaptation efforts application security can help counterattacks such as server side request forgery sql injection cross site scripting and distributed denial of service ai technology can also be utilized to improve system security and safeguard our privacy randrianasolo 2012 suggested a security system based on artificial intelligence that can recognize intrusions and adapt to perform better in order to improve cloud computing security sahil 2015 created a user profile system for the cloud environment with ai techniques suspect user behavior machine learning can identify fraud or compromised applications as they occur education ai elevates teaching focusing on significant issues like the knowledge nexus and educational equality the evolution of ai in education and technology should be used to improve human capabilities in relationships where they do not replace humans unesco recognizes the future of ai in education as an instrument to reach sustainable development goal 4 called inclusive and equitable quality education the world economic forum also stresses ai s contribution to students overall improvement and transforming teaching into a more enjoyable process personalized learning ai driven tutoring systems such as khan academy duolingo and carnegie learning are the forefoot of delivering personalized education these platforms leverage ai algorithms to analyze individual learning patterns strengths and weaknesses enabling the customization of content and algorithm to suit each student s pace and style of learning administrative efficiency in educational institutions ai is increasingly used to automate routine tasks like attendance tracking grading and marking which allows educators to devote more time to interactive teaching and direct student engagement furthermore ai tools are employed to monitor student progress analyze learning behaviors and predict academic challenges facilitating timely and proactive interventions for students who may be at risk of falling behind ethical and privacy concerns despite the benefits the integration of ai in education raises significant ethical and privacy concerns particularly regarding the handling of sensitive student data it is imperative that ai systems in education are designed and operated with a strong emphasis on transparency security and respect for privacy to maintain trust and uphold the integrity of educational practices much of the regulation will be influenced by the ai act the world s first comprehensive ai law finance financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm flagging these for human investigation the use of ai in banking began in 1987 when security pacific national bank launched a fraud prevention task force to counter the unauthorized use of debit cards kasisto and moneystream use ai banks use ai to organize operations for bookkeeping investing in stocks and managing properties ai can adapt to changes during non business hours ai is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies the use of ai in applications such as online trading and decision making has changed major economic theories for example ai based buying and selling platforms estimate personalized demand and supply curves thus enabling individualized pricing ai systems reduce information asymmetry in the market and thus make markets more efficient the application of artificial intelligence in the financial industry can alleviate the financing constraints of non state owned enterprises especially for smaller and more innovative enterprises trading and investment algorithmic trading involves the use of ai systems to make trading decisions at speeds orders of magnitude greater than any human is capable of making millions of trades in a day without human intervention such high frequency trading represents a fast growing sector many banks funds and proprietary trading firms now have entire portfolios that are ai managed automated trading systems are typically used by large institutional investors but include smaller firms trading with their own ai systems large financial institutions use ai to assist with their investment practices blackrock s ai engine aladdin is used both within the company and by clients to help with investment decisions its functions include the use of natural language processing to analyze text such as news broker reports and social media feeds it then gauges the sentiment on the companies mentioned and assigns a score banks such as ubs and deutsche bank use sqreem sequential quantum reduction and extraction model to mine data to develop consumer profiles and match them with wealth management products underwriting online lender upstart uses machine learning for underwriting zestfinance s zest automated machine learning zaml platform is used for credit underwriting this platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers the platform is particularly useful to assign credit scores to those with limited credit histories audit ai makes continuous auditing possible potential benefits include reducing audit risk increasing the level of assurance and reducing audit duration continuous auditing with ai allows real time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making anti money laundering ai software such as laundrograph which uses contemporary suboptimal datasets could be used for anti money laundering aml history in the 1980s ai started to become prominent in finance as expert systems were commercialized for example dupont created 100 expert systems which helped them to save almost 10 million per year one of the first systems was the pro trader expert system that predicted the 87 point drop in the dow jones industrial average in 1986 the major junctions of the system were to monitor premiums in the market determine the optimum investment strategy execute transactions when appropriate and modify the knowledge base through a learning mechanism one of the first expert systems to help with financial plans was planpowerm and client profiling system created by applied expert systems apex it was launched in 1986 it helped create personal financial plans for people in the 1990s ai was applied to fraud detection in 1993 fincen artificial intelligence system fais launched it was able to review over 200 000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to 1 billion these expert systems were later replaced by machine learning systems ai can enhance entrepreneurial activity and ai is one of the most dynamic areas for start ups with significant venture capital flowing into ai government ai facial recognition systems are used for mass surveillance notably in china in 2019 bengaluru india deployed ai managed traffic signals this system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic military various countries are deploying ai military applications the main applications enhance command and control communications sensors integration and interoperability research is targeting intelligence collection and analysis logistics cyber operations information operations and semiautonomous and autonomous vehicles ai technologies enable coordination of sensors and effectors threat detection and identification marking of enemy positions target acquisition coordination and deconfliction of distributed joint fires between networked combat vehicles involving manned and unmanned teams ai has been used in military operations in iraq syria israel and ukraine health healthcare ai in healthcare is often used for classification to evaluate a ct scan or electrocardiogram or to identify high risk patients for population health ai is helping with the high cost problem of dosing one study suggested that ai could save 16 billion in 2016 a study reported that an ai derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients current research has indicated that non cardiac vascular illnesses are also being treated with artificial intelligence ai for certain disorders ai algorithms can aid in diagnosis recommended treatments outcome prediction and patient progress tracking as ai technology advances it is anticipated that it will become more significant in the healthcare industry the early detection of diseases like cancer is made possible by ai algorithms which diagnose diseases by analyzing complex sets of medical data for example the ibm watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem microsoft s ai project hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines its goal is to memorize all the relevant papers to predict which combinations of drugs will be most effective for each patient myeloid leukemia is one target another study reported on an ai that was as good as doctors in identifying skin cancers another project monitors multiple high risk patients by asking each patient questions based on data acquired from doctor patient interactions in one study done with transfer learning an ai diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals another study demonstrated surgery with an autonomous robot the team supervised the robot while it performed soft tissue surgery stitching together a pig s bowel judged better than a surgeon artificial neural networks are used as clinical decision support systems for medical diagnosis such as in concept processing technology in emr software other healthcare tasks thought suitable for an ai that are in development include screening heart sound analysis companion robots for elder care medical record analysis treatment plan design medication management assisting blind people consultations drug creation e g by identifying candidate drugs and by using existing drug screening data such as in life extension research clinical training outcome prediction for surgical procedures hiv prognosis identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics based fingerprints including pandemic pathogens helping link genes to their functions otherwise analyzing genes and identification of novel biological targets help development of biomarkers help tailor therapies to individuals in personalized medicine precision medicine workplace health and safety ai enabled chatbots decrease the need for humans to perform basic call center tasks machine learning in sentiment analysis can spot fatigue in order to prevent overwork similarly decision support systems can prevent industrial disasters and make disaster response more efficient for manual workers in material handling predictive analytics may be used to reduce musculoskeletal injury data collected from wearable sensors can improve workplace health surveillance risk assessment and research ai can auto code workers compensation claims ai enabled virtual reality systems can enhance safety training for hazard recognition ai can more efficiently detect accident near misses which are important in reducing accident rates but are often underreported biochemistry alphafold 2 can determine the 3d structure of a folded protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science more than 200 million chemistry and biology machine learning has been used for drug design it has also been used for predicting molecular properties and exploring large chemical reaction spaces computer planned syntheses via computational reaction networks described as a platform that combines computational synthesis with ai algorithms to predict molecular properties have been used to explore the origins of life on earth drug syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals chemical synthesis design there is research about which types of computer aided chemistry would benefit from machine learning it can also be used for drug discovery and development drug repurposing improving pharmaceutical productivity and clinical trials it has been used for the design of proteins with prespecified functional sites it has been used with databases for the development of a 46 day process to design synthesize and test a drug which inhibits enzymes of a particular gene ddr1 ddr1 is involved in cancers and fibrosis which is one reason for the high quality datasets that enabled these results there are various types of applications for machine learning in decoding human biology such as helping to map gene expression patterns to functional activation patterns or identifying functional dna motifs it is widely used in genetic research there also is some use of machine learning in synthetic biology disease biology nanotechnology e g nanostructured materials and bionanotechnology and materials science novel types of machine learning there are also prototype robot scientists including robot embodied ones like the two robot scientists which show a form of machine learning not commonly associated with the term similarly there is research and development of biological wetware computers that can learn e g for use as biosensors and or implantation into an organism s body e g for use to control prosthetics polymer based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components moreover if whole brain emulation is possible via both scanning and replicating the at least bio chemical brain as premised in the form of digital replication in the age of em possibly using physical neural networks that may have applications as or more extensive than e g valued human activities and may imply that society would face substantial moral choices societal risks and ethical problems such as whether and how such are built sent through space and used compared to potentially competing e g potentially more synthetic and or less human and or non less sentient types of artificial semi artificial intelligence an alternative or additive approach to scanning are types of reverse engineering of the brain a subcategory of artificial intelligence is embodied some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world digital ghosts biological computing in ai and as ai however biological computers even if both highly artificial and intelligent are typically distinguished from synthetic often silicon based computers they could however be combined or used for the design of either moreover many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent understood bias free apparently effective and goal aligned and its trained data sufficiently large and cleansed such as in cases were the underlying or available metrics values or data are inappropriate computer aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as ai for narrow tasks or making use of such without substantially relying on its results see also human in the loop a study described the biological as a limitation of ai with as long as the biological system cannot be understood formalized and imitated we will not be able to develop technologies that can mimic it and that if it was understood this does not mean there being a technological solution to imitate natural intelligence technologies that integrate biology and are often ai based include biorobotics astronomy space activities and ufology artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications mainly for classification regression clustering forecasting generation discovery and the development of new scientific insights for example for discovering exoplanets forecasting solar activity and distinguishing between signals and instrumental effects in gravitational wave astronomy it could also be used for activities in space such as space exploration including analysis of data from space missions real time science decisions of spacecraft space debris avoidance and more autonomous operation in the search for extraterrestrial intelligence seti machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data such as real time observations and other technosignatures e g via anomaly detection in ufology the skycam 5 project headed by prof hakan kayal and the galileo project headed by avi loeb use machine learning to attempt to detect and classify types of ufos the galileo project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of ai oumuamua like interstellar objects and non manmade artificial satellites machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals such as phosphine possibly detected on venus which could prevent miss assignments and if accuracy is improved be used in future detections and identifications of molecules on other planets other fields of research evidence of general impacts in april 2024 the scientific advice mechanism to the european commission published advice including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research as benefits the evidence review highlighted its role in accelerating research and innovation its capacity to automate workflows enhancing dissemination of scientific work as challenges limitations and risks around transparency reproducibility and interpretability poor performance inaccuracy risk of harm through misuse or unintended use societal concerns including the spread of misinformation and increasing inequalities archaeology history and imaging of sites machine learning can help to restore and attribute ancient texts it can help to index texts for example to enable better and easier searching and classification of fragments artificial intelligence can also be used to investigate genomes to uncover genetic history such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population not neanderthal or denisovan was inferred it can also be used for non invasive and non destructive access to internal structures of archaeological remains physics a deep learning system was reported to learn intuitive physics from visual data of virtual 3d environments based on an unpublished approach inspired by studies of visual cognition in infants other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems future dynamics from video recordings of their behavior in the future it may be possible that such can be used to automate the discovery of physical laws of complex systems materials science ai could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure in november 2023 researchers at google deepmind and lawrence berkeley national laboratory announced that they had developed an ai system known as gnome this system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe gnome employs deep learning techniques to efficiently explore potential material structures achieving a significant increase in the identification of stable inorganic crystal structures the system s predictions were validated through autonomous robotic experiments demonstrating a noteworthy success rate of 71 the data of newly discovered materials is publicly available through the materials project database offering researchers the opportunity to identify materials with desired properties for various applications this development has implications for the future of scientific discovery and the integration of ai in material science research potentially expediting material innovation and reducing costs in product development the use of ai and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds reverse engineering machine learning is used in diverse types of reverse engineering for example machine learning has been used to reverse engineer a composite material part enabling unauthorized production of high quality parts and for quickly understanding the behavior of malware it can be used to reverse engineer artificial intelligence models it can also design components by engaging in a type of reverse engineering of not yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites biological network reverse engineering could model interactions in a human understandable way e g bas on time series data of gene expression levels law legal analysis ai is a mainstay of law related professions algorithms and machine learning do some tasks previously done by entry level lawyers while its use is common it is not expected to replace most work done by lawyers in the near future the electronic discovery industry uses machine learning to reduce manual searching law enforcement and legal proceedings law enforcement has begun using facial recognition systems frs to identify suspects from visual data frs results have proven to be more accurate when compared to eyewitness results furthermore frs has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants compas is a commercial system used by u s courts to assess the likelihood of recidivism one concern relates to algorithmic bias ai programs may become biased after processing data that exhibits bias propublica claims that the average compas assigned recidivism risk level of black defendants is significantly higher than that of white defendants in 2019 the city of hangzhou china established a pilot program artificial intelligence based internet court to adjudicate disputes related to ecommerce and internet related intellectual property claims 124 parties appear before the court via videoconference and ai evaluates the evidence presented and applies relevant legal standards 124 services human resources another application of ai is in human resources ai can screen resumes and rank candidates based on their qualifications predict candidate success in given roles and automate repetitive communication tasks via chatbots job search ai has simplified the recruiting job search process for both recruiters and job seekers according to raj mukherjee from indeed 65 of job searchers search again within 91 days after hire an ai powered engine streamlines the complexity of job hunting by assessing information on job skills salaries and user tendencies matching job seekers to the most relevant positions machine intelligence calculates appropriate wages and highlights resume information for recruiters using nlp which extracts relevant words and phrases from text another application is an ai resume builder that compiles a cv in 5 minutes chatbots assist website visitors and refine workflows online and telephone customer service ai underlies avatars automated online assistants on web pages it can reduce operation and training costs pypestream automated customer service for its mobile application to streamline communication with customers a google app analyzes language and converts speech into text the platform can identify angry customers through their language and respond appropriately amazon uses a chatbot for customer service that can perform tasks like checking the status of an order cancelling orders offering refunds and connecting the customer with a human representative generative ai genai such as chatgpt is increasingly used in business to automate tasks and enhance decision making hospitality in the hospitality industry ai is used to reduce repetitive tasks analyze trends interact with guests and predict customer needs ai hotel services come in the form of a chatbot application virtual voice assistant and service robots media ai applications analyze media content such as movies tv programs advertisement videos or user generated content the solutions often involve computer vision typical scenarios include the analysis of images using object recognition or face recognition techniques or the analysis of video for scene recognizing scenes objects or faces ai based media analysis can facilitate media search the creation of descriptive keywords for content content policy monitoring such as verifying the suitability of content for a particular tv viewing time speech to text for archival or other purposes and the detection of logos products or celebrity faces for ad placement motion interpolation pixel art scaling algorithms image scaling image restoration photo colorization film restoration and video upscaling photo tagging automated species identification such as identifying plants fungi and animals with an app text to image models such as dall e midjourney and stable diffusion image to video text to video such as make a video from meta imagen video and phenaki from google text to music with ai models such as musiclm text to speech such as elevenlabs and 15 ai motion capture make image transparent deep fakes deep fakes can be used for comedic purposes but are better known for fake news and hoaxes deepfakes can portray individuals in harmful or compromising situations causing significant reputational damage and emotional distress especially when the content is defamatory or violates personal ethics while defamation and false light laws offer some recourse their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof in january 2016 the horizon 2020 program financed the invid project to help journalists and researchers detect fake documents made available as browser plugins in june 2016 the visual computing group of the technical university of munich and from stanford university developed face2face a program that animates photographs of faces mimicking the facial expressions of another person the technology has been demonstrated animating the faces of people including barack obama and vladimir putin other methods have been demonstrated based on deep neural networks from which the name deep fake was taken in september 2018 u s senator mark warner proposed to penalize social media companies that allow sharing of deep fake documents on their platforms in 2018 darius afchar and vincent nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames darpa gave 68 million dollars to work on deep fake detection audio deepfakes and ai software capable of detecting deep fakes and cloning human voices have been developed respeecher is a program that enables one person to speak with the voice of another video surveillance analysis and manipulated media detection ai algorithms have been used to detect deepfake videos video production artificial intelligence is also starting to be used in video production with tools and software being developed that utilize generative ai in order to create new video or alter existing video some of the major tools that are being used in these processes currently are dall e mid journey and runway way mark studios utilized the tools offered by both dall e and mid journey to create a fully ai generated film called the frost in the summer of 2023 way mark studios is experimenting with using these ai tools to generate advertisements and commercials for companies in mere seconds yves bergquist a director of the ai neuroscience in media project at usc s entertainment technology center says post production crews in hollywood are already using generative ai and predicts that in the future more companies will embrace this new technology music ai has been used to compose music of various genres david cope created an ai called emily howell that managed to become well known in the field of algorithmic computer music the algorithm behind emily howell is registered as a us patent in 2012 ai iamus created the first complete classical album aiva artificial intelligence virtual artist composes symphonic music mainly classical music for film scores it achieved a world first by becoming the first virtual composer to be recognized by a musical professional association melomics creates computer generated music for stress and pain relief at sony csl research laboratory the flow machines software creates pop songs by learning music styles from a huge database of songs it can compose in multiple styles the watson beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style the software was open sourced and musicians such as taryn southern collaborated with the project to create music south korean singer hayeon s debut song eyes on you was composed using ai which was supervised by real composers including nuvo writing and reporting narrative science sells computer generated news and reports it summarizes sporting events based on statistical data from the game it also creates financial reports and real estate analyses automated insights generates personalized recaps and previews for yahoo sports fantasy football yseop uses ai to turn structured data into natural language comments and recommendations yseop writes financial reports executive summaries personalized sales or marketing documents and more in multiple languages including english spanish french and german talespin made up stories similar to the fables of aesop the program started with a set of characters who wanted to achieve certain goals the story narrated their attempts to satisfy these goals mark riedl and vadim bulitko asserted that the essence of storytelling was experience management or how to balance the need for a coherent story progression with user agency which is often at odds while ai storytelling focuses on story generation character and plot story communication also received attention in 2002 researchers developed an architectural framework for narrative prose generation they faithfully reproduced text variety and complexity on stories such as little red riding hood in 2016 a japanese ai co wrote a short story and almost won a literary prize south korean company hanteo global uses a journalism bot to write articles literary authors are also exploring uses of ai an example is david jhave johnston s work rerites 2017 2019 where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications sports writing in 2010 artificial intelligence used baseball statistics to automatically generate news articles this was launched by the big ten network using software from narrative science after being unable to cover every minor league baseball game with a large team associated press collaborated with automated insights in 2016 to create game recaps that were automated by artificial intelligence uol in brazil expanded the use of ai in its writing rather than just generating news stories they programmed the ai to include commonly searched words on google el pais a spanish news site that covers many things including sports allows users to make comments on each news article they use the perspective api to moderate these comments and if the software deems a comment to contain toxic language the commenter must modify it in order to publish it a local dutch media group used ai to create automatic coverage of amateur soccer set to cover 60 000 games in just a single season ndc partnered with united robots to create this algorithm and cover what would have never been possible before without an extremely large team lede ai has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper this was met with significant criticism from readers for the very robotic diction that was published with some descriptions of games being a close encounter of the athletic kind readers were not pleased and let the publishing company gannett know on social media gannett has since halted their used of lede ai until they come up with a solution for what they call an experiment wikipedia millions of its articles have been edited by bots which however are usually not artificial intelligence software many ai platforms use wikipedia data mainly for training machine learning applications there is research and development of various artificial intelligence applications for wikipedia such as for identifying outdated sentences detecting covert vandalism or recommending articles and tasks to new editors machine translation see above has also be used for translating wikipedia articles and could play a larger role in creating updating expanding and generally improving articles in the future a content translation tool allows editors of some wikipedias to more easily translate articles across several select languages video games in video games ai is routinely used to generate behavior in non player characters npcs in addition ai is used for pathfinding some researchers consider npc ai in games to be a solved problem for most production tasks games with less typical ai include the ai director of left 4 dead 2008 and the neuroevolutionary training of platoons in supreme commander 2 2010 ai is also used in alien isolation 2014 as a way to control the actions the alien will perform next kinect which provides a 3d body motion interface for the xbox 360 and the xbox one uses algorithms that emerged from ai research art ai has been used to produce visual art the first ai art program called aaron was developed by harold cohen in 1968 with the goal of being able to code the act of drawing it started by creating simple black and white drawings and later to painting using special brushes and dyes that were chosen by the program itself without mediation from cohen ai platforms such as dall e stable diffusion imagen and midjourney have been used for generating visual images from inputs such as text or other images some ai tools allow users to input images and output changed versions of that image such as to display an object or product in different environments ai image models can also attempt to replicate the specific styles of artists and can add visual complexity to rough sketches since their design in 2014 generative adversarial networks gans have been used by ai artists gan computer programming generates technical images through machine learning frameworks that surpass the need for human operators examples of gan programs that generate art include artbreeder and deepdream art analysis in addition to the creation of original art research methods that utilize ai have been generated to quantitatively analyze digital art collections although the main goal of the large scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections the use of ai in analyzing them has brought about new research perspectives two computational methods close reading and distant viewing are the typical approaches used to analyze digitized art while distant viewing includes the analysis of large collections close reading involves one piece of artwork computer animation ai has been in use since the early 2000s most notably by a system designed by pixar called genesis it was designed to learn algorithms and create 3d models for its characters and props notable movies that used this technology included up and the good dinosaur ai has been used less ceremoniously in recent years in 2023 it was revealed netflix of japan was using ai to generate background images for their upcoming show to be met with backlash online in recent years motion capture became an easily accessible form of ai animation for example move ai is a program built to capture any human movement and reanimate it in its animation program using learning ai utilities energy system power electronics converters are used in renewable energy energy storage electric vehicles and high voltage direct current transmission these converters are failure prone which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications ai can guide the design process for reliable power electronics converters by calculating exact design parameters that ensure the required lifetime the u s department of energy underscores ai s pivotal role in realizing national climate goals with ai the ambitious target of achieving net zero greenhouse gas emissions across the economy becomes feasible ai also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability machine learning can be used for energy consumption prediction and scheduling e g to help with renewable energy intermittency management see also smart grid and climate change mitigation in the power grid telecommunications many telecommunications companies make use of heuristic search to manage their workforces for example bt group deployed heuristic search in an application that schedules 20 000 engineers machine learning is also used for speech recognition sr including of voice controlled devices and sr related transcription including of videos manufacturing sensors artificial intelligence has been combined with digital spectrometry by ideacuria inc enable applications such as at home water quality monitoring toys and games in the 1990s early artificial intelligence tools controlled tamagotchis and giga pets the internet and the first widely released robot furby aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy mattel created an assortment of ai enabled toys that understand conversations give intelligent responses and learn oil and gas oil and gas companies have used artificial intelligence tools to automate functions foresee equipment issues and increase oil and gas output transport automotive ai in transport is expected to provide safe efficient and reliable transportation while minimizing the impact on the environment and communities the major development challenge is the complexity of transportation systems that involves independent components and parties with potentially conflicting objectives ai based fuzzy logic controllers operate gearboxes for example the 2006 audi tt vw touareg and vw caravell feature the dsp transmission a number of škoda variants škoda fabia include a fuzzy logic based controller cars have ai based driver assist features such as self parking and adaptive cruise control there are also prototypes of autonomous automotive public transport vehicles such as electric mini buses as well as autonomous rail transport in operation there also are prototypes of autonomous delivery vehicles sometimes including delivery robots transportation s complexity means that in most cases training an ai in a real world driving environment is impractical simulator based testing can reduce the risks of on road training ai underpins self driving vehicles companies involved with ai include tesla waymo and general motors ai based systems control functions such as braking lane changing collision prevention navigation and mapping autonomous trucks are in the testing phase the uk government passed legislation to begin testing of autonomous truck platoons in 2018 a group of autonomous trucks follow closely behind each other german corporation daimler is testing its freightliner inspiration autonomous vehicles require accurate maps to be able to navigate between destinations some autonomous vehicles do not allow human drivers they have no steering wheels or pedals traffic management ai has been used to optimize traffic management which reduces wait times energy use and emissions by as much as 25 percent smart traffic lights have been developed at carnegie mellon since 2009 professor stephen smith has started a company since then surtrac that has installed smart traffic control systems in 22 cities it costs about 20 000 per intersection to install drive time has been reduced by 25 and traffic jam waiting time has been reduced by 40 at the intersections it has been installed military the royal australian air force raaf air operations division aod uses ai for expert systems ais operate as surrogate operators for combat and training simulators mission management aids support systems for tactical decision making and post processing of the simulator data into symbolic summaries aircraft simulators use ai for training aviators flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft air combat can also be simulated ai can also be used to operate planes analogously to their control of ground vehicles autonomous drones can fly independently or in swarms aod uses the interactive fault diagnosis and isolation system or ifdis which is a rule based expert system using information from tf 30 documents and expert advice from mechanics that work on the tf 30 this system was designed to be used for the development of the tf 30 for the f 111c the system replaced specialized workers the system allowed regular workers to communicate with the system and avoid mistakes miscalculations or having to speak to one of the specialized workers speech recognition allows traffic controllers to give verbal directions to drones artificial intelligence supported design of aircraft or aida is used to help designers in the process of creating conceptual designs of aircraft this program allows the designers to focus more on the design itself and less on the design process the software also allows the user to focus less on the software tools the aida uses rule based systems to compute its data this is a diagram of the arrangement of the aida modules although simple the program is proving effective nasa in 2003 a dryden flight research center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved the software compensated for damaged components by relying on the remaining undamaged components the 2016 intelligent autopilot system combined apprenticeship learning and behavioral cloning whereby the autopilot observed low level actions required to maneuver the airplane and high level strategy used to apply those actions maritime neural networks are used by situational awareness systems in ships and boats there also are autonomous boats environmental monitoring autonomous ships that monitor the ocean ai driven satellite data analysis passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning for example global plastic watch is an ai based satellite monitoring platform for analysis tracking of plastic waste sites to help prevention of plastic pollution primarily ocean pollution by helping identify who and where mismanages plastic waste dumping it into oceans early warning systems machine learning can be used to spot early warning signs of disasters and environmental issues possibly including natural pandemics earthquakes landslides heavy rainfall long term water supply vulnerability tipping points of ecosystem collapse cyanobacterial bloom outbreaks and droughts computer science programming assistance ai powered code assisting tools ai can be used for real time code completion chat and automated test generation these tools are typically integrated with editors and ides as plugins they differ in functionality quality speed and approach to privacy code suggestions could be incorrect and should be carefully reviewed by software developers before accepted github copilot is an artificial intelligence model developed by github and openai that is able to autocomplete code in multiple programming languages price for individuals 10 mo or 100 yr with one free month trial tabnine was created by jacob jackson and was originally owned by tabnine company in late 2019 tabnine was acquired by codota tabnine tool is available as plugin to most popular ides it offers multiple pricing options including limited starter free version codiumai by codiumai small startup in tel aviv offers automated test creation currently supports python js and ts ghostwriter by replit offers code completion and chat they have multiple pricing plans including a free one and a hacker plan for 7 month codewhisperer by amazon collects individual users content including files open in the ide they claim to focus on security both during transmission and when storing individual plan is free professional plan is 19 user month other tools sourcegraph cody codecompletefauxpilot tabby neural network design ai can be used to create other ais for example around november 2017 google s automl project to evolve new neural net topologies created nasnet a system optimized for imagenet and poco f1 nasnet s performance exceeded all previously published performance on imagenet quantum computing machine learning has been used for noise cancelling in quantum technology including quantum sensors moreover there is substantial research and development of using quantum computers with machine learning algorithms for example there is a prototype photonic quantum memristive device for neuromorphic quantum computers nc artificial neural networks and nc using quantum materials with some variety of potential neuromorphic computing related applications and quantum machine learning is a field with some variety of applications under development ai could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for ai applications there may also be some usefulness in chemistry e g for drug discovery and in materials science e g for materials optimization discovery with possible relevance to quantum materials manufacturing historical contributions ai researchers have created many tools to solve the most difficult problems in computer science many of their inventions have been adopted by mainstream computer science and are no longer considered ai all of the following were originally developed in ai laboratories time sharing interactive interpreters graphical user interfaces and the computer mouse rapid application development environments the linked list data structure automatic storage management symbolic programming functional programming dynamic programming object oriented programming optical character recognition constraint satisfaction business content extraction an optical character reader is used in the extraction of data in business documents like invoices and receipts it can also be used in business contract documents e g employment agreements to extract critical data like employment terms delivery terms termination clauses etc architecture ai in architecture has created a way for architects to create things beyond human understanding ai implementation of machine learning text to render technologies like dall e and stable diffusion gives power to visualization complex ai allows designers to demonstrate their creativity and even invent new ideas while designing in future ai will not replace architects instead it will improve the speed of translating ideas sketching list of applications see also applications of artificial intelligence to legal informatics applications of deep learning applications of machine learning artificial intelligence and elections collective intelligence applications list of artificial intelligence projects list of datasets for machine learning research open data progress in artificial intelligence timeline of computing 2020 present footnotes further reading kaplan a m haenlein m 2018 siri siri in my hand who s the fairest in the land on the interpretations illustrations and implications of artificial intelligence business horizons 62 1 15 25 doi 10 1016 j bushor 2018 08 004 s2cid 158433736 kurzweil ray 2005 the singularity is near when humans transcend biology new york viking isbn 978 0 670 03384 3 national research council 1999 developments in artificial intelligence funding a revolution government support for computing research national academy press isbn 978 0 309 06278 7 oclc 246584055 moghaddam m j soleymani m r farsi m a 2015 sequence planning for stamping operations in progressive dies journal of intelligent manufacturing 26 2 347 357 doi 10 1007 s10845 013 0788 0 s2cid 7843287 felten ed 3 may 2016 preparing for the future of artificial intelligence ",
            "total_words": 8372,
            "unique_words_percentage": 30.422838031533683,
            "stopwords_percentage": 32.89536550406116
        },
        {
            "title": "Artificial intelligence rhetoric",
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence_rhetoric",
            "content": "artificial intelligence rhetoric or ai rhetoric is a term primarily applied to persuasive text and speech generated by chatbots using generative artificial intelligence although the term can also apply to the language that humans type or speak when communicating with a chatbot this emerging field of rhetoric scholarship is related to the fields of digital rhetoric and human computer interaction description persuasive text and persuasive digital speech can be examined as ai rhetoric when the text or speech is a product or output of advanced machines that mimic human communication in some way historical examples of fictional artificial intelligence capable of speech are portrayed in mythology folk tales and science fiction modern computer technology from the mid 20th century began producing what can be studied as real world examples of ai rhetoric with programs like joseph weizenbaum s eliza while chatbot development in the 1990s further enhanced a foundation for texts produced by generative ai programs of the 21st century from an additional perspective ai rhetoric may be understood as the natural language humans use either typewritten or spoken to prompt and direct ai technologies in persuasive ways as opposed to traditional computer coding this is closely related to the concepts of prompt engineering and prompt hacking history while much of the research related to artificial intelligence was historically conducted by computer scientists experts across a wide range of subjects such as cognitive science philosophy languages and cultural studies have contributed to a more robust understanding of ai for decades the advent of 21st century ai technologies like chatgpt generated a swell of interest from the arts and humanities generative ai technology and chatbots gained notoriety and rapid widespread use in the 2020s questions and theories about the power of machines computers and robots to persuasively communicate date back to the very beginnings of computer development more than a decade before the first computer language programs were created and tested in 1950 alan turing imagined a scenario called the imitation game where a machine using only typewritten communication might be successfully programmed to fool a human reader into believing the machine s responses came from a person by the 1960s computer programs using basic natural language processing such as joseph weizenbaum s eliza began to pass turing s test as human research subjects reading the machine s outputs became very hard to convince that eliza is not human future computer language programs would build on weizenbaum s work but the first generation of internet chatbots in the 1990s up to the virtual assistants of the 2010s like apple s siri and amazon s alexa received harsh criticism for their less than humanlike responses and inability to reason in a helpful manner by the late 1980s and early 1990s scholars in the humanities began laying the groundwork for ai rhetoric to become a recognized area of study michael l johnson s mind language machine artificial intelligence in the poststructuralist age argued for the interdisciplinary synthesis necessary to guide an understanding of the relationship between ai and rhetoric lynette hunter professor of the history of rhetoric and performance at the university of california davis published rhetoric and artificial intelligence in 1991 and was among the first to directly apply the lens of rhetoric to ai twenty first century developments in the scholarship of ai rhetoric are outlined in the july 2024 special issue of rhetoric society quarterly which is devoted to rhetoric of with ai special issue editors s scott graham and zoltan p majdik summarize the state of the field when they write rhetorical research related to ai engages all manner of specialty domains because ai now touches on almost all areas of human activity rhetorics of ai can help contribute to longstanding discussions in rhetoric of science rhetoric of health and medicine cultural rhetorics public address writing studies ideological rhetoric and many other areas but studies on the rhetoric of ai can also offer many insights to the broader interdisciplinary study of ai itself 223 4 media coverage since chatgpt s release in 2022 many prominent publications have focused on the uncanny persuasive capabilities of language based generative ai models like chatbots new york times technology columnist kevin roose wrote a viral piece in 2023 about how a microsoft ai named sydney attempted to convince him to leave his wife and he followed up with a 2024 article explaining a new world of a i manipulation where users can rely on creative prompt engineering to influence the outputs of generative ai programs a february 2024 report cited by the journal nature claims to provide the first empirical evidence demonstrating how content generated by artificial intelligence can scale personalized persuasion with only limited information about the message recipient psychology today reported on a 2024 study using the attention grabbing headline ai is becoming more persuasive than humans ai rhetoric in education in addition to ai s rhetorical capabilities gaining attention in the media in the early 2020s many colleges and universities began offering undergraduate graduate and certificate courses in ai prompting and ai rhetoric with titles like stanford s rhetoric of artificial intelligence and robots and the university of florida s the rhetoric of artificial intelligence primary and secondary schools designing and implementing ai literacy curricula also incorporate ai rhetoric concepts into lessons on ai bias and ethical usage of ai see also artificial intelligence and elections digital rhetoric references ",
            "total_words": 899,
            "unique_words_percentage": 45.82869855394883,
            "stopwords_percentage": 36.818687430478306
        },
        {
            "title": "Artificial intelligence art",
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence_art",
            "content": "artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence ai programs artists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded throughout its history artificial intelligence art has raised many philosophical concerns related to the human mind artificial beings and what can be considered art in a human ai collaboration since the 20th century artists have used ai to create art some of which has been exhibited in museums and won awards during the ai boom of the early 2020s text to image models such as midjourney dall e stable diffusion and flux 1 became widely available to the public allowing non artists to quickly generate imagery with little effort commentary about ai art in the 2020s has often focused on issues related to copyright deception defamation and its impact on more traditional artists including technological unemployment opinions have also risen on the possible effect ai generated art might have on creativity history early history the concept of automated art dates back at least to the automata of ancient greek civilization where inventors such as daedalus and hero of alexandria were described as having designed machines capable of writing text generating sounds and playing music early experiments were driven by the idea that computers beyond performing logical operations could generate aesthetically pleasing works offering a new dimension to creativity the tradition of creative automatons has flourished throughout history such as maillardet s automaton created around 1800 and capable of creating multiple drawings and poems stored in its cams the brass disks that hold memory along with this ada lovelace typically known for her work on the analytical engine in her notes begins to conceptualize the idea computing operations could be used to generate music and poems this concept resulted in what is now referred to as the lovelace effect which gives a concrete set of tools to analyze situations where a computer s behavior is viewed by users as creative however lovelace also discusses a concept in her notes that is known as the lovelace objection where she argues that machines have no pretensions whatever to originate anything which is a direct contradiction to the idea of artificial intelligence and creative machines in 1950 with the publication of alan turing s paper computing machinery and intelligence there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly shortly after the academic discipline of artificial intelligence was founded at a research workshop at dartmouth college in 1956 and has experienced several waves of advancement and optimism in the decades since since its founding researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human like intelligence these issues have previously been explored by myth fiction and philosophy since antiquity 1950s to 2000s early implementations since the founding of ai in the 1950s artists and researchers have used artificial intelligence to create artistic works these works were sometimes referred to as algorithmic art computer art digital art or new media art one of the first significant ai art systems is aaron developed by harold cohen beginning in the late 1960s at the university of california at san diego aaron uses a symbolic rule based approach to generate technical images in the era of gofai programming and it was developed by cohen with the goal of being able to code the act of drawing in its earliest form aaron created abstract black and white drawings which would later be finished by cohen painting them throughout the years he also began to develop a way for aaron to paint as well using special brushes and dyes that were chosen by the program itself without mediation from cohen after years of work aaron was exhibited in 1972 at the los angeles county museum of art from 1973 to 1975 cohen refined aaron during a residency at the artificial intelligence laboratory at stanford university in 2024 the whitney museum of american art exhibited ai art from throughout cohen s career including re created versions of his early robotic drawing machines karl sims has exhibited art created with artificial life since the 1980s he received an m s in computer graphics from the mit media lab in 1987 and was artist in residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company thinking machines in both 1991 and 1992 sims won the golden nica award at prix ars electronica for his 3d ai animated videos using artificial evolution in 1997 sims created the interactive installation galápagos for the ntt intercommunication center in tokyo in this installation viewers help evolve 3d animated creatures by selecting which ones will be allowed to live and produce new mutated offspring furthermore sims received an emmy award in 2019 for outstanding achievement in engineering development eric millikin has been creating animated films using artificial intelligence since the 1980s and began posting art on the internet using compuserve in the early 1980s in 1999 scott draves and a team of several engineers created and released electric sheep as a free software screensaver electric sheep is a volunteer computing project for animating and evolving fractal flames which are in turn distributed to the networked computers which display them as a screensaver the screensaver used ai to create an infinite animation by learning from its audience in 2001 draves won the fundacion telefónica life 4 0 prize for electric sheep 2010s deep learning deep learning characterized by its multi layer structure that attempts to mimic the human brain first came about in the 2010s and causing a significant shift in the world of ai art during the deep learning era there are mainly these types of designs for generative art autoregressive models diffusion models gans normalizing flows in 2014 ian goodfellow and colleagues at université de montréal developed the generative adversarial network gan a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images the gan uses a generator to create new images and a discriminator to decide which created images are considered successful unlike previous algorithmic art that followed hand coded rules generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images in 2015 a team at google released deepdream a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia the process creates deliberately over processed images with a dream like appearance reminiscent of a psychedelic experience later in 2017 a conditional gan learned to generate 1000 image classes of imagenet a large visual database designed for use in visual object recognition software research by conditioning the gan on both random noise and a specific class label this approach enhanced the quality of image synthesis for class conditional models autoregressive models were used for image generation such as pixelrnn 2016 which autoregressively generates one pixel after another with a recurrent neural network immediately after the transformer architecture was proposed in attention is all you need 2018 it was used for autoregressive generation of images but without text conditioning in 2018 an auction sale of artificial intelligence art was held at christie s in new york where the ai artwork edmond de belamy a pun on goodfellow s name sold for us 432 500 which was almost 45 times higher than its estimate of us 7 000 10 000 the artwork was created by obvious a paris based collective furthermore the website artbreeder launched in 2018 uses the models stylegan and biggan to allow users to generate and modify images such as faces landscapes and paintings in 2019 stephanie dinkins won the creative capital award for her creation of an evolving artificial intelligence based on the interests and culture s of people of color also in 2019 sougwen chung won the lumen prize for her performances with a robotic arm that uses ai to attempt to draw in a manner similar to chung 2020s text to image and diffusion models in the 2020s text to image models which generate images based on prompts became widely used marking yet another shift in the creation of ai generated artworks in 2021 using the influential large language generative pre trained transformer models that are used in gpt 2 and gpt 3 openai released a series of images created with the text to image ai model dall e 1 it was an autoregressive generative model with essentially the same architecture as gpt 3 along with this later in 2021 eleutherai released the open source vqgan clip based on openai s clip model diffusion models generative models used to create synthetic data based on existing data were first proposed in 2015 but they only became better than gans in early 2021 latent diffusion model was published in december 2021 and became the basis for the later stable diffusion august 2022 in 2022 midjourney was released followed by google brain s imagen and parti which were announced in may 2022 microsoft s nuwa infinity and the source available stable diffusion which was released in august 2022 dall e 2 a successor to dall e was beta tested and released stability ai has a stable diffusion web interface called dreamstudio plugins for krita photoshop blender and gimp and the automatic1111 web based open source user interface stable diffusion s main pre trained model is shared on the hugging face hub in 2023 eric millikin released the dance of the nain rouge a documentary film created using ai deepfake technology about the detroit folklore legend of the nain rouge the film is described as an experimental decolonial detroit demonology deepfake dream dance documentary it was awarded the best innovative technologies award premio migliori tecnologie innovative at the 2024 pisa robot film festival in italy and best animation film at the 2024 absurd film festival in italy ideogram was released in august 2023 this model is known for its ability to generate legible text in 2024 flux was released this model can generate realistic images with consistent results and was integrated into grok the chatbot used on x formerly twitter and le chat the chatbot of mistral ai flux was developed by black forest labs founded by the researchers behind stable diffusion grok later switched to its own text to image model aurora in december 2024 along with this some examples of text to video model models of the mid 2020s are runway s gen 2 google s videopoet and openai s sora which released in december 2024 tools and processes imagery there are many tools available to the artist when working with diffusion models they can define both positive and negative prompts but they are also afforded a choice in using or omitting the use of vaes loras hypernetworks ipadapter and embeddings textual inversions variables including cfg seed steps sampler scheduler denoise upscaler and encoder are sometimes available for adjustment additional influence can be exerted during pre inference by means of noise manipulation while traditional post processing techniques are frequently used post inference artists can also train their own models in addition procedural rule based generation of images using mathematical patterns algorithms that simulate brush strokes and other painted effects and deep learning algorithms such as generative adversarial networks gans and transformers have been developed several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt there also exist programs which transform photos into art like images in the style of well known sets of paintings there are many options ranging from simple consumer facing mobile apps to jupyter notebooks and webuis that require powerful gpus to run effectively additional functionalities include textual inversion which refers to enabling the use of user provided concepts like an object or a style learned from a few images novel art can then be generated from the associated word s the text that has been assigned to the learned often abstract concept and model extensions or fine tuning such as dreambooth impact and applications ai has the potential for a societal transformation which may include enabling the expansion of noncommercial niche genres such as cyberpunk derivatives like solarpunk by amateurs novel entertainment fast prototyping increasing art making accessibility and artistic output per effort and or expenses and or time e g via generating drafts draft refinitions and image components inpainting generated images are sometimes used as sketches low cost experiments inspiration or illustrations of proof of concept stage ideas additional functionalities or improvements may also relate to post generation manual editing i e polishing such as subsequent tweaking with an image editor prompt engineering and sharing prompts for some text to image models can also include images and keywords and configurable parameters such as artistic style which is often used via keyphrases like in the style of in the prompt and or selection of a broad aesthetic art style there are platforms for sharing trading searching forking refining and or collaborating on prompts for generating specific imagery from image generators prompts are often shared along with images on image sharing websites such as reddit and ai art dedicated websites a prompt is not the complete input needed for the generation of an image additional inputs that determine the generated image include the output resolution random seed and random sampling parameters related terminology synthetic media which includes ai art was described in 2022 as a major technology driven trend that will affect business in the coming years harvard kennedy school researchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of ai art on the x platform synthography is a proposed term for the practice of generating images that are similar to photographs using ai impact bias a major concern raised about ai generated images and art is sampling bias within model training data leading towards discriminatory output from ai art models in 2023 university of washington researchers found evidence of racial bias within the stable diffusion model with images of a person corresponding most frequently with images of males from europe or north america looking more into the sampling bias found within ai training data in 2017 researchers at princeton university used ai software to link over 2 million words finding that european names were viewed as more pleasant than african americans names and that the words woman and girl were more likely to be associated with the arts instead of science and math which were most likely connected to males generative ai models typically work based on user entered word based prompts especially in the case of diffusion models and this word related bias may lead to biased results along with this generative ai can perpetuate harmful stereotypes regarding women for example lensa an ai app that trended on tiktok in 2023 was known to lighten black skin make users thinner and generate hypersexualized images of women melissa heikkilä a senior reporter at mit technology review shared the findings of an experiment using lensa noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner experts suggest that such outcomes can result from biases in the datasets used to train ai models which can sometimes contain imbalanced representations including hypersexual or nude imagery in 2024 google s chatbot gemini s ai image generator was criticized for perceived racial bias with claims that gemini deliberately underrepresented white people in its results users reported that it generated images of white historical figures like the founding fathers nazi soldiers and vikings as other races and that it refused to process prompts such as happy white people and ideal nuclear family google later apologized for missing the mark and took gemini s image generator offline for updates this prompted discussions about the ethical implications of representing historical figures through a contemporary lens leading critics to argue that these outputs could mislead audiences regarding actual historical contexts copyright legal scholars artists and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century some artists use ai art to critique and explore the ethics of using gathered data to produce new artwork in 1985 intellectual property law professor pamela samuelson argued that us copyright should allocate algorithmically generated artworks to the user of the computer program a 2019 florida law review article presented three perspectives on the issue in the first artificial intelligence itself would become the copyright owner to do this section 101 of the us copyright act would need to be amended to define author as a computer in the second following samuelson s argument the user programmer or artificial intelligence company would be the copyright owner this would be an expansion of the work for hire doctrine under which ownership of a copyright is transferred to the employer in the third situation copyright assignments would never take place and such works would be in the public domain as copyright assignments require an act of authorship in 2022 coinciding with the rising availability of consumer grade ai image generation services popular discussion renewed over the legality and ethics of ai generated art a particular topic is the inclusion of copyrighted artwork and images in ai training datasets with artists objecting to commercial ai products using their works without consent credit or financial compensation in september 2022 reema selhi of the design and artists copyright society stated that there are no safeguards for artists to be able to identify works in databases that are being used and opt out some have claimed that images generated with these models can bear resemblance to extant artwork sometimes including the remains of the original artist s signature in december 2022 users of the portfolio platform artstation staged an online protest against non consensual use of their artwork within datasets this resulted in opt out services such as have i been trained increasing in profile as well as some online art platforms promising to offer their own opt out options according to the us copyright office artificial intelligence programs are unable to hold copyright a decision upheld at the federal district level as of august 2023 followed the reasoning from the monkey selfie copyright dispute openai the developer of dall e has its own policy on who owns generated art they assign the right and title of a generated image to the creator meaning the user who inputted the prompt owns the image generated along with the right to sell reprint and merchandise it in january 2023 three artists sarah andersen kelly mckernan and karla ortiz filed a copyright infringement lawsuit against stability ai midjourney and deviantart claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web in july 2023 u s district judge william orrick was inclined to dismiss most of the lawsuits filed by andersen mckernan and ortiz but allowed them to file a new complaint also in 2023 stability ai was sued by getty images for using its images in the training data a tool built by simon willison allowed people to search 0 5 of the training data for stable diffusion v1 1 i e 12 million of the 2 3 billion instances from laion 2b artist karen hallion discovered that her copyrighted images were used as training data without their consent in march 2024 tennessee enacted the elvis act which prohibits the use of ai to mimic a musician s voice without permission a month later in that year adam schiff introduced the generative ai copyright disclosure act which if passed would require that ai companies to submit copyrighted works in their datasets to the register of copyrights before releasing new generative ai systems deception as with other types of photo manipulation since the early 19th century some people in the early 21st century have been concerned that ai could be used to create content that is misleading and can be made to damage a person s reputation such as deepfakes artist sarah andersen who previously had her art copied and edited to depict neo nazi beliefs stated that the spread of hate speech online can be worsened by the use of image generators some also generate images or videos for the purpose of catfishing ai systems have the ability to create deepfake content which is often viewed as harmful and offensive the creation of deepfakes poses a risk to individuals who have not consented to it this mainly refers to deepfake pornography which is used as revenge porn where sexually explicit material is disseminated to humiliate or harm another person ai generated child pornography has been deemed a potential danger to society due to its unlawful nature to mitigate some deceptions openai developed a tool in 2024 to detect images that were generated by dall e 3 in testing this tool accurately identified dall e 3 generated images approximately 98 of the time the tool is also fairly capable of recognizing images that have been visually modified by users post generation after winning the 2023 creative open competition sony world photography awards boris eldagsen stated that his entry was actually created with artificial intelligence photographer feroz khan commented to the bbc that eldagsen had clearly shown that even experienced photographers and art experts can be fooled smaller contests have been affected as well in 2023 a contest run by author mark lawrence as self published fantasy blog off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with midjourney in may 2023 on social media sites such as reddit and twitter attention was given to a midjourney generated image of pope francis wearing a white puffer coat additionally an ai generated image of an attack on the pentagon went viral as part of a hoax news story on twitter in the days before march 2023 indictment of donald trump as part of the stormy daniels donald trump scandal several ai generated images allegedly depicting trump s arrest went viral online on march 20 british journalist eliot higgins generated various images of donald trump being arrested or imprisoned using midjourney v5 and posted them on twitter two images of trump struggling against arresting officers went viral under the mistaken impression that they were genuine accruing more than 5 million views in three days according to higgins the images were not meant to mislead but he was banned from using midjourney services as a result as of april 2024 the tweet had garnered more than 6 8 million views in february 2024 the paper cellular functions of spermatogonial stem cells in relation to jak stat signaling pathway was published using ai generated images it was later retracted from frontiers in cell and developmental biology because the paper does not meet the standards income and employment stability as generative ai image software such as stable diffusion and dall e continue to advance the potential problems and concerns that these systems pose for creativity and artistry have risen in 2022 artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money particularly if ai based images started replacing artists working in the illustration and design industries in august 2022 digital artist r j palmer stated that i could easily envision a scenario where using ai a single artist or art director could take the place of 5 10 entry level artists i have seen a lot of self published authors and such say how great it will be that they don t have to hire an artist scholars jiang et al state that leaders of companies like open ai and stability ai have openly stated that they expect generative ai systems to replace creatives imminently a 2022 case study found that ai produced images created by technology like dall e caused some traditional artists to be concerned about losing work while others use it to their advantage and view it as a tool ai based images have become more commonplace in art markets and search engines because ai based text to image systems are trained from pre existing artistic images sometimes without the original artist s consent allowing the software to mimic specific artists styles for example polish digital artist greg rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are ai generated specifically to mimic his style furthermore some training databases on which ai systems are based are not accessible to the public the ability of ai based art software to mimic or forge artistic style also raises concerns of malice or greed works of ai generated art such as théâtre d opéra spatial a text to image ai illustration that won the grand prize in the august 2022 digital art competition at the colorado state fair have begun to overwhelm art contests and other submission forums meant for small artists the netflix short film the dog the boy released in january 2023 received backlash online for its use of artificial intelligence art to create the film s background artwork within the same vein disney released secret invasion a marvel tv show with an ai generated intro on disney in 2023 causing concern and backlash regarding the idea that artists could be made obsolete by machine learning tools ai art has sometimes been deemed to be able to replace traditional stock images in 2023 shutterstock announced a beta test of an ai tool that can regenerate partial content of other shutterstock s images getty images and nvidia have partnered with the launch of generative ai by istock a model trained on getty s library and istock s photo library using nvidia s picasso model power usage researchers from hugging face and carnegie mellon university reported in a 2023 paper that generating one thousand 1024 1024 images using stable diffusion s xl 1 0 base model requires 11 49 kwh of energy and generates 1 594 grams 56 2 oz of carbon dioxide which is roughly equivalent to driving an average gas powered car a distance of 4 1 miles 6 6 km comparing 88 different models the paper concluded that image generation models used on average around 2 9 kwh of energy per 1 000 inferences analysis of existing art using ai in addition to the creation of original art research methods that use ai have been generated to quantitatively analyze digital art collections this has been made possible due to the large scale digitization of artwork in the past few decades according to cetinic and she 2022 using artificial intelligence to analyze already existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences two computational methods close reading and distant viewing are the typical approaches used to analyze digitized art close reading focuses on specific visual aspects of one piece some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties in contrast through distant viewing methods the similarity across an entire collection for a specific feature can be statistically visualized common tasks relating to this method include automatic classification object detection multimodal tasks knowledge discovery in art history and computational aesthetics synthetic images can also be used to train ai algorithms for art authentication and to detect forgeries researchers have also introduced models that predict emotional responses to art one such model is artemis a large scale dataset paired with machine learning models artemis includes emotional annotations from over 6 500 participants along with textual explanations by analyzing both visual inputs and the accompanying text descriptions from this dataset artemis enables the generation of nuanced emotional predictions other forms of art ai has also been used in arts outside of visual arts generative ai has been used in video game production beyond imagery especially for level design e g for custom maps and creating new content e g quests or dialogue or interactive stories in video games ai has also been used in the literary arts such as helping with writer s block inspiration or rewriting segments in the culinary arts some prototype cooking robots can dynamically taste which can assist chefs in analyzing the content and flavor of dishes during the cooking process see also references ",
            "total_words": 4792,
            "unique_words_percentage": 32.99248747913189,
            "stopwords_percentage": 36.41485809682805
        },
        {
            "title": "History of artificial intelligence",
            "link": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
            "content": "the history of artificial intelligence ai began in antiquity with myths stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen the study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s a machine based on abstract mathematical reasoning this device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain the field of ai research was founded at a workshop held on the campus of dartmouth college in 1956 attendees of the workshop became the leaders of ai research for decades many of them predicted that machines as intelligent as humans would exist within a generation the u s government provided millions of dollars with the hope of making this vision come true eventually it became obvious that researchers had grossly underestimated the difficulty of this feat in 1974 criticism from james lighthill and pressure from the u s congress led the u s and british governments to stop funding undirected research into artificial intelligence seven years later a visionary initiative by the japanese government and the success of expert systems reinvigorated investment in ai and by the late 1980s the industry had grown into a billion dollar enterprise however investors enthusiasm waned in the 1990s and the field was criticized in the press and avoided by industry a period known as an ai winter nevertheless research and funding continued to grow under other names in the early 2000s machine learning was applied to a wide range of problems in academia and industry the success was due to the availability of powerful computer hardware the collection of immense data sets and the application of solid mathematical methods soon after deep learning proved to be a breakthrough technology eclipsing all other methods the transformer architecture debuted in 2017 and was used to produce impressive generative ai applications amongst other use cases investment in ai boomed in the 2020s precursors mythical fictional and speculative precursors myth and legend in greek mythology talos was a giant made of bronze who acted as guardian for the island of crete he would throw boulders at the ships of invaders and would complete 3 circuits around the island s perimeter daily according to pseudo apollodorus bibliotheke hephaestus forged talos with the aid of a cyclops and presented the automaton as a gift to minos in the argonautica jason and the argonauts defeated talos by removing a plug near his foot causing the vital ichor to flow out from his body and rendering him lifeless pygmalion was a legendary king and sculptor of greek mythology famously represented in ovid s metamorphoses in the 10th book of ovid s narrative poem pygmalion becomes disgusted with women when he witnesses the way in which the propoetides prostitute themselves despite this he makes offerings at the temple of venus asking the goddess to bring to him a woman just like a statue he carved medieval legends of artificial beings in of the nature of things the swiss alchemist paracelsus describes a procedure that he claims can fabricate an artificial man by placing the sperm of a man in horse dung and feeding it the arcanum of mans blood after 40 days the concoction will become a living infant the earliest written account regarding golem making is found in the writings of eleazar ben judah of worms in the early 13th century during the middle ages it was believed that the animation of a golem could be achieved by insertion of a piece of paper with any of god s names on it into the mouth of the clay figure unlike legendary automata like brazen heads a golem was unable to speak takwin the artificial creation of life was a frequent topic of ismaili alchemical manuscripts especially those attributed to jabir ibn hayyan islamic alchemists attempted to create a broad range of life through their work ranging from plants to animals in faust the second part of the tragedy by johann wolfgang von goethe an alchemically fabricated homunculus destined to live forever in the flask in which he was made endeavors to be born into a full human body upon the initiation of this transformation however the flask shatters and the homunculus dies modern fiction by the 19th century ideas about artificial men and thinking machines became a popular theme in fiction notable works like mary shelley s frankenstein and karel čapek s r u r rossum s universal robots explored the concept of artificial life speculative essays such as samuel butler s darwin among the machines and edgar allan poe s maelzel s chess player reflected society s growing interest in machines with artificial intelligence ai remains a common topic in science fiction today automata realistic humanoid automata were built by craftsman from many civilizations including yan shi hero of alexandria al jazari haroun al rashid jacques de vaucanson leonardo torres y quevedo pierre jaquet droz and wolfgang von kempelen the oldest known automata were the sacred statues of ancient egypt and greece the faithful believed that craftsman had imbued these figures with very real minds capable of wisdom and emotion hermes trismegistus wrote that by discovering the true nature of the gods man has been able to reproduce it english scholar alexander neckham asserted that the ancient roman poet virgil had built a palace with automaton statues during the early modern period these legendary automata were said to possess the magical ability to answer questions put to them the late medieval alchemist and proto protestant roger bacon was purported to have fabricated a brazen head having developed a legend of having been a wizard these legends were similar to the norse myth of the head of mímir according to legend mímir was known for his intellect and wisdom and was beheaded in the æsir vanir war odin is said to have embalmed the head with herbs and spoke incantations over it such that mímir s head remained able to speak wisdom to odin odin then kept the head near him for counsel formal reasoning artificial intelligence is based on the assumption that the process of human thought can be mechanized the study of mechanical or formal reasoning has a long history chinese indian and greek philosophers all developed structured methods of formal deduction by the first millennium bce their ideas were developed over the centuries by philosophers such as aristotle who gave a formal analysis of the syllogism euclid whose elements was a model of formal reasoning al khwārizmī who developed algebra and gave his name to the word algorithm and european scholastic philosophers such as william of ockham and duns scotus spanish philosopher ramon llull 1232 1315 developed several logical machines devoted to the production of knowledge by logical means llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations produced by the machine by mechanical meanings in such ways as to produce all the possible knowledge llull s work had a great influence on gottfried leibniz who redeveloped his ideas in the 17th century leibniz thomas hobbes and rené descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry hobbes famously wrote in leviathan for reason is nothing but reckoning that is adding and subtracting leibniz envisioned a universal language of reasoning the characteristica universalis which would reduce argumentation to calculation so that there would be no more need of disputation between two philosophers than between two accountants for it would suffice to take their pencils in hand down to their slates and to say each other with a friend as witness if they liked let us calculate these philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of ai research the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible the foundations had been set by such works as boole s the laws of thought and frege s begriffsschrift building on frege s system russell and whitehead presented a formal treatment of the foundations of mathematics in their masterpiece the principia mathematica in 1913 inspired by russell s success david hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question can all of mathematical reasoning be formalized his question was answered by gödel s incompleteness proof turing s machine and church s lambda calculus their answer was surprising in two ways first they proved that there were in fact limits to what mathematical logic could accomplish but second and more important for ai their work suggested that within these limits any form of mathematical reasoning could be mechanized the church turing thesis implied that a mechanical device shuffling symbols as simple as 0 and 1 could imitate any conceivable process of mathematical deduction the key insight was the turing machine a simple theoretical construct that captured the essence of abstract symbol manipulation this invention would inspire a handful of scientists to begin discussing the possibility of thinking machines computer science calculating machines were designed or built in antiquity and throughout history by many people including gottfried leibniz joseph marie jacquard charles babbage percy ludgate leonardo torres quevedo vannevar bush and others ada lovelace speculated that babbage s machine was a thinking or reasoning machine but warned it is desirable to guard against the possibility of exaggerated ideas that arise as to the powers of the machine the first modern computers were the massive machines of the second world war such as konrad zuse s z3 alan turing s heath robinson and colossus atanasoff and berry s and abc and eniac at the university of pennsylvania eniac was based on the theoretical foundation laid by alan turing and developed by john von neumann and proved to be the most influential birth of artificial intelligence 1941 56 the earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s 1940s and early 1950s recent research in neurology had shown that the brain was an electrical network of neurons that fired in all or nothing pulses norbert wiener s cybernetics described control and stability in electrical networks claude shannon s information theory described digital signals i e all or nothing signals alan turing s theory of computation showed that any form of computation could be described digitally the close relationship between these ideas suggested that it might be possible to construct an electronic brain in the 1940s and 50s a handful of scientists from a variety of fields mathematics psychology engineering economics and political science explored several research directions that would be vital to later ai research alan turing was among the first people to seriously investigate the theoretical possibility of machine intelligence the field of artificial intelligence research was founded as an academic discipline in 1956 turing test in 1950 turing published a landmark paper computing machinery and intelligence in which he speculated about the possibility of creating machines that think in the paper he noted that thinking is difficult to define and devised his famous turing test if a machine could carry on a conversation over a teleprinter that was indistinguishable from a conversation with a human being then it was reasonable to say that the machine was thinking this simplified version of the problem allowed turing to argue convincingly that a thinking machine was at least plausible and the paper answered all the most common objections to the proposition the turing test was the first serious proposal in the philosophy of artificial intelligence artificial neural networks walter pitts and warren mcculloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943 they were the first to describe what later researchers would call a neural network the paper was influenced by turing s paper on computable numbers from 1936 using similar two state boolean neurons but was the first to apply it to neuronal function one of the students inspired by pitts and mcculloch was marvin minsky who was a 24 year old graduate student at the time in 1951 minsky and dean edmonds built the first neural net machine the snarc minsky would later become one of the most important leaders and innovators in ai cybernetic robots experimental robots such as w grey walter s turtles and the johns hopkins beast were built in the 1950s these machines did not use computers digital electronics or symbolic reasoning they were controlled entirely by analog circuitry game ai in 1951 using the ferranti mark 1 machine of the university of manchester christopher strachey wrote a checkers program and dietrich prinz wrote one for chess arthur samuel s checkers program the subject of his 1959 paper some studies in machine learning using the game of checkers eventually achieved sufficient skill to challenge a respectable amateur samuel s program was among the first uses of what would later be called machine learning game ai would continue to be used as a measure of progress in ai throughout its history symbolic reasoning and the logic theorist when access to digital computers became possible in the mid fifties a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought this was a new approach to creating thinking machines in 1955 allen newell and future nobel laureate herbert a simon created the logic theorist with help from j c shaw the program would eventually prove 38 of the first 52 theorems in russell and whitehead s principia mathematica and find new and more elegant proofs for some simon said that they had solved the venerable mind body problem explaining how a system composed of matter can have the properties of mind the symbolic reasoning paradigm they introduced would dominate ai research and funding until the middle 90s as well as inspire the cognitive revolution dartmouth workshop the dartmouth workshop of 1956 was a pivotal event that marked the formal inception of ai as an academic discipline it was organized by marvin minsky and john mccarthy with the support of two senior scientists claude shannon and nathan rochester of ibm the proposal for the conference stated they intended to test the assertion that every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it the term artificial intelligence was introduced by john mccarthy at the workshop the participants included ray solomonoff oliver selfridge trenchard more arthur samuel allen newell and herbert a simon all of whom would create important programs during the first decades of ai research at the workshop newell and simon debuted the logic theorist the workshop was the moment that ai gained its name its mission its first major success and its key players and is widely considered the birth of ai cognitive revolution in the autumn of 1956 newell and simon also presented the logic theorist at a meeting of the special interest group in information theory at the massachusetts institute of technology mit at the same meeting noam chomsky discussed his generative grammar and george miller described his landmark paper the magical number seven plus or minus two miller wrote i left the symposium with a conviction more intuitive than rational that experimental psychology theoretical linguistics and the computer simulation of cognitive processes were all pieces from a larger whole this meeting was the beginning of the cognitive revolution an interdisciplinary paradigm shift in psychology philosophy computer science and neuroscience it inspired the creation of the sub fields of symbolic artificial intelligence generative linguistics cognitive science cognitive psychology cognitive neuroscience and the philosophical schools of computationalism and functionalism all these fields used related tools to model the mind and results discovered in one field were relevant to the others the cognitive approach allowed researchers to consider mental objects like thoughts plans goals facts or memories often analyzed using high level symbols in functional networks these objects had been forbidden as unobservable by earlier paradigms such as behaviorism symbolic mental objects would become the major focus of ai research and funding for the next several decades early successes 1956 1974 the programs developed in the years after the dartmouth workshop were to most people simply astonishing computers were solving algebra word problems proving theorems in geometry and learning to speak english few at the time would have believed that such intelligent behavior by machines was possible at all researchers expressed an intense optimism in private and in print predicting that a fully intelligent machine would be built in less than 20 years government agencies like the defense advanced research projects agency darpa then known as arpa poured money into the field artificial intelligence laboratories were set up at a number of british and us universities in the latter 1950s and early 1960s approaches there were many successful programs and new directions in the late 50s and 1960s among the most influential were these reasoning planning and problem solving as search many early ai programs used the same basic algorithm to achieve some goal like winning a game or proving a theorem they proceeded step by step towards it by making a move or a deduction as if searching through a maze backtracking whenever they reached a dead end the principal difficulty was that for many problems the number of possible paths through the maze was astronomical a situation known as a combinatorial explosion researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution newell and simon tried to capture a general version of this algorithm in a program called the general problem solver other searching programs were able to accomplish impressive tasks like solving problems in geometry and algebra such as herbert gelernter s geometry theorem prover 1958 and symbolic automatic integrator saint written by minsky s student james slagle in 1961 other programs searched through goals and subgoals to plan actions like the strips system developed at stanford to control the behavior of the robot shakey natural language an important goal of ai research is to allow computers to communicate in natural languages like english an early success was daniel bobrow s program student which could solve high school algebra word problems a semantic net represents concepts e g house door as nodes and relations among concepts as links between the nodes e g has a the first ai program to use a semantic net was written by ross quillian and the most successful and controversial version was roger schank s conceptual dependency theory joseph weizenbaum s eliza could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program see eliza effect but in fact eliza simply gave a canned response or repeated back what was said to it rephrasing its response with a few grammar rules eliza was the first chatbot micro worlds in the late 60s marvin minsky and seymour papert of the mit ai laboratory proposed that ai research should focus on artificially simple situations known as micro worlds they pointed out that in successful sciences like physics basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies much of the research focused on a blocks world which consists of colored blocks of various shapes and sizes arrayed on a flat surface this paradigm led to innovative work in machine vision by gerald sussman adolfo guzman david waltz who invented constraint propagation and especially patrick winston at the same time minsky and papert built a robot arm that could stack blocks bringing the blocks world to life terry winograd s shrdlu could communicate in ordinary english sentences about the micro world plan operations and execute them perceptrons and early neural networks in the 1960s funding was primarily directed towards laboratories researching symbolic ai however several people still pursued research in neural networks the perceptron a single layer neural network was introduced in 1958 by frank rosenblatt who had been a schoolmate of marvin minsky at the bronx high school of science like most ai researchers he was optimistic about their power predicting that a perceptron may eventually be able to learn make decisions and translate languages rosenblatt was primarily funded by office of naval research bernard widrow and his student ted hoff built adaline 1960 and madaline 1962 which had up to 1000 adjustable weights a group at stanford research institute led by charles a rosen and alfred e ted brain built two neural network machines named minos i 1960 and ii 1963 mainly funded by u s army signal corps minos ii had 6600 adjustable weights and was controlled with an sds 910 computer in a configuration named minos iii 1968 which could classify symbols on army maps and recognize hand printed characters on fortran coding sheets most of neural network research during this early period involved building and using bespoke hardware rather than simulation on digital computers however partly due to lack of results and partly due to competition from symbolic ai research the minos project ran out of funding in 1966 rosenblatt failed to secure continued funding in the 1960s in 1969 research came to a sudden halt with the publication of minsky and papert s 1969 book perceptrons it suggested that there were severe limitations to what perceptrons could do and that rosenblatt s predictions had been grossly exaggerated the effect of the book was that virtually no research was funded in connectionism for 10 years the competition for government funding ended with the victory of symbolic ai approaches over neural networks minsky who had worked on snarc became a staunch objector to pure connectionist ai widrow who had worked on adaline turned to adaptive signal processing the sri group which worked on minos turned to symbolic ai and robotics the main problem was the inability to train multilayered networks versions of backpropagation had already been used in other fields but it was unknown to these researchers the ai community became aware of backpropogation in the 80s and in the 21st century neural networks would become enormously successful fulfilling all of rosenblatt s optimistic predictions rosenblatt did not live to see this however as he died in a boating accident in 1971 optimism the first generation of ai researchers made these predictions about their work 1958 h a simon and allen newell within ten years a digital computer will be the world s chess champion and within ten years a digital computer will discover and prove an important new mathematical theorem 1965 h a simon machines will be capable within twenty years of doing any work a man can do 1967 marvin minsky within a generation the problem of creating artificial intelligence will substantially be solved 1970 marvin minsky in life magazine in from three to eight years we will have a machine with the general intelligence of an average human being financing in june 1963 mit received a 2 2 million grant from the newly created advanced research projects agency arpa later known as darpa the money was used to fund project mac which subsumed the ai group founded by minsky and mccarthy five years earlier darpa continued to provide 3 million each year until the 70s darpa made similar grants to newell and simon s program at carnegie mellon university and to stanford university s ai lab founded by john mccarthy in 1963 another important ai laboratory was established at edinburgh university by donald michie in 1965 these four institutions would continue to be the main centers of ai research and funding in academia for many years the money was given with few strings attached j c r licklider then the director of arpa believed that his organization should fund people not projects and allowed researchers to pursue whatever directions might interest them this created a freewheeling atmosphere at mit that gave birth to the hacker culture but this hands off approach did not last first ai winter 1974 1980 in the 1970s ai was subject to critiques and financial setbacks ai researchers had failed to appreciate the difficulty of the problems they faced their tremendous optimism had raised public expectations impossibly high and when the promised results failed to materialize funding targeted at ai was severely reduced the lack of success indicated the techniques being used by ai researchers at the time were insufficient to achieve their goals these setbacks did not affect the growth and progress of the field however the funding cuts only impacted a handful of major laboratories and the critiques were largely ignored general public interest in the field continued to grow the number of researchers increased dramatically and new ideas were explored in logic programming commonsense reasoning and many other areas historian thomas haigh argued in 2023 that there was no winter and ai researcher nils nilsson described this period as the most exciting time to work in ai problems in the early seventies the capabilities of ai programs were limited even the most impressive could only handle trivial versions of the problems they were supposed to solve all the programs were in some sense toys ai researchers had begun to run into several limits that would be only conquered decades later and others that still stymie the field in the 2020s limited computer power there was not enough memory or processing speed to accomplish anything truly useful for example ross quillian s successful work on natural language was demonstrated with a vocabulary of only 20 words because that was all that would fit in memory hans moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence he suggested an analogy artificial intelligence requires computer power in the same way that aircraft require horsepower below a certain threshold it s impossible but as power increases eventually it could become easy with enough horsepower he wrote anything will fly intractability and the combinatorial explosion in 1972 richard karp building on stephen cook s 1971 theorem showed there are many problems that can only be solved in exponential time finding optimal solutions to these problems requires extraordinary amounts of computer time except when the problems are trivial this limitation applied to all symbolic ai programs that used search trees and meant that many of the toy solutions used by ai would never scale to useful systems moravec s paradox early ai research had been very successful at getting computers to do intelligent tasks like proving theorems solving geometry problems and playing chess their success at these intelligent tasks convinced them that the problem of intelligent behavior had been largely solved however they utterly failed to make progress on unintelligent tasks like recognizing a face or crossing a room without bumping into anything by the 1980s researchers would realize that symbolic reasoning was utterly unsuited for these perceptual and sensorimotor tasks and that there were limits to this approach the breadth of commonsense knowledge many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world the program needs to have some idea of what it might be looking at or what it is talking about this requires that the program know most of the same things about the world that a child does researchers soon discovered that this was a vast amount of information with billions of atomic facts no one in 1970 could build a database large enough and no one knew how a program might learn so much information representing commonsense reasoning a number of related problems appeared when researchers tried to represent commonsense reasoning using formal logic or symbols descriptions of very ordinary deductions tended to get longer and longer the more one worked on them as more and more exceptions clarifications and distinctions were required however when people thought about ordinary concepts they did not rely on precise definitions rather they seemed to make hundreds of imprecise assumptions correcting them when necessary using their entire body of commonsense knowledge gerald sussman observed that using precise language to describe essentially imprecise concepts doesn t make them any more precise decrease in funding the agencies which funded ai research such as the british government darpa and the national research council nrc became frustrated with the lack of progress and eventually cut off almost all funding for undirected ai research the pattern began in 1966 when the automatic language processing advisory committee alpac report criticized machine translation efforts after spending 20 million the nrc ended all support in 1973 the lighthill report on the state of ai research in the uk criticized the failure of ai to achieve its grandiose objectives and led to the dismantling of ai research in that country the report specifically mentioned the combinatorial explosion problem as a reason for ai s failings darpa was deeply disappointed with researchers working on the speech understanding research program at cmu and canceled an annual grant of 3 million hans moravec blamed the crisis on the unrealistic predictions of his colleagues many researchers were caught up in a web of increasing exaggeration however there was another issue since the passage of the mansfield amendment in 1969 darpa had been under increasing pressure to fund mission oriented direct research rather than basic undirected research funding for the creative freewheeling exploration that had gone on in the 60s would not come from darpa which instead directed money at specific projects with clear objectives such as autonomous tanks and battle management systems the major laboratories mit stanford cmu and edinburgh had been receiving generous support from their governments and when it was withdrawn these were the only places that were seriously impacted by the budget cuts the thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected philosophical and ethical critiques several philosophers had strong objections to the claims being made by ai researchers one of the earliest was john lucas who argued that gödel s incompleteness theorem showed that a formal system such as a computer program could never see the truth of certain statements while a human being could hubert dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of ai arguing that human reasoning actually involved very little symbol processing and a great deal of embodied instinctive unconscious know how john searle s chinese room argument presented in 1980 attempted to show that a program could not be said to understand the symbols that it uses a quality called intentionality if the symbols have no meaning for the machine searle argued then the machine can not be described as thinking these critiques were not taken seriously by ai researchers problems like intractability and commonsense knowledge seemed much more immediate and serious it was unclear what difference know how or intentionality made to an actual computer program mit s minsky said of dreyfus and searle they misunderstand and should be ignored dreyfus who also taught at mit was given a cold shoulder he later said that ai researchers dared not be seen having lunch with me joseph weizenbaum the author of eliza was also an outspoken critic of dreyfus positions but he deliberately made it plain that was not the way to treat a human being and was unprofessional and childish weizenbaum began to have serious ethical doubts about ai when kenneth colby wrote a computer program which can conduct psychotherapeutic dialogue based on eliza weizenbaum was disturbed that colby saw a mindless program as a serious therapeutic tool a feud began and the situation was not helped when colby did not credit weizenbaum for his contribution to the program in 1976 weizenbaum published computer power and human reason which argued that the misuse of artificial intelligence has the potential to devalue human life logic at stanford cmu and edinburgh logic was introduced into ai research as early as 1958 by john mccarthy in his advice taker proposal in 1963 j alan robinson had discovered a simple method to implement deduction on computers the resolution and unification algorithm however straightforward implementations like those attempted by mccarthy and his students in the late 1960s were especially intractable the programs required astronomical numbers of steps to prove simple theorems a more fruitful approach to logic was developed in the 1970s by robert kowalski at the university of edinburgh and soon this led to the collaboration with french researchers alain colmerauer and philippe roussel who created the successful logic programming language prolog prolog uses a subset of logic horn clauses closely related to rules and production rules that permit tractable computation rules would continue to be influential providing a foundation for edward feigenbaum s expert systems and the continuing work by allen newell and herbert a simon that would lead to soar and their unified theories of cognition critics of the logical approach noted as dreyfus had that human beings rarely used logic when they solved problems experiments by psychologists like peter wason eleanor rosch amos tversky daniel kahneman and others provided proof mccarthy responded that what people do is irrelevant he argued that what is really needed are machines that can solve problems not machines that think as people do mit s anti logic approach among the critics of mccarthy s approach were his colleagues across the country at mit marvin minsky seymour papert and roger schank were trying to solve problems like story understanding and object recognition that required a machine to think like a person in order to use ordinary concepts like chair or restaurant they had to make all the same illogical assumptions that people normally made unfortunately imprecise concepts like these are hard to represent in logic mit chose instead to focus on writing programs that solved a given task without using high level abstract definitions or general theories of cognition and measured performance by iterative testing rather than arguments from first principles schank described their anti logic approaches as scruffy as opposed to the neat paradigm used by mccarthy kowalski feigenbaum newell and simon in 1975 in a seminal paper minsky noted that many of his fellow researchers were using the same kind of tool a framework that captures all our common sense assumptions about something for example if we use the concept of a bird there is a constellation of facts that immediately come to mind we might assume that it flies eats worms and so on none of which are true for all birds minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals or over ridden as necessary he called these structures frames schank used a version of frames he called scripts to successfully answer questions about short stories in english frames would eventually be widely used in software engineering under the name object oriented programming the logicians rose to the challenge pat hayes claimed that most of frames is just a new syntax for parts of first order logic but he noted that there are one or two apparently minor details which give a lot of trouble however especially defaults ray reiter admitted that conventional logics such as first order logic lack the expressive power to adequately represent the knowledge required for reasoning by default he proposed augmenting first order logic with a closed world assumption that a conclusion holds by default if its contrary cannot be shown he showed how such an assumption corresponds to the common sense assumption made in reasoning with frames he also showed that it has its procedural equivalent as negation as failure in prolog the closed world assumption as formulated by reiter is not a first order notion it is a meta notion however keith clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first order logic including a unique name assumption that different terms denote different individuals during the late 1970s and throughout the 1980s a variety of logics and extensions of first order logic were developed both for negation as failure in logic programming and for default reasoning more generally collectively these logics have become known as non monotonic logics boom 1980 1987 in the 1980s a form of ai program called expert systems was adopted by corporations around the world and knowledge became the focus of mainstream ai research governments provided substantial funding such as japan s fifth generation computer project and the u s strategic computing initiative overall the ai industry boomed from a few million dollars in 1980 to billions of dollars in 1988 expert systems become widely used an expert system is a program that answers questions or solves problems about a specific domain of knowledge using logical rules that are derived from the knowledge of experts the earliest examples were developed by edward feigenbaum and his students dendral begun in 1965 identified compounds from spectrometer readings mycin developed in 1972 diagnosed infectious blood diseases they demonstrated the feasibility of the approach expert systems restricted themselves to a small domain of specific knowledge thus avoiding the commonsense knowledge problem and their simple design made it relatively easy for programs to be built and then modified once they were in place all in all the programs proved to be useful something that ai had not been able to achieve up to this point in 1980 an expert system called r1 was completed at cmu for the digital equipment corporation it was an enormous success it was saving the company 40 million dollars annually by 1986 corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on ai most of it to in house ai departments an industry grew up to support them including hardware companies like symbolics and lisp machines and software companies such as intellicorp and aion government funding increases in 1981 the japanese ministry of international trade and industry set aside 850 million for the fifth generation computer project their objectives were to write programs and build machines that could carry on conversations translate languages interpret pictures and reason like human beings much to the chagrin of scruffies they initially chose prolog as the primary computer language for the project other countries responded with new programs of their own the uk began the 350 million alvey project a consortium of american companies formed the microelectronics and computer technology corporation or mcc to fund large scale projects in ai and information technology darpa responded as well founding the strategic computing initiative and tripling its investment in ai between 1984 and 1988 knowledge revolution the power of expert systems came from the expert knowledge they contained they were part of a new direction in ai research that had been gaining ground throughout the 70s ai researchers were beginning to suspect reluctantly for it violated the scientific canon of parsimony that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways writes pamela mccorduck he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge sometimes quite detailed knowledge of a domain where a given task lay knowledge based systems and knowledge engineering became a major focus of ai research in the 1980s it was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required in the 1980s some researchers attempted to attack the commonsense knowledge problem directly by creating a massive database that would contain all the mundane facts that the average person knows douglas lenat who started a database called cyc argued that there is no shortcut the only way for machines to know the meaning of human concepts is to teach them one concept at a time by hand new directions in the 1980s although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding it was still unable to solve problems in perception robotics learning and common sense a small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches such as connectionism robotics soft computing and reinforcement learning nils nilsson called these approaches sub symbolic revival of neural networks connectionism in 1982 physicist john hopfield was able to prove that a form of neural network now called a hopfield net could learn and process information and provably converges after enough time under any fixed condition it was a breakthrough as it was previously thought that nonlinear networks would in general evolve chaotically around the same time geoffrey hinton and david rumelhart popularized a method for training neural networks called backpropagation these two developments helped to revive the exploration of artificial neural networks neural networks along with several other similar models received widespread attention after the 1986 publication of the parallel distributed processing a two volume collection of papers edited by rumelhart and psychologist james mcclelland the new field was christened connectionism and there was a considerable debate between advocates of symbolic ai the connectionists hinton called symbols the luminous aether of ai that is an unworkable and misleading model of intelligence in 1990 yann lecun at bell labs used convolutional neural networks to recognize handwritten digits the system was used widely in 90s reading zip codes and personal checks this was the first genuinely useful application of neural networks robotics and embodied reason rodney brooks hans moravec and others argued that in order to show real intelligence a machine needs to have a body it needs to perceive move survive and deal with the world sensorimotor skills are essential to higher level skills such as commonsense reasoning they can t be efficiently implemented using abstract symbolic reasoning so ai should solve the problems of perception mobility manipulation and survival without using symbolic representation at all these robotics researchers advocated building intelligence from the bottom up a precursor to this idea was david marr who had come to mit in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision he rejected all symbolic approaches both mccarthy s logic and minsky s frames arguing that ai needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place marr s work would be cut short by leukemia in 1980 in his 1990 paper elephants don t play chess robotics researcher brooks took direct aim at the physical symbol system hypothesis arguing that symbols are not always necessary since the world is its own best model it is always exactly up to date it always has every detail there is to be known the trick is to sense it appropriately and often enough in the 1980s and 1990s many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning a theory called the embodied mind thesis soft computing and probabilistic reasoning soft computing uses methods that work with incomplete and imprecise information they do not attempt to give precise logical answers but give results that are only probably correct this allowed them to solve problems that precise symbolic methods could not handle press accounts often claimed these tools could think like a human judea pearl s probabilistic reasoning in intelligent systems networks of plausible inference an influential 1988 book brought probability and decision theory into ai fuzzy logic developed by lofti zadeh in the 60s began to be more widely used in ai and robotics evolutionary computation and artificial neural networks also handle imprecise information and are classified as soft in the 90s and early 2000s many other soft computing tools were developed and put into use including bayesian networks hidden markov models information theory and stochastic modeling these tools in turn depended on advanced mathematical techniques such as classical optimization for a time in the 1990s and early 2000s these soft tools were studied by a subfield of ai called computational intelligence reinforcement learning reinforcement learning gives an agent a reward every time it performs a desired action well and may give negative rewards or punishments when it performs poorly it was described in the first half of the twentieth century by psychologists using animal models such as thorndike pavlov and skinner in the 1950s alan turing and arthur samuel foresaw the role of reinforcement learning in ai a successful and influential research program was led by richard sutton and andrew barto beginning 1972 their collaboration revolutionized the study of reinforcement learning and decision making over the four decades in 1988 sutton described machine learning in terms of decision theory i e the markov decision process this gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research also in 1988 sutton and barto developed the temporal difference td learning algorithm where the agent is rewarded only when its predictions about the future show improvement it significantly outperformed previous algorithms td learning was used by gerald tesauro in 1992 in the program td gammon which played backgammon as well as the best human players the program learned the game by playing against itself with zero prior knowledge in an interesting case of interdisciplinary convergence neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the td learning algorithm td learning would be become highly influential in the 21st century used in both alphago and alphazero second ai winter the business community s fascination with ai rose and fell in the 1980s in the classic pattern of an economic bubble as dozens of companies failed the perception in the business world was that the technology was not viable the damage to ai s reputation would last into the 21st century inside the field there was little agreement on the reasons for ai s failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s together all these factors helped to fragment ai into competing subfields focused on particular problems or approaches sometimes even under new names that disguised the tarnished pedigree of artificial intelligence over the next 20 years ai consistently delivered working solutions to specific isolated problems by the late 1990s it was being used throughout the technology industry although somewhat behind the scenes the success was due to increasing computer power by collaboration with other fields such as mathematical optimization and statistics and using the highest standards of scientific accountability by 2000 ai had achieved some of its oldest goals the field was both more cautious and more successful than it had ever been ai winter the term ai winter was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow their fears were well founded in the late 1980s and early 1990s ai suffered a series of financial setbacks the first indication of a change in weather was the sudden collapse of the market for specialized ai hardware in 1987 desktop computers from apple and ibm had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive lisp machines made by symbolics and others there was no longer a good reason to buy them an entire industry worth half a billion dollars was demolished overnight eventually the earliest successful expert systems such as xcon proved too expensive to maintain they were difficult to update they could not learn and they were brittle i e they could make grotesque mistakes when given unusual inputs expert systems proved useful but only in a few special contexts in the late 1980s the strategic computing initiative cut funding to ai deeply and brutally new leadership at darpa had decided that ai was not the next wave and directed funds towards projects that seemed more likely to produce immediate results by 1991 the impressive list of goals penned in 1981 for japan s fifth generation project had not been met indeed some of them like carry on a casual conversation would not be accomplished for another 40 years as with other ai projects expectations had run much higher than what was actually possible over 300 ai companies had shut down gone bankrupt or been acquired by the end of 1993 effectively ending the first commercial wave of ai in 1994 hp newquist stated in the brain makers that the immediate future of artificial intelligence in its commercial form seems to rest in part on the continued success of neural networks ai behind the scenes in the 1990s algorithms originally developed by ai researchers began to appear as parts of larger systems ai had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry such as data mining industrial robotics logistics speech recognition banking software medical diagnosis and google s search engine the field of ai received little or no credit for these successes in the 1990s and early 2000s many of ai s greatest innovations have been reduced to the status of just another item in the tool chest of computer science nick bostrom explains a lot of cutting edge ai has filtered into general applications often without being called ai because once something becomes useful enough and common enough it s not labeled ai anymore many researchers in ai in the 1990s deliberately called their work by other names such as informatics knowledge based systems cognitive systems or computational intelligence in part this may have been because they considered their field to be fundamentally different from ai but also the new names help to procure funding in the commercial world at least the failed promises of the ai winter continued to haunt ai research into the 2000s as the new york times reported in 2005 computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild eyed dreamers mathematical rigor greater collaboration and a narrow focus ai researchers began to develop and use sophisticated mathematical tools more than they ever had in the past most of the new directions in ai relied heavily on mathematical models including artificial neural networks probabilistic reasoning soft computing and reinforcement learning in the 90s and 2000s many other highly mathematical tools were adapted for ai these tools were applied to machine learning perception and mobility there was a widespread realization that many of the problems that ai needed to solve were already being worked on by researchers in fields like statistics mathematics electrical engineering economics or operations research the shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable ai had become a more rigorous scientific discipline another key reason for the success in the 90s was that ai researchers focussed on specific problems with verifiable solutions an approach later derided as narrow ai this provided useful tools in the present rather than speculation about the future intelligent agents a new paradigm called intelligent agents became widely accepted during the 1990s although earlier researchers had proposed modular divide and conquer approaches to ai the intelligent agent did not reach its modern form until judea pearl allen newell leslie p kaelbling and others brought concepts from decision theory and economics into the study of ai when the economist s definition of a rational agent was married to computer science s definition of an object or module the intelligent agent paradigm was complete an intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success by this definition simple programs that solve specific problems are intelligent agents as are human beings and organizations of human beings such as firms the intelligent agent paradigm defines ai research as the study of intelligent agents this is a generalization of some earlier definitions of ai it goes beyond studying human intelligence it studies all kinds of intelligence the paradigm gave researchers license to study isolated problems and to disagree about methods but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence milestones and moore s law on may 11 1997 deep blue became the first computer chess playing system to beat a reigning world chess champion garry kasparov in 2005 a stanford robot won the darpa grand challenge by driving autonomously for 131 miles along an unrehearsed desert trail two years later a team from cmu won the darpa urban challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws these successes were not due to some revolutionary new paradigm but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s in fact deep blue s computer was 10 million times faster than the ferranti mark 1 that christopher strachey taught to play chess in 1951 this dramatic increase is measured by moore s law which predicts that the speed and memory capacity of computers doubles every two years the fundamental problem of raw computer power was slowly being overcome big data deep learning agi 2005 2017 in the first decades of the 21st century access to large amounts of data known as big data cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy a turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks including image and video processing text analysis and speech recognition investment in ai increased along with its capabilities and by 2016 the market for ai related products hardware and software reached more than 8 billion and the new york times reported that interest in ai had reached a frenzy in 2002 ben goertzel and others became concerned that ai had largely abandoned its original goal of producing versatile fully intelligent machines and argued in favor of more direct research into artificial general intelligence by the mid 2010s several companies and institutions had been founded to pursue artificial general intelligence agi such as openai and google s deepmind during the same period new insights into superintelligence raised concerns that ai was an existential threat the risks and unintended consequences of ai technology became an area of serious academic research after 2016 big data and big machines the success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers russell and norvig wrote that the improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm geoffrey hinton recalled that back in the 90s the problem was that our labeled datasets were thousands of times too small our computers were millions of times too slow this was no longer true by 2010 the most useful data in the 2000s came from curated labeled data sets created specifically for machine learning and ai in 2007 a group at umass amherst released labeled faces in the wild an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades fei fei li developed imagenet a database of three million images captioned by volunteers using the amazon mechanical turk released in 2009 it was a useful body of training data and a benchmark for testing for the next generation of image processing systems google released word2vec in 2013 as an open source resource it used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word users were surprised at how well it was able to capture word meanings for example ordinary vector addition would give equivalences like china river yangtze london england france paris this database in particular would be essential for the development of large language models in the late 2010s the explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped and for specific problems large privately held databases contained the relevant data mckinsey global institute reported that by 2009 nearly all sectors in the us economy had at least an average of 200 terabytes of stored data this collection of information was known in the 2000s as big data in a jeopardy exhibition match in february 2011 ibm s question answering system watson defeated the two best jeopardy champions brad rutter and ken jennings by a significant margin watson s expertise would have been impossible without the information available on the internet deep learning in 2012 alexnet a deep learning model developed by alex krizhevsky won the imagenet large scale visual recognition challenge with significantly fewer errors than the second place winner krizhevsky worked with geoffrey hinton at the university of toronto this was a turning point in machine learning over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning deep learning uses a multi layer perceptron although this architecture has been known since the 60s getting it to work requires powerful hardware and large amounts of training data before these became available improving performance of image processing systems required hand crafted ad hoc features that were difficult to implement deep learning was simpler and more general deep learning was applied to dozens of problems over the next few years such as speech recognition machine translation medical diagnosis and game playing in every case it showed enormous gains in performance investment and interest in ai boomed as a result the alignment problem it became fashionable in the 2000s to begin talking about the future of ai again and several popular books considered the possibility of superintelligent machines and what they might mean for human society some of this was optimistic such as ray kurzweil s the singularity is near but others warned that a sufficiently powerful ai was existential threat to humanity such as nick bostrom and eliezer yudkowsky the topic became widely covered in the press and many leading intellectuals and politicians commented on the issue ai programs in the 21st century are defined by their goals the specific measures that they are designed to optimize nick bostrom s influential 2005 book superintelligence argued that if one isn t careful about defining these goals the machine may cause harm to humanity in the process of achieving a goal stuart j russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged reasoning you can t fetch the coffee if you re dead this problem is known by the technical term instrumental convergence the solution is to align the machine s goal function with the goals of its owner and humanity in general thus the problem of mitigating the risks and unintended consequences of ai became known as the value alignment problem or ai alignment at the same time machine learning systems had begun to have disturbing unintended consequences cathy o neil explained how statistical algorithms had been among the causes of the 2008 economic crash julia angwin of propublica argued that the compas system used by the criminal justice system exhibited racial bias under some measures others showed that many machine learning systems exhibited some form of racial bias and there were many other examples of dangerous outcomes that had resulted from machine learning systems in 2016 the election of donald trump and the controversy over the compas system illuminated several problems with the current technological infrastructure including misinformation social media algorithms designed to maximize engagement the misuse of personal data and the trustworthiness of predictive models issues of fairness and unintended consequences became significantly more popular at ai conferences publications vastly increased funding became available and many researchers re focussed their careers on these issues the value alignment problem became a serious field of academic study artificial general intelligence research in the early 2000s several researchers became concerned that mainstream ai was too focused on measurable performance in specific applications known as narrow ai and had abandoned ai s original goal of creating versatile fully intelligent machines an early critic was nils nilsson in 1995 and similar opinions were published by ai elder statesmen john mccarthy marvin minsky and patrick winston in 2007 2009 minsky organized a symposium on human level ai in 2004 ben goertzel adopted the term artificial general intelligence for the new sub field founding a journal and holding conferences beginning in 2008 the new field grew rapidly buoyed by the continuing success of artificial neural networks and the hope that it was the key to agi several competing companies laboratories and foundations were founded to develop agi in the 2010s deepmind was founded in 2010 by three english scientists demis hassabis shane legg and mustafa suleyman with funding from peter thiel and later elon musk the founders and financiers were deeply concerned about ai safety and the existential risk of ai deepmind s founders had a personal connection with yudkowsky and musk was among those who was actively raising the alarm hassabis was both worried about the dangers of agi and optimistic about its power he hoped they could solve ai then solve everything else in 2012 geoffrey hinton who been leading neural network research since the 80s was approached by baidu which wanted to hire him and all his students for an enormous sum hinton decided to hold an auction and at a lake tahoe ai conference they sold themselves to google for a price of 44 million hassabis took notice and sold deepmind to google in 2014 on the condition that it would not accept military contracts and would be overseen by an ethics board larry page of google unlike musk and hassabis was an optimist about the future of ai musk and paige became embroiled in an argument about the risk of agi at musk s 2015 birthday party they had been friends for decades but stopped speaking to each other shortly afterwards musk attended the one and only meeting of the deepmind s ethics board where it became clear that google was uninterested in mitigating the harm of agi frustrated by his lack of influence he founded openai in 2015 enlisting sam altman to run it and hiring top scientists openai began as a non profit free from the economic incentives that were driving google and other corporations musk became frustrated again and left the company in 2018 openai turned to microsoft for continued financial support and altman and openai formed a for profit version of the company with more than 1 billion in financing in 2021 dario amodei and 14 other scientists left openai over concerns that the company was putting profits above safety they formed anthropic which soon had 6 billion in financing from microsoft and google the new york times wrote in 2023 at the heart of this competition is a brain stretching paradox the people who say they are most worried about ai are among the most determined to create it and enjoy its riches they have justified their ambition with their strong belief that they alone can keep ai from endangering earth large language models ai boom 2020 present the ai boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017 leading to the scaling and development of large language models exhibiting human like traits of knowledge attention and creativity the new ai era began around 2020 2023 with the public release of scaled large language models llms such as chatgpt transformer architecture and large language models in 2017 the transformer architecture was proposed by google researchers it exploits an attention mechanism and became widely used in large language models large language models based on the transformer were developed by agi companies openai released gpt 3 in 2020 and deepmind released gato in 2022 these are foundation models they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks these models can discuss a huge number of topics and display general knowledge the question naturally arises are these models an example of artificial general intelligence bill gates was skeptical of the new technology and the hype that surrounded agi however altman presented him with a live demo of chatgpt4 passing an advanced biology test gates was convinced in 2023 microsoft research tested the model with a large variety of tasks and concluded that it could reasonably be viewed as an early yet still incomplete version of an artificial general intelligence agi system in 2024 openai o3 a type of advanced reasoning model developed by openai was announced on the abstraction and reasoning corpus for artificial general intelligence arc agi benchmark developed by françois chollet in 2019 the model achieved an unofficial score of 87 5 on the semi private test surpassing the typical human score of 84 the benchmark is supposed to be a necessary but not sufficient test for agi speaking of the benchmark chollet has said you ll know agi is here when the exercise of creating tasks that are easy for regular humans but hard for ai becomes simply impossible neurosymbolic ai deepmind describes their approach as neurosymbolic because they use deep learning in combination with symbolic techniques for example alphazero uses deep learning to evaluate the strength of a position and to suggest policies courses of action but it uses monte carlo tree search to lookahead at new positions ai boom investment in ai grew exponentially in after 2020 chatgpt was launched on nov 30 2022 within days of its release it went viral and ushered in what is generally considered to be artificial intelligence s break out year when it entered the public consciousness by mid 2024 several financial began to question the capacity of ai companies to produce a return on investment some investors like jeremy grantham or jeffrey gundlach speculated that ai was experiencing another bubble similar to the dot com bubble 2024 nobel prizes in 2024 the royal swedish academy of sciences awarded nobel prizes in recognition of groundbreaking contributions to artificial intelligence the recipients included in physics john hopfield for his work on physics inspired hopfield networks and geoffrey hinton for foundational contributions to boltzmann machines and deep learning in chemistry david baker demis hassabis and john jumper for their advancements in protein folding predictions see alphafold see also history of artificial neural networks history of knowledge representation and reasoning history of natural language processing outline of artificial intelligence progress in artificial intelligence timeline of artificial intelligence timeline of machine learning notes references ",
            "total_words": 11228,
            "unique_words_percentage": 24.51905949412184,
            "stopwords_percentage": 39.41930887068044
        }
    ],
    "Machine Learning": [
        {
            "title": "Machine learning",
            "link": "https://en.wikipedia.org/wiki/Machine_learning",
            "content": "machine learning ml is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance ml finds application in many fields including natural language processing computer vision speech recognition email filtering agriculture and medicine the application of ml to business problems is known as predictive analytics statistics and mathematical optimization mathematical programming methods comprise the foundations of machine learning data mining is a related field of study focusing on exploratory data analysis eda via unsupervised learning from a theoretical viewpoint probably approximately correct pac learning provides a framework for describing machine learning history the term machine learning was coined in 1959 by arthur samuel an ibm employee and pioneer in the field of computer gaming and artificial intelligence the synonym self teaching computers was also used in this time period although the earliest machine learning model was introduced in the 1950s when arthur samuel invented a program that calculated the winning chance in checkers for each side the history of machine learning roots back to decades of human desire and effort to study human cognitive processes in 1949 canadian psychologist donald hebb published the book the organization of behavior in which he introduced a theoretical neural structure formed by certain interactions among nerve cells hebb s model of neurons interacting with one another set a groundwork for how ais and machine learning algorithms work under nodes or artificial neurons used by computers to communicate data other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well including logician walter pitts and warren mcculloch who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes by the early 1960s an experimental learning machine with punched tape memory called cybertron had been developed by raytheon company to analyse sonar signals electrocardiograms and speech patterns using rudimentary reinforcement learning it was repetitively trained by a human operator teacher to recognize patterns and equipped with a goof button to cause it to reevaluate incorrect decisions a representative book on research into machine learning during the 1960s was nilsson s book on learning machines dealing mostly with machine learning for pattern classification interest related to pattern recognition continued into the 1970s as described by duda and hart in 1973 in 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters 26 letters 10 digits and 4 special symbols from a computer terminal tom m mitchell provided a widely quoted more formal definition of the algorithms studied in the machine learning field a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t as measured by p improves with experience e this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms this follows alan turing s proposal in his paper computing machinery and intelligence in which the question can machines think is replaced with the question can machines do what we as thinking entities can do modern day machine learning has two objectives one is to classify data based on models which have been developed the other purpose is to make predictions for future outcomes based on these models a hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles a machine learning algorithm for stock trading may inform the trader of future potential predictions relationships to other fields artificial intelligence as a scientific endeavor machine learning grew out of the quest for artificial intelligence ai in the early days of ai as an academic discipline some researchers were interested in having machines learn from data they attempted to approach the problem with various symbolic methods as well as what were then termed neural networks these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics probabilistic reasoning was also employed especially in automated medical diagnosis 488 however an increasing emphasis on the logical knowledge based approach caused a rift between ai and machine learning probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation 488 by 1980 expert systems had come to dominate ai and statistics was out of favor work on symbolic knowledge based learning did continue within ai leading to inductive logic programming ilp but the more statistical line of research was now outside the field of ai proper in pattern recognition and information retrieval 708 710 755 neural networks research had been abandoned by ai and computer science around the same time this line too was continued outside the ai cs field as connectionism by researchers from other disciplines including john hopfield david rumelhart and geoffrey hinton their main success came in the mid 1980s with the reinvention of backpropagation 25 machine learning ml reorganized and recognized as its own field started to flourish in the 1990s the field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature it shifted focus away from the symbolic approaches it had inherited from ai and toward methods and models borrowed from statistics fuzzy logic and probability theory data compression data mining machine learning and data mining often employ the same methods and overlap significantly but while machine learning focuses on prediction based on known properties learned from the training data data mining focuses on the discovery of previously unknown properties in the data this is the analysis step of knowledge discovery in databases data mining uses many machine learning methods but with different goals on the other hand machine learning also employs data mining methods as unsupervised learning or as a preprocessing step to improve learner accuracy much of the confusion between these two research communities which do often have separate conferences and separate journals ecml pkdd being a major exception comes from the basic assumptions they work with in machine learning performance is usually evaluated with respect to the ability to reproduce known knowledge while in knowledge discovery and data mining kdd the key task is the discovery of previously unknown knowledge evaluated with respect to known knowledge an uninformed unsupervised method will easily be outperformed by other supervised methods while in a typical kdd task supervised methods cannot be used due to the unavailability of training data machine learning also has intimate ties to optimization many learning problems are formulated as minimization of some loss function on a training set of examples loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances for example in classification one wants to assign a label to instances and models are trained to correctly predict the preassigned labels of a set of examples generalization characterizing the generalization of various learning algorithms is an active topic of current research especially for deep learning algorithms statistics machine learning and statistics are closely related fields in terms of methods but distinct in their principal goal statistics draws population inferences from a sample while machine learning finds generalizable predictive patterns according to michael i jordan the ideas of machine learning from methodological principles to theoretical tools have had a long pre history in statistics he also suggested the term data science as a placeholder to call the overall field conventional statistical analyses require the a priori selection of a model most suitable for the study data set in addition only significant or theoretically relevant variables based on previous experience are included for analysis in contrast machine learning is not built on a pre structured model rather the data shape the model by detecting underlying patterns the more variables input used to train the model the more accurate the ultimate model will be leo breiman distinguished two statistical modeling paradigms data model and algorithmic model wherein algorithmic model means more or less the machine learning algorithms like random forest some statisticians have adopted methods from machine learning leading to a combined field that they call statistical learning statistical physics analytical and computational techniques derived from deep rooted physics of disordered systems can be extended to large scale problems including machine learning e g to analyse the weight space of deep neural networks statistical physics is thus finding applications in the area of medical diagnostics theory a core objective of a learner is to generalize from its experience generalization in this context is the ability of a learning machine to perform accurately on new unseen examples tasks after having experienced a learning data set the training examples come from some generally unknown probability distribution considered representative of the space of occurrences and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases the computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning pac model because training sets are finite and the future is uncertain learning theory usually does not yield guarantees of the performance of algorithms instead probabilistic bounds on the performance are quite common the bias variance decomposition is one way to quantify generalization error for the best performance in the context of generalization the complexity of the hypothesis should match the complexity of the function underlying the data if the hypothesis is less complex than the function then the model has under fitted the data if the complexity of the model is increased in response then the training error decreases but if the hypothesis is too complex then the model is subject to overfitting and generalization will be poorer in addition to performance bounds learning theorists study the time complexity and feasibility of learning in computational learning theory a computation is considered feasible if it can be done in polynomial time there are two kinds of time complexity results positive results show that a certain class of functions can be learned in polynomial time negative results show that certain classes cannot be learned in polynomial time approaches machine learning approaches are traditionally divided into three broad categories which correspond to learning paradigms depending on the nature of the signal or feedback available to the learning system supervised learning the computer is presented with example inputs and their desired outputs given by a teacher and the goal is to learn a general rule that maps inputs to outputs unsupervised learning no labels are given to the learning algorithm leaving it on its own to find structure in its input unsupervised learning can be a goal in itself discovering hidden patterns in data or a means towards an end feature learning reinforcement learning a computer program interacts with a dynamic environment in which it must perform a certain goal such as driving a vehicle or playing a game against an opponent as it navigates its problem space the program is provided feedback that s analogous to rewards which it tries to maximize although each algorithm has advantages and limitations no single algorithm works for all problems supervised learning supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs the data known as training data consists of a set of training examples each training example has one or more inputs and the desired output also known as a supervisory signal in the mathematical model each training example is represented by an array or vector sometimes called a feature vector and the training data is represented by a matrix through iterative optimization of an objective function supervised learning algorithms learn a function that can be used to predict the output associated with new inputs an optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data an algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task types of supervised learning algorithms include active learning classification and regression classification algorithms are used when the outputs are restricted to a limited set of values and regression algorithms are used when the outputs may have any numerical value within a range as an example for a classification algorithm that filters emails the input would be an incoming email and the output would be the name of the folder in which to file the email examples of regression would be predicting the height of a person or the future temperature similarity learning is an area of supervised machine learning closely related to regression and classification but the goal is to learn from examples using a similarity function that measures how similar or related two objects are it has applications in ranking recommendation systems visual identity tracking face verification and speaker verification unsupervised learning unsupervised learning algorithms find structures in data that has not been labeled classified or categorized instead of responding to feedback unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data central applications of unsupervised machine learning include clustering dimensionality reduction and density estimation cluster analysis is the assignment of a set of observations into subsets called clusters so that observations within the same cluster are similar according to one or more predesignated criteria while observations drawn from different clusters are dissimilar different clustering techniques make different assumptions on the structure of the data often defined by some similarity metric and evaluated for example by internal compactness or the similarity between members of the same cluster and separation the difference between clusters other methods are based on estimated density and graph connectivity a special type of unsupervised learning called self supervised learning involves training a model by generating the supervisory signal from the data itself semi supervised learning semi supervised learning falls between unsupervised learning without any labeled training data and supervised learning with completely labeled training data some of the training examples are missing training labels yet many machine learning researchers have found that unlabeled data when used in conjunction with a small amount of labeled data can produce a considerable improvement in learning accuracy in weakly supervised learning the training labels are noisy limited or imprecise however these labels are often cheaper to obtain resulting in larger effective training sets reinforcement learning reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward due to its generality the field is studied in many other disciplines such as game theory control theory operations research information theory simulation based optimization multi agent systems swarm intelligence statistics and genetic algorithms in reinforcement learning the environment is typically represented as a markov decision process mdp many reinforcements learning algorithms use dynamic programming techniques reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the mdp and are used when exact models are infeasible reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent dimensionality reduction dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables in other words it is a process of reducing the dimension of the feature set also called the number of features most of the dimensionality reduction techniques can be considered as either feature elimination or extraction one of the popular methods of dimensionality reduction is principal component analysis pca pca involves changing higher dimensional data e g 3d to a smaller space e g 2d the manifold hypothesis proposes that high dimensional data sets lie along low dimensional manifolds and many dimensionality reduction techniques make this assumption leading to the area of manifold learning and manifold regularization other types other approaches have been developed which do not fit neatly into this three fold categorization and sometimes more than one is used by the same machine learning system for example topic modeling meta learning self learning self learning as a machine learning paradigm was introduced in 1982 along with a neural network capable of self learning named crossbar adaptive array caa it is learning with no external rewards and no external teacher advice the caa self learning algorithm computes in a crossbar fashion both decisions about actions and emotions feelings about consequence situations the system is driven by the interaction between cognition and emotion the self learning algorithm updates a memory matrix w w a s such that in each iteration executes the following machine learning routine in situation s perform action a receive a consequence situation s compute emotion of being in the consequence situation v s update crossbar memory w a s w a s v s it is a system with only one input situation and only one output action or behavior a there is neither a separate reinforcement input nor an advice input from the environment the backpropagated value secondary reinforcement is the emotion toward the consequence situation the caa exists in two environments one is the behavioral environment where it behaves and the other is the genetic environment wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment after receiving the genome species vector from the genetic environment the caa learns a goal seeking behavior in an environment that contains both desirable and undesirable situations feature learning several learning algorithms aim at discovering better representations of the inputs provided during training classic examples include principal component analysis and cluster analysis feature learning algorithms also called representation learning algorithms often attempt to preserve the information in their input but also transform it in a way that makes it useful often as a pre processing step before performing classification or predictions this technique allows reconstruction of the inputs coming from the unknown data generating distribution while not being necessarily faithful to configurations that are implausible under that distribution this replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task feature learning can be either supervised or unsupervised in supervised feature learning features are learned using labeled input data examples include artificial neural networks multilayer perceptrons and supervised dictionary learning in unsupervised feature learning features are learned with unlabeled input data examples include dictionary learning independent component analysis autoencoders matrix factorization and various forms of clustering manifold learning algorithms attempt to do so under the constraint that the learned representation is low dimensional sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse meaning that the mathematical model has many zeros multilinear subspace learning algorithms aim to learn low dimensional representations directly from tensor representations for multidimensional data without reshaping them into higher dimensional vectors deep learning algorithms discover multiple levels of representation or a hierarchy of features with higher level more abstract features defined in terms of or generating lower level features it has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process however real world data such as images video and sensory data has not yielded attempts to algorithmically define specific features an alternative is to discover such features or representations through examination without relying on explicit algorithms sparse dictionary learning sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix the method is strongly np hard and difficult to solve approximately a popular heuristic method for sparse dictionary learning is the k svd algorithm sparse dictionary learning has been applied in several contexts in classification the problem is to determine the class to which a previously unseen training example belongs for a dictionary where each class has already been built a new training example is associated with the class that is best sparsely represented by the corresponding dictionary sparse dictionary learning has also been applied in image de noising the key idea is that a clean image patch can be sparsely represented by an image dictionary but the noise cannot anomaly detection in data mining anomaly detection also known as outlier detection is the identification of rare items events or observations which raise suspicions by differing significantly from the majority of the data typically the anomalous items represent an issue such as bank fraud a structural defect medical problems or errors in a text anomalies are referred to as outliers novelties noise deviations and exceptions in particular in the context of abuse and network intrusion detection the interesting objects are often not rare objects but unexpected bursts of inactivity this pattern does not adhere to the common statistical definition of an outlier as a rare object many outlier detection methods in particular unsupervised algorithms will fail on such data unless aggregated appropriately instead a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns three broad categories of anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit the least to the remainder of the data set supervised anomaly detection techniques require a data set that has been labeled as normal and abnormal and involves training a classifier the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection semi supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model robot learning robot learning is inspired by a multitude of machine learning methods starting from supervised learning reinforcement learning and finally meta learning e g maml association rules association rule learning is a rule based machine learning method for discovering relationships between variables in large databases it is intended to identify strong rules discovered in databases using some measure of interestingness rule based machine learning is a general term for any machine learning method that identifies learns or evolves rules to store manipulate or apply knowledge the defining characteristic of a rule based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system this is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction rule based machine learning approaches include learning classifier systems association rule learning and artificial immune systems based on the concept of strong rules rakesh agrawal tomasz imieliński and arun swami introduced association rules for discovering regularities between products in large scale transaction data recorded by point of sale pos systems in supermarkets for example the rule o n i o n s p o t a t o e s b u r g e r displaystyle mathrm onions potatoes rightarrow mathrm burger found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together they are likely to also buy hamburger meat such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements in addition to market basket analysis association rules are employed today in application areas including web usage mining intrusion detection continuous production and bioinformatics in contrast with sequence mining association rule learning typically does not consider the order of items either within a transaction or across transactions learning classifier systems lcs are a family of rule based machine learning algorithms that combine a discovery component typically a genetic algorithm with a learning component performing either supervised learning reinforcement learning or unsupervised learning they seek to identify a set of context dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions inductive logic programming ilp is an approach to rule learning using logic programming as a uniform representation for input examples background knowledge and hypotheses given an encoding of the known background knowledge and a set of examples represented as a logical database of facts an ilp system will derive a hypothesized logic program that entails all positive and no negative examples inductive programming is a related field that considers any kind of programming language for representing hypotheses and not only logic programming such as functional programs inductive logic programming is particularly useful in bioinformatics and natural language processing gordon plotkin and ehud shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting shapiro built their first implementation model inference system in 1981 a prolog program that inductively inferred logic programs from positive and negative examples the term inductive here refers to philosophical induction suggesting a theory to explain observed facts rather than mathematical induction proving a property for all members of a well ordered set models a machine learning model is a type of mathematical model that once trained on a given dataset can be used to make predictions or classifications on new data during training a learning algorithm iteratively adjusts the model s internal parameters to minimize errors in its predictions by extension the term model can refer to several levels of specificity from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned various types of models have been used and researched for machine learning systems picking the best model for a task is called model selection artificial neural networks artificial neural networks anns or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains such systems learn to perform tasks by considering examples generally without being programmed with any task specific rules an ann is a model based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain each connection like the synapses in a biological brain can transmit information a signal from one artificial neuron to another an artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it in common ann implementations the signal at a connection between artificial neurons is a real number and the output of each artificial neuron is computed by some non linear function of the sum of its inputs the connections between artificial neurons are called edges artificial neurons and edges typically have a weight that adjusts as learning proceeds the weight increases or decreases the strength of the signal at a connection artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold typically artificial neurons are aggregated into layers different layers may perform different kinds of transformations on their inputs signals travel from the first layer the input layer to the last layer the output layer possibly after traversing the layers multiple times the original goal of the ann approach was to solve problems in the same way that a human brain would however over time attention moved to performing specific tasks leading to deviations from biology artificial neural networks have been used on a variety of tasks including computer vision speech recognition machine translation social network filtering playing board and video games and medical diagnosis deep learning consists of multiple hidden layers in an artificial neural network this approach tries to model the way the human brain processes light and sound into vision and hearing some successful applications of deep learning are computer vision and speech recognition decision trees decision tree learning uses a decision tree as a predictive model to go from observations about an item represented in the branches to conclusions about the item s target value represented in the leaves it is one of the predictive modeling approaches used in statistics data mining and machine learning tree models where the target variable can take a discrete set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels decision trees where the target variable can take continuous values typically real numbers are called regression trees in decision analysis a decision tree can be used to visually and explicitly represent decisions and decision making in data mining a decision tree describes data but the resulting classification tree can be an input for decision making support vector machines support vector machines svms also known as support vector networks are a set of related supervised learning methods used for classification and regression given a set of training examples each marked as belonging to one of two categories an svm training algorithm builds a model that predicts whether a new example falls into one category an svm training algorithm is a non probabilistic binary linear classifier although methods such as platt scaling exist to use svm in a probabilistic classification setting in addition to performing linear classification svms can efficiently perform a non linear classification using what is called the kernel trick implicitly mapping their inputs into high dimensional feature spaces regression analysis regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features its most common form is linear regression where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares the latter is often extended by regularization methods to mitigate overfitting and bias as in ridge regression when dealing with non linear problems go to models include polynomial regression for example used for trendline fitting in microsoft excel logistic regression often used in statistical classification or even kernel regression which introduces non linearity by taking advantage of the kernel trick to implicitly map input variables to higher dimensional space bayesian networks a bayesian network belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph dag for example a bayesian network could represent the probabilistic relationships between diseases and symptoms given symptoms the network can be used to compute the probabilities of the presence of various diseases efficient algorithms exist that perform inference and learning bayesian networks that model sequences of variables like speech signals or protein sequences are called dynamic bayesian networks generalizations of bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams gaussian processes a gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution and it relies on a pre defined covariance function or kernel that models how pairs of points relate to each other depending on their locations given a set of observed points or input output examples the distribution of the unobserved output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new unobserved point gaussian processes are popular surrogate models in bayesian optimization used to do hyperparameter optimization genetic algorithms a genetic algorithm ga is a search algorithm and heuristic technique that mimics the process of natural selection using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem in machine learning genetic algorithms were used in the 1980s and 1990s conversely machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms belief functions the theory of belief functions also referred to as evidence theory or dempster shafer theory is a general framework for reasoning with uncertainty with understood connections to other frameworks such as probability possibility and imprecise probability theories these theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined e g dempster s rule of combination just like how in a pmf based bayesian approach would combine probabilities however there are many caveats to these beliefs functions when compared to bayesian approaches in order to incorporate ignorance and uncertainty quantification these belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner s decision boundary low samples and ambiguous class issues that standard machine learning approach tend to have difficulty resolving however the computational complexity of these algorithms are dependent on the number of propositions classes and can lead to a much higher computation time when compared to other machine learning approaches training models typically machine learning models require a high quantity of reliable data to perform accurate predictions when training a machine learning model machine learning engineers need to target and collect a large and representative sample of data data from the training set can be as varied as a corpus of text a collection of images sensor data and data collected from individual users of a service overfitting is something to watch out for when training a machine learning model trained models derived from biased or non evaluated data can result in skewed or undesired predictions biased models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives algorithmic bias is a potential result of data not being fully prepared for training machine learning ethics is becoming a field of study and notably becoming integrated within machine learning engineering teams federated learning federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process allowing for users privacy to be maintained by not needing to send their data to a centralized server this also increases efficiency by decentralizing the training process to many devices for example gboard uses federated machine learning to train search query prediction models on users mobile phones without having to send individual searches back to google applications there are many applications for machine learning including in 2006 the media services provider netflix held the first netflix prize competition to find a program to better predict user preferences and improve the accuracy of its existing cinematch movie recommendation algorithm by at least 10 a joint team made up of researchers from at t labs research in collaboration with the teams big chaos and pragmatic theory built an ensemble model to win the grand prize in 2009 for 1 million shortly after the prize was awarded netflix realized that viewers ratings were not the best indicators of their viewing patterns everything is a recommendation and they changed their recommendation engine accordingly in 2010 the wall street journal wrote about the firm rebellion research and their use of machine learning to predict the financial crisis in 2012 co founder of sun microsystems vinod khosla predicted that 80 of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software in 2014 it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists in 2019 springer nature published the first research book created using machine learning in 2020 machine learning technology was used to help make diagnoses and aid researchers in developing a cure for covid 19 machine learning was recently applied to predict the pro environmental behavior of travelers recently machine learning technology was also applied to optimize smartphone s performance and thermal behavior based on the user s interaction with the phone when applied correctly machine learning algorithms mlas can utilize a wide range of company characteristics to predict stock returns without overfitting by employing effective feature engineering and combining forecasts mlas can generate results that far surpass those obtained from basic linear techniques like ols recent advancements in machine learning have extended into the field of quantum chemistry where novel algorithms now enable the prediction of solvent effects on chemical reactions thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes machine learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes other applications have been focusing on pre evacuation decisions in building fires limitations although machine learning has been transformative in some fields machine learning programs often fail to deliver expected results reasons for this are numerous lack of suitable data lack of access to the data data bias privacy problems badly chosen tasks and algorithms wrong tools and people lack of resources and evaluation problems the black box theory poses another yet significant challenge black box refers to a situation where the algorithm or the process of producing an output is entirely opaque meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data the house of lords select committee which claimed that such an intelligence system that could have a substantial impact on an individual s life would not be considered acceptable unless it provided a full and satisfactory explanation for the decisions it makes in 2018 a self driving car from uber failed to detect a pedestrian who was killed after a collision attempts to use machine learning in healthcare with the ibm watson system failed to deliver even after years of time and billions of dollars invested microsoft s bing chat chatbot has been reported to produce hostile and offensive response against its users machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature while it has improved with training sets it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves explainability explainable ai xai or interpretable ai or explainable machine learning xml is artificial intelligence ai in which humans can understand the decisions or predictions made by the ai it contrasts with the black box concept in machine learning where even its designers cannot explain why an ai arrived at a specific decision by refining the mental models of users of ai powered systems and dismantling their misconceptions xai promises to help users perform more effectively xai may be an implementation of the social right to explanation overfitting settling on a bad overly complex theory gerrymandered to fit all the past training data is known as overfitting many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is other limitations and vulnerabilities learners can also disappoint by learning the wrong lesson a toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses a real world example is that unlike humans current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture and they learn relationships between pixels that humans are oblivious to but that still correlate with images of certain types of real objects modifying these patterns on a legitimate image can result in adversarial images that the system misclassifies adversarial vulnerabilities can also result in nonlinear systems or from non pattern perturbations for some systems it is possible to change the output by only changing a single adversarially chosen pixel machine learning models are often vulnerable to manipulation and or evasion via adversarial machine learning researchers have demonstrated how backdoors can be placed undetectably into classifying e g for categories spam and well visible not spam of posts machine learning models that are often developed and or trained by third parties parties can change the classification of any input including in cases for which a type of data software transparency is provided possibly including white box access model assessments classification of machine learning models can be validated by accuracy estimation techniques like the holdout method which splits the data in a training and test set conventionally 2 3 training set and 1 3 test set designation and evaluates the performance of the training model on the test set in comparison the k fold cross validation method randomly partitions the data into k subsets and then k experiments are performed each respectively considering 1 subset for evaluation and the remaining k 1 subsets for training the model in addition to the holdout and cross validation methods bootstrap which samples n instances with replacement from the dataset can be used to assess model accuracy in addition to overall accuracy investigators frequently report sensitivity and specificity meaning true positive rate tpr and true negative rate tnr respectively similarly investigators sometimes report the false positive rate fpr as well as the false negative rate fnr however these rates are ratios that fail to reveal their numerators and denominators receiver operating characteristic roc along with the accompanying area under the roc curve auc offer additional tools for classification model assessment higher auc is associated with a better performing model ethics bias different machine learning approaches can suffer from different data biases a machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data when trained on human made data machine learning is likely to pick up the constitutional and unconscious biases already present in society systems that are trained on datasets collected with biases may exhibit these biases upon use algorithmic bias thus digitizing cultural prejudices for example in 1988 the uk s commission for racial equality found that st george s medical school had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non european sounding names using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants another example includes predictive policing company geolitica s predictive algorithm that resulted in disproportionately high levels of over policing in low income and minority communities after being trained with historical crime data while responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning some researchers blame lack of participation and representation of minority population in the field of ai for machine learning s vulnerability to biases in fact according to research carried out by the computing research association cra in 2021 female faculty merely make up 16 1 of all faculty members who focus on ai among several universities around the world furthermore among the group of new u s resident ai phd graduates 45 identified as white 22 4 as asian 3 2 as hispanic and 2 4 as african american which further demonstrates a lack of diversity in the field of ai language models learned from data have been shown to contain human like biases because human languages contain biases machines trained on language corpora will necessarily also learn these biases in 2016 microsoft tested tay a chatbot that learned from twitter and it quickly picked up racist and sexist language in an experiment carried out by propublica an investigative journalism organization a machine learning algorithm s insight into the recidivism rates among prisoners falsely flagged black defendants high risk twice as often as white defendants in 2015 google photos once tagged a couple of black people as gorillas which caused controversy the gorilla label was subsequently removed and in 2023 it still cannot recognize gorillas similar issues with recognizing non white people have been found in many other systems because of such challenges the effective use of machine learning may take longer to be adopted in other domains concern for fairness in machine learning that is reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists including fei fei li who said that here s nothing artificial about ai it s inspired by people it s created by people and most importantly it impacts people it is a powerful tool we are only just beginning to understand and that is a profound responsibility financial incentives there are concerns among health care professionals that these systems might not be designed in the public s interest but as income generating machines this is especially true in the united states where there is a long standing ethical dilemma of improving health care but also increasing profits for example the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm s proprietary owners hold stakes there is potential for machine learning in health care to provide professionals an additional tool to diagnose medicate and plan recovery paths for patients but this requires these biases to be mitigated hardware since the 2010s advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks a particular narrow subdomain of machine learning that contain many layers of nonlinear hidden units by 2019 graphics processing units gpus often with ai specific enhancements had displaced cpus as the dominant method of training large scale commercial cloud ai openai estimated the hardware compute used in the largest deep learning projects from alexnet 2012 to alphazero 2017 and found a 300 000 fold increase in the amount of compute required with a doubling time trendline of 3 4 months neuromorphic computing neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks these systems may be implemented through software based simulations on conventional hardware or through specialized hardware architectures physical neural networks a physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials such as memristors to emulate the function of neural synapses the term physical neural network highlights the use of physical hardware for computation as opposed to software based implementations it broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses embedded machine learning embedded machine learning is a sub field of machine learning where models are deployed on embedded systems with limited computing resources such as wearable computers edge devices and microcontrollers running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing thereby reducing the risk of data breaches privacy leaks and theft of intellectual property personal data and business secrets embedded machine learning can be achieved through various techniques such as hardware acceleration approximate computing and model optimization common optimization techniques include pruning quantization knowledge distillation low rank factorization network architecture search and parameter sharing software software suites containing a variety of machine learning algorithms include the following free and open source software proprietary software with free and open source editions knime rapidminer proprietary software journals journal of machine learning research machine learning nature machine intelligence neural computation ieee transactions on pattern analysis and machine intelligence conferences aaai conference on artificial intelligence association for computational linguistics acl european conference on machine learning and principles and practice of knowledge discovery in databases ecml pkdd international conference on computational intelligence methods for bioinformatics and biostatistics cibb international conference on machine learning icml international conference on learning representations iclr international conference on intelligent robots and systems iros conference on knowledge discovery and data mining kdd conference on neural information processing systems neurips see also automated machine learning process of automating the application of machine learning big data extremely large or complex datasets deep learning branch of ml concerned with artificial neural networks differentiable programming programming paradigm list of datasets for machine learning research m theory learning framework machine unlearning references sources domingos pedro september 22 2015 the master algorithm how the quest for the ultimate learning machine will remake our world basic books isbn 978 0465065707 nilsson nils 1998 artificial intelligence a new synthesis morgan kaufmann isbn 978 1 55860 467 4 archived from the original on 26 july 2020 retrieved 18 november 2019 poole david mackworth alan goebel randy 1998 computational intelligence a logical approach new york oxford university press isbn 978 0 19 510270 3 archived from the original on 26 july 2020 retrieved 22 august 2020 russell stuart j norvig peter 2003 artificial intelligence a modern approach 2nd ed upper saddle river new jersey prentice hall isbn 0 13 790395 2 further reading external links international machine learning society mloss is an academic database of open source machine learning software ",
            "total_words": 8288,
            "unique_words_percentage": 25.108590733590734,
            "stopwords_percentage": 36.66747104247104
        },
        {
            "title": "Quantum machine learning",
            "link": "https://en.wikipedia.org/wiki/Quantum_machine_learning",
            "content": "quantum machine learning is the integration of quantum algorithms within machine learning programs the most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer i e quantum enhanced machine learning while machine learning algorithms are used to compute immense quantities of data quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program this includes hybrid methods that involve both classical and quantum processing where computationally difficult subroutines are outsourced to a quantum device these routines can be more complex in nature and executed faster on a quantum computer furthermore quantum algorithms can be used to analyze quantum states instead of classical data beyond quantum computing the term quantum machine learning is also associated with classical machine learning methods applied to data generated from quantum experiments i e machine learning of quantum systems such as learning the phase transitions of a quantum system or creating new quantum experiments quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems in particular neural networks for example some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa furthermore researchers investigate more abstract notions of learning theory with respect to quantum information sometimes referred to as quantum learning theory machine learning with quantum computers quantum enhanced machine learning refers to quantum algorithms that solve tasks in machine learning thereby improving and often expediting classical machine learning techniques such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing subsequently quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system for example the outcome of the measurement of a qubit reveals the result of a binary classification task while many proposals of quantum machine learning algorithms are still purely theoretical and require a full scale universal quantum computer to be tested others have been implemented on small scale or special purpose quantum devices quantum associative memories and quantum pattern recognition associative or content addressable memories are able to recognize stored content on the basis of a similarity measure rather than fixed addresses like in random access memories as such they must be able to retrieve both incomplete and corrupted patterns the essential machine learning task of pattern recognition typical classical associative memories store p patterns in the o n 2 displaystyle o n 2 interactions synapses of a real symmetric energy matrix over a network of n artificial neurons the encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy starting from an initial configuration unfortunately classical associative memories are severely limited by the phenomenon of cross talk when too many patterns are stored spurious memories appear which quickly proliferate so that the energy landscape becomes disordered and no retrieval is anymore possible the number of storable patterns is typically limited by a linear function of the number of neurons p o n displaystyle p leq o n quantum associative memories in their simplest realization store patterns in a unitary matrix u acting on the hilbert space of n qubits retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input by its very quantum nature the retrieval process is thus probabilistic because quantum associative memories are free from cross talk however spurious memories are never generated correspondingly they have a superior capacity than classical ones the number of parameters in the unitary matrix u is o p n displaystyle o pn one can thus have efficient spurious memory free quantum associative memories for any polynomial number of patterns linear algebra simulation with quantum amplitudes a number of quantum algorithms for machine learning are based on the idea of amplitude encoding that is to associate the amplitudes of a quantum state with the inputs and outputs of computations since a state of n displaystyle n qubits is described by 2 n displaystyle 2 n complex amplitudes this information encoding can allow for an exponentially compact representation intuitively this corresponds to associating a discrete probability distribution over binary random variables with a classical vector the goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits n displaystyle n which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input many quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations colloquially called hhl after the paper s authors which under specific conditions performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix one of these conditions is that a hamiltonian which entry wise corresponds to the matrix can be simulated efficiently which is known to be possible if the matrix is sparse or low rank for reference any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix e g o n 2 373 displaystyle o mathord left n 2 373 right but they are not restricted to sparse matrices quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations for example in least squares linear regression the least squares version of support vector machines and gaussian processes a crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset although efficient methods for state preparation are known for specific cases this step easily hides the complexity of the task variational quantum algorithms vqas vqas are one of the most studied classes of quantum algorithms as modern research demonstrates their applicability to the vast majority of known major applications of the quantum computer and they appear to be a leading hope for gaining quantum supremacy vqas are a mixed quantum classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer vqas are considered best for nisq as vqas are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits researchers have studied circuit based algorithms to solve optimization problems and find the ground state energy of complex systems which were difficult to solve or required a large time to perform the computation using a classical computer variational quantum circuits vqcs variational quantum circuits also known as parametrized quantum circuits pqcs are based on variational quantum algorithms vqas vqcs consist of three parts preparation of initial states quantum circuit and measurement researchers are extensively studying vqcs as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts it is theoretically and numerically proven that we can approximate non linear functions like those used in neural networks on quantum circuits due to vqcs superiority neural network has been replaced by vqcs in reinforcement learning tasks and generative algorithms the intrinsic nature of quantum devices towards decoherence random gate error and measurement errors caused to have high potential to limit the training of the variation circuits training the vqcs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training quantum binary classifier pattern reorganization is one of the important tasks of machine learning binary classification is one of the tools or algorithms to find patterns binary classification is used in supervised learning and in unsupervised learning in quantum machine learning classical bits are converted to qubits and they are mapped to hilbert space complex value data are used in a quantum binary classifier to use the advantage of hilbert space by exploiting the quantum mechanic properties such as superposition entanglement interference the quantum binary classifier produces the accurate result in short period of time quantum machine learning algorithms based on grover search another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on grover s search algorithm which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms these quantum routines can be employed for learning algorithms that translate into an unstructured search task as can be done for instance in the case of the k medians and the k nearest neighbors algorithms other applications include quadratic speedups in the training of perceptron and the computation of attention an example of amplitude amplification being used in a machine learning algorithm is grover s search algorithm minimization in which a subroutine uses grover s search algorithm to find an element less than some previously defined element this can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one grover s algorithm can then find an element such that our condition is met the minimization is initialized by some random element in our data set and iteratively does this subroutine to find the minimum element in the data set this minimization is notably used in quantum k medians and it has a speed up of at least o n k displaystyle mathcal o left sqrt frac n k right compared to classical versions of k medians where n displaystyle n is the number of data points and k displaystyle k is the number of clusters amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup quantum walks have been proposed to enhance google s pagerank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework quantum enhanced reinforcement learning reinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning which also admits quantum enhancements in quantum enhanced reinforcement learning a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions which allows the agent to adapt its behavior in other words to learn what to do in order to gain more rewards in some situations either because of the quantum processing capability of the agent or due to the possibility to probe the environment in superpositions a quantum speedup may be achieved implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits a quantum speedup of the agent s internal decision making time has been experimentally demonstrated in trapped ions while a quantum speedup of the learning time in a fully coherent quantum interaction between agent and environment has been experimentally realized in a photonic setup quantum annealing quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions this is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function the process can be distinguished from simulated annealing by the quantum tunneling process by which particles tunnel through kinetic or potential barriers from a high state to a low state quantum annealing starts from a superposition of all possible states of a system weighted equally then the time dependent schrödinger equation guides the time evolution of the system serving to affect the amplitude of each state as time increases eventually the ground state can be reached to yield the instantaneous hamiltonian of the system nisq circuit as quantum model as the depth of the quantum circuit advances on nisq devices the noise level rises posing a significant challenge to accurately computing costs and gradients on training models the noise tolerance will be improved by using the quantum perceptron and the quantum algorithm on the currently accessible quantum hardware a regular connection of similar components known as neurons forms the basis of even the most complex brain networks typically a neuron has two operations the inner product and an activation function as opposed to the activation function which is typically nonlinear the inner product is a linear process with quantum computing linear processes may be easily accomplished additionally due to the simplicity of implementation the threshold function is preferred by the majority of quantum neurons for activation functions quantum sampling techniques sampling from high dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science engineering and society examples include deep learning probabilistic programming and other machine learning and artificial intelligence applications a computationally hard problem which is key for some relevant machine learning tasks is the estimation of averages over probabilistic models defined in terms of a boltzmann distribution sampling from generic probabilistic models is hard algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become even though quantum annealers like those produced by d wave systems were designed for challenging combinatorial optimization problems it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects some research groups have recently explored the use of quantum annealing hardware for training boltzmann machines and deep neural networks the standard approach to training boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques such as markov chain monte carlo algorithms another possibility is to rely on a physical process like quantum annealing that naturally generates samples from a boltzmann distribution the objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset the d wave 2x system hosted at nasa ames research center has been recently used for the learning of a special class of restricted boltzmann machines that can serve as a building block for deep learning architectures complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks the same device was later used to train a fully connected boltzmann machine to generate reconstruct and classify down scaled low resolution handwritten digits among other synthetic datasets in both cases the models trained by quantum annealing had a similar or better performance in terms of quality the ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward reverse annealing has been used as well to solve a fully connected quantum restricted boltzmann machine inspired by the success of boltzmann machines based on classical boltzmann distribution a new machine learning approach based on quantum boltzmann distribution of a transverse field ising hamiltonian was recently proposed due to the non commutative nature of quantum mechanics the training process of the quantum boltzmann machine can become nontrivial this problem was to some extent circumvented by introducing bounds on the quantum probabilities allowing the authors to train the model efficiently by sampling it is possible that a specific type of quantum boltzmann machine has been trained in the d wave 2x by using a learning rule analogous to that of classical boltzmann machines quantum annealing is not the only technology for sampling in a prepare and measure scenario a universal quantum computer prepares a thermal state which is then sampled by measurements this can reduce the time required to train a deep restricted boltzmann machine and provide a richer and more comprehensive framework for deep learning than classical computing the same quantum methods also permit efficient training of full boltzmann machines and multi layer fully connected models and do not have well known classical counterparts relying on an efficient thermal state preparation protocol starting from an arbitrary state quantum enhanced markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first order logic template this provides an exponential reduction in computational complexity in probabilistic inference and while the protocol relies on a universal quantum computer under mild assumptions it can be embedded on contemporary quantum annealing hardware quantum neural networks quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks the term is claimed by a wide range of approaches including the implementation and extension of neural networks using photons layered variational circuits or quantum ising type models quantum neural networks are often defined as an expansion on deutsch s model of a quantum computational network within this model nonlinear and irreversible gates dissimilar to the hamiltonian operator are deployed to speculate the given data set such gates make certain phases unable to be observed and generate specific oscillations quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing current research shows that qnn can exponentially increase the amount of computing power and the degrees of freedom for a computer which is limited for a classical computer to its size a quantum neural network has computational capabilities to decrease the number of steps qubits used and computation time the wave function to quantum mechanics is the neuron for neural networks to test quantum applications in a neural network quantum dot molecules are deposited on a substrate of gaas or similar to record how they communicate with one another each quantum dot can be referred as an island of electric activity and when such dots are close enough approximately 10 20 nm electrons can tunnel underneath the islands an even distribution across the substrate in sets of two create dipoles and ultimately two spin states up or down these states are commonly known as qubits with corresponding states of 0 displaystyle 0 rangle and 1 displaystyle 1 rangle in dirac notation quantum convolution neural network a novel design for multi dimensional vectors that uses circuits as convolution filters is qcnn it was inspired by the advantages of cnns and the power of qml it is made using a combination of a variational quantum circuit vqc and a deep neural network dnn fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits the main strategy is to carry out an iterative optimization process in the nisq devices without the negative impact of noise which is possibly incorporated into the circuit parameter and without the need for quantum error correction the quantum circuit must effectively handle spatial information in order for qcnn to function as cnn the convolution filter is the most basic technique for making use of spatial information one or more quantum convolutional filters make up a quantum convolutional neural network qcnn and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way three parts that make up the quantum convolutional filter are the encoder the parameterized quantum circuit pqc and the measurement the quantum convolutional filter can be seen as an extension of the filter in the traditional cnn because it was designed with trainable parameters quantum neural networks take advantage of the hierarchical structures and for each subsequent layer the number of qubits from the preceding layer is decreased by a factor of two for n input qubits these structure have o log n layers allowing for shallow circuit depth additionally they are able to avoid barren plateau one of the most significant issues with pqc based algorithms ensuring trainability despite the fact that the qcnn model does not include the corresponding quantum operation the fundamental idea of the pooling layer is also offered to assure validity in qcnn architecture the pooling layer is typically placed between succeeding convolutional layers its function is to shrink the representation s spatial size while preserving crucial features which allows it to reduce the number of parameters streamline network computing and manage over fitting such process can be accomplished applying full tomography on the state to reduce it all the way down to one qubit and then processed it in subway the most frequently used unit type in the pooling layer is max pooling although there are other types as well similar to conventional feed forward neural networks the last module is a fully connected layer with full connections to all activations in the preceding layer translational invariance which requires identical blocks of parameterized quantum gates within a layer is a distinctive feature of the qcnn architecture dissipative quantum neural network dissipative qnns dqnns are constructed from layers of qubits coupled by perceptron called building blocks which have an arbitrary unitary design each node in the network layer of a dqnn is given a distinct collection of qubits and each qubit is also given a unique quantum perceptron unitary to characterize it the input states information are transported through the network in a feed forward fashion layer to layer transition mapping on the qubits of the two adjacent layers as the name implies dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer when performing a broad supervised learning task dqnn are used to learn a unitary matrix connecting the input and output quantum states the training data for this task consists of the quantum state and the corresponding classical labels inspired by the extremely successful classical generative adversarial network gan dissipative quantum generative adversarial network dqgan is introduced for unsupervised learning of the unlabeled training data the generator and the discriminator are the two dqnns that make up a single dqgan the generator s goal is to create false training states that the discriminator cannot differentiate from the genuine ones while the discriminator s objective is to separate the real training states from the fake states created by the generator the relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set dqgan has a fully quantum architecture and is trained in quantum data hidden quantum markov models hidden quantum markov models hqmms are a quantum enhanced version of classical hidden markov models hmms which are typically used to model sequential data in various fields like robotics and natural language processing unlike the approach taken by other quantum enhanced machine learning algorithms hqmms can be viewed as models inspired by quantum mechanics that can be run on classical computers as well where classical hmms use probability vectors to represent hidden belief states hqmms use the quantum analogue density matrices recent work has shown that these models can be successfully learned by maximizing the log likelihood of the given data via classical optimization and there is some empirical evidence that these models can better model sequential data compared to classical hmms in practice although further work is needed to determine exactly when and how these benefits are derived additionally since classical hmms are a particular kind of bayes net an exciting aspect of hqmms is that the techniques used show how we can perform quantum analogous bayesian inference which should allow for the general construction of the quantum versions of probabilistic graphical models fully quantum machine learning in the most general case of quantum machine learning both the learning device and the system under study as well as their interaction are fully quantum this section gives a few examples of results on this topic one class of problem that can benefit from the fully quantum approach is that of learning unknown quantum states processes or measurements in the sense that one can subsequently reproduce them on another quantum system for example one may wish to learn a measurement that discriminates between two coherent states given not a classical description of the states to be discriminated but instead a set of example quantum systems prepared in these states the naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information this would only require classical learning however one can show that a fully quantum approach is strictly superior in this case this also relates to work on quantum pattern matching the problem of learning unitary transformations can be approached in a similar way going beyond the specific problem of learning states and transformations the task of clustering also admits a fully quantum version wherein both the oracle which returns the distance between data points and the information processing device which runs the algorithm are quantum finally a general framework spanning supervised unsupervised and reinforcement learning in the fully quantum setting was introduced in where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning such a speedup in the reinforcement learning paradigm has been experimentally demonstrated in a photonic setup explainable quantum machine learning the need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning or xqml in analogy to xai xml these efforts are often also referred to as interpretable machine learning iml and by extension iqml xqml iqml can be considered as an alternative research direction instead of finding a quantum advantage for example xqml has been used in the context of mobile malware detection and classification quantum shapley values have also been proposed to interpret gates within a circuit based on a game theoretic approach for this purpose gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest additionally a quantum version of the classical technique known as lime linear interpretable model agnostic explanations has also been proposed known as q lime classical learning applied to quantum problems the term quantum machine learning sometimes refers to classical machine learning performed on data from quantum systems a basic example of this is quantum state tomography where a quantum state is learned from measurement other applications include learning hamiltonians and automatically generating quantum experiments quantum learning theory quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed ups or other improvements that they may provide the framework is very similar to that of classical computational learning theory but the learner in this case is a quantum information processing device while the data may be either classical or quantum quantum learning theory should be contrasted with the quantum enhanced machine learning discussed above where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems although quantum learning theory is still under development partial results in this direction have been obtained the starting point in learning theory is typically a concept class a set of possible concepts usually a concept is a function on some domain such as 0 1 n displaystyle 0 1 n for example the concept class could be the set of disjunctive normal form dnf formulas on n bits or the set of boolean circuits of some constant depth the goal for the learner is to learn exactly or approximately an unknown target concept from this concept class the learner may be actively interacting with the target concept or passively receiving samples from it in active learning a learner can make membership queries to the target concept c asking for its value c x on inputs x chosen by the learner the learner then has to reconstruct the exact target concept with high probability in the model of quantum exact learning the learner can make membership queries in quantum superposition if the complexity of the learner is measured by the number of membership queries it makes then quantum exact learners can be polynomially more efficient than classical learners for some concept classes but not more if complexity is measured by the amount of time the learner uses then there are concept classes that can be learned efficiently by quantum learners but not by classical learners under plausible complexity theoretic assumptions a natural model of passive learning is valiant s probably approximately correct pac learning here the learner receives random examples x c x where x is distributed according to some unknown distribution d the learner s goal is to output a hypothesis function h such that h x c x with high probability when x is drawn according to d the learner has to be able to produce such an approximately correct h for every d and every target concept c in its concept class we can consider replacing the random examples by potentially more powerful quantum examples x d x x c x displaystyle sum _ x sqrt d x x c x rangle in the pac model and the related agnostic model this doesn t significantly reduce the number of examples needed for every concept class classical and quantum sample complexity are the same up to constant factors however for learning under some fixed distribution d quantum examples can be very helpful for example for learning dnf under the uniform distribution when considering time complexity there exist concept classes that can be pac learned efficiently by quantum learners even from classical examples but not by classical learners again under plausible complexity theoretic assumptions this passive learning type is also the most common scheme in supervised learning a learning algorithm typically takes the training examples fixed without the ability to query the label of unlabelled examples outputting a hypothesis h is a step of induction classically an inductive model splits into a training and an application phase the model parameters are estimated in the training phase and the learned model is applied an arbitrary many times in the application phase in the asymptotic limit of the number of applications this splitting of phases is also present with quantum resources implementations and experiments the earliest experiments were conducted using the adiabatic d wave quantum computer for instance to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009 many experiments followed on the same architecture and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations in 2013 google research nasa and the universities space research association launched the quantum artificial intelligence lab which explores the use of the adiabatic d wave quantum computer a more recent example trained a probabilistic generative models with arbitrary pairwise connectivity showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits using a different annealing technology based on nuclear magnetic resonance nmr a quantum hopfield network was implemented in 2009 that mapped the input data and memorized data to hamiltonians allowing the use of adiabatic quantum computation nmr technology also enables universal quantum computing and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number 6 and 9 on a liquid state quantum computer in 2015 the training data involved the pre processing of the image which maps them to normalized 2 dimensional vectors to represent the images as the states of a qubit the two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image once the vectors are defined on the feature space the quantum support vector machine was implemented to classify the unknown input vector the readout avoids costly quantum tomography by reading out the final state in terms of direction up down of the nmr signal photonic implementations are attracting more attention not the least because they do not require extensive cooling simultaneous spoken digit and speaker recognition and chaotic time series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013 using non linear photonics to implement an all optical linear classifier a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule a core building block in many learning algorithms is to calculate the distance between two vectors this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015 recently based on a neuromimetic approach a novel ingredient has been added to the field of quantum machine learning in the form of a so called quantum memristor a quantized model of the standard classical memristor this device can be constructed by means of a tunable resistor weak measurements on the system and a classical feed forward mechanism an implementation of a quantum memristor in superconducting circuits has been proposed and an experiment with quantum dots performed a quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network since 2016 ibm has launched an online cloud based platform for quantum software developers called the ibm q experience this platform consists of several fully operational quantum processors accessible via the ibm web api in doing so the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities new architectures are being explored on an experimental basis up to 32 qubits using both trapped ion and superconductive quantum computing methods in october 2019 it was noted that the introduction of quantum random number generators qrngs to machine learning models including neural networks and convolutional neural networks for random initial weight distribution and random forests for splitting processes had a profound effect on their ability when compared to the classical method of pseudorandom number generators prngs however in a more recent publication from 2021 these claims could not be reproduced for neural network weight initialization and no significant advantage of using qrngs over prngs was found the work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non trivial task on nisq devices and qrngs are therefore typically much more difficult to use in practice than prngs a paper published in december 2018 reported on an experiment using a trapped ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware in march 2021 a team of researchers from austria the netherlands the us and germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment the relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor skepticism while machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research quantum machine learning remains a purely theoretical field of studies attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient further another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random this creates an often considerable overhead as many executions of a quantum learning model have to be aggregated to obtain an actual prediction many of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future sophia chen collected some of the statements made by well known scientists in the field i think we haven t done our homework yet this is an extremely new scientific field physicist maria schuld of canada based quantum computing startup xanadu when mixing machine learning with quantum you catalyse a hype condensate jacob biamonte a contributor to the theory of quantum computation there is a lot more work that needs to be done before claiming quantum machine learning will actually work computer scientist iordanis kerenidis the head of quantum algorithms at the silicon valley based quantum computing startup qc ware i have not seen a single piece of evidence that there exists a meaningful task for which it would make sense to use a quantum computer and not a classical computer physicist ryan sweke of the free university of berlin in germany don t fall for the hype frank zickert who is the author of probably the most practical book related to the subject beware that quantum computers are far away from advancing machine learning for their representation ability and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved furthermore nobody among the active researchers in the field make any forecasts about when it could possibly become practical see also differentiable programming quantum computing quantum algorithm for linear systems of equations quantum annealing quantum neural network quantum image references ",
            "total_words": 6232,
            "unique_words_percentage": 23.66816431322208,
            "stopwords_percentage": 38.84788189987163
        },
        {
            "title": "Neural network (machine learning)",
            "link": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
            "content": "in machine learning a neural network also artificial neural network or neural net abbreviated ann or nn is a model inspired by the structure and function of biological neural networks in animal brains an ann consists of connected units or nodes called artificial neurons which loosely model the neurons in the brain artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance these are connected by edges which model the synapses in the brain each artificial neuron receives signals from connected neurons then processes them and sends a signal to other connected neurons the signal is a real number and the output of each neuron is computed by some non linear function of the sum of its inputs called the activation function the strength of the signal at each connection is determined by a weight which adjusts during the learning process typically neurons are aggregated into layers different layers may perform different transformations on their inputs signals travel from the first layer the input layer to the last layer the output layer possibly passing through multiple intermediate layers hidden layers a network is typically called a deep neural network if it has at least two hidden layers artificial neural networks are used for various tasks including predictive modeling adaptive control and solving problems in artificial intelligence they can learn from experience and can derive conclusions from a complex and seemingly unrelated set of information training neural networks are typically trained through empirical risk minimization this method is based on the idea of optimizing the network s parameters to minimize the difference or empirical risk between the predicted output and the actual target values in a given dataset gradient based methods such as backpropagation are usually used to estimate the parameters of the network during the training phase anns learn from labeled training data by iteratively updating their parameters to minimize a defined loss function this method allows the network to generalize to unseen data history early work today s deep neural networks are based on early work in statistics over 200 years ago the simplest kind of feedforward neural network fnn is a linear network which consists of a single layer of output nodes with linear activation functions the inputs are fed directly to the outputs via a series of weights the sum of the products of the weights and the inputs is calculated at each node the mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights this technique has been known for over two centuries as the method of least squares or linear regression it was used as a means of finding a good rough linear fit to a set of points by legendre 1805 and gauss 1795 for the prediction of planetary movement historically digital computers such as the von neumann model operate via the execution of explicit instructions with access to memory by a number of processors some neural networks on the other hand originated from efforts to model information processing in biological systems through the framework of connectionism unlike the von neumann model connectionist computing does not separate memory and processing warren mcculloch and walter pitts 1943 considered a non learning computational model for neural networks this model paved the way for research to split into two approaches one approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence in the late 1940s d o hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as hebbian learning it was used in many early neural networks such as rosenblatt s perceptron and the hopfield network farley and clark 1954 used computational machines to simulate a hebbian network other neural network computational machines were created by rochester holland habit and duda 1956 in 1958 psychologist frank rosenblatt described the perceptron one of the first implemented artificial neural networks funded by the united states office of naval research r d joseph 1960 mentions an even earlier perceptron like device by farley and clark farley and clark of mit lincoln laboratory actually preceded rosenblatt in the development of a perceptron like device however they dropped the subject the perceptron raised public excitement for research in artificial neural networks causing the us government to drastically increase funding this contributed to the golden age of ai fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence the first perceptrons did not have adaptive hidden units however joseph 1960 also discussed multilayer perceptrons with an adaptive hidden layer rosenblatt 1962 section 16 cited and adopted these ideas also crediting work by h d block and b w knight unfortunately these early efforts did not lead to a working learning algorithm for hidden units i e deep learning deep learning breakthroughs in the 1960s and 1970s fundamental research was conducted on anns in the 1960s and 1970s the first working deep learning algorithm was the group method of data handling a method to train arbitrarily deep neural networks published by alexey ivakhnenko and lapa in the soviet union 1965 they regarded it as a form of polynomial regression or a generalization of rosenblatt s perceptron a 1971 paper described a deep network with eight layers trained by this method which is based on layer by layer training through regression analysis superfluous hidden units are pruned using a separate validation set since the activation functions of the nodes are kolmogorov gabor polynomials these were also the first deep networks with multiplicative units or gates the first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by shun ichi amari in computer experiments conducted by amari s student saito a five layer mlp with two modifiable layers learned internal representations to classify non linearily separable pattern classes subsequent developments in hardware and hyperparameter tunings have made end to end stochastic gradient descent the currently dominant training technique in 1969 kunihiko fukushima introduced the relu rectified linear unit activation function the rectifier has become the most popular activation function for deep learning nevertheless research stagnated in the united states following the work of minsky and papert 1969 who emphasized that basic perceptrons were incapable of processing the exclusive or circuit this insight was irrelevant for the deep networks of ivakhnenko 1965 and amari 1967 in 1976 transfer learning was introduced in neural networks learning deep learning architectures for convolutional neural networks cnns with convolutional layers and downsampling layers and weight replication began with the neocognitron introduced by kunihiko fukushima in 1979 though not trained by backpropagation backpropagation backpropagation is an efficient application of the chain rule derived by gottfried wilhelm leibniz in 1673 to networks of differentiable nodes the terminology back propagating errors was actually introduced in 1962 by rosenblatt but he did not know how to implement this although henry j kelley had a continuous precursor of backpropagation in 1960 in the context of control theory in 1970 seppo linnainmaa published the modern form of backpropagation in his master thesis 1970 g m ostrovski et al republished it in 1971 paul werbos applied backpropagation to neural networks in 1982 his 1974 phd thesis reprinted in a 1994 book did not yet describe the algorithm in 1986 david e rumelhart et al popularised backpropagation but did not cite the original work convolutional neural networks kunihiko fukushima s convolutional neural network cnn architecture of 1979 also introduced max pooling a popular downsampling procedure for cnns cnns have become an essential tool for computer vision the time delay neural network tdnn was introduced in 1987 by alex waibel to apply cnn to phoneme recognition it used convolutions weight sharing and backpropagation in 1988 wei zhang applied a backpropagation trained cnn to alphabet recognition in 1989 yann lecun et al created a cnn called lenet for recognizing handwritten zip codes on mail training required 3 days in 1990 wei zhang implemented a cnn on optical computing hardware in 1991 a cnn was applied to medical image object segmentation and breast cancer detection in mammograms lenet 5 1998 a 7 level cnn by yann lecun et al that classifies digits was applied by several banks to recognize hand written numbers on checks digitized in 32 32 pixel images from 1988 onward the use of neural networks transformed the field of protein structure prediction in particular when the first cascading networks were trained on profiles matrices produced by multiple sequence alignments recurrent neural networks one origin of rnn was statistical mechanics in 1972 shun ichi amari proposed to modify the weights of an ising model by hebbian learning rule as a model of associative memory adding in the component of learning this was popularized as the hopfield network by john hopfield 1982 another origin of rnn was neuroscience the word recurrent is used to describe loop like structures in anatomy in 1901 cajal observed recurrent semicircles in the cerebellar cortex hebb considered reverberating circuit as an explanation for short term memory the mcculloch and pitts paper 1943 considered neural networks that contains cycles and noted that the current activity of such networks can be affected by activity indefinitely far in the past in 1982 a recurrent neural network with an array architecture rather than a multilayer perceptron architecture named crossbar adaptive array used direct recurrent connections from the output to the supervisor teaching inputs in addition of computing actions decisions it computed internal state evaluations emotions of the consequence situations eliminating the external supervisor it introduced the self learning method in neural networks in cognitive psychology the journal american psychologist in early 1980 s carried out a debate on relation between cognition and emotion zajonc in 1980 stated that emotion is computed first and is independent from cognition while lazarus in 1982 stated that cognition is computed first and is inseparable from emotion in 1982 the crossbar adaptive array gave a neural network model of cognition emotion relation it was an example of a debate where an ai system a recurrent neural network contributed to an issue in the same time addressed by cognitive psychology two early influential works were the jordan network 1986 and the elman network 1990 which applied rnn to study cognitive psychology in the 1980s backpropagation did not work well for deep rnns to overcome this problem in 1991 jürgen schmidhuber proposed the neural sequence chunker or neural history compressor which introduced the important concepts of self supervised pre training the p in chatgpt and neural knowledge distillation in 1993 a neural history compressor system solved a very deep learning task that required more than 1000 subsequent layers in an rnn unfolded in time in 1991 sepp hochreiter s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it he and schmidhuber introduced long short term memory lstm which set accuracy records in multiple applications domains this was not yet the modern version of lstm which required the forget gate which was introduced in 1999 it became the default choice for rnn architecture during 1985 1995 inspired by statistical mechanics several architectures and methods were developed by terry sejnowski peter dayan geoffrey hinton etc including the boltzmann machine restricted boltzmann machine helmholtz machine and the wake sleep algorithm these were designed for unsupervised learning of deep generative models deep learning between 2009 and 2012 anns began winning prizes in image recognition contests approaching human level performance on various tasks initially in pattern recognition and handwriting recognition in 2011 a cnn named dannet by dan ciresan ueli meier jonathan masci luca maria gambardella and jürgen schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest outperforming traditional methods by a factor of 3 it then won more contests they also showed how max pooling cnns on gpu improved performance significantly in october 2012 alexnet by alex krizhevsky ilya sutskever and geoffrey hinton won the large scale imagenet competition by a significant margin over shallow machine learning methods further incremental improvements included the vgg 16 network by karen simonyan and andrew zisserman and google s inceptionv3 in 2012 ng and dean created a network that learned to recognize higher level concepts such as cats only from watching unlabeled images unsupervised pre training and increased computing power from gpus and distributed computing allowed the use of larger networks particularly in image and visual recognition problems which became known as deep learning radial basis function and wavelet networks were introduced in 2013 these can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications generative adversarial network gan ian goodfellow et al 2014 became state of the art in generative modeling during 2014 2018 period the gan principle was originally published in 1991 by jürgen schmidhuber who called it artificial curiosity two neural networks contest with each other in the form of a zero sum game where one network s gain is the other network s loss the first network is a generative model that models a probability distribution over output patterns the second network learns by gradient descent to predict the reactions of the environment to these patterns excellent image quality is achieved by nvidia s stylegan 2018 based on the progressive gan by tero karras et al here the gan generator is grown from small to large scale in a pyramidal fashion image generation by gan reached popular success and provoked discussions concerning deepfakes diffusion models 2015 eclipsed gans in generative modeling since then with systems such as dall e 2 2022 and stable diffusion 2022 in 2014 the state of the art was training very deep neural network with 20 to 30 layers stacking too many layers led to a steep reduction in training accuracy known as the degradation problem in 2015 two techniques were developed to train very deep networks the highway network was published in may 2015 and the residual neural network resnet in december 2015 resnet behaves like an open gated highway net during the 2010s the seq2seq model was developed and attention mechanisms were added it led to the modern transformer architecture in 2017 in attention is all you need it requires computation time that is quadratic in the size of the context window jürgen schmidhuber s fast weight controller 1992 scales linearly and was later shown to be equivalent to the unnormalized linear transformer transformers have increasingly become the model of choice for natural language processing many modern large language models such as chatgpt gpt 4 and bert use this architecture models anns began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with they soon reoriented towards improving empirical results abandoning attempts to remain true to their biological precursors anns have the ability to learn and model non linearities and complex relationships this is achieved by neurons being connected in various patterns allowing the output of some neurons to become the input of others the network forms a directed weighted graph an artificial neural network consists of simulated neurons each neuron is connected to other nodes via links like a biological axon synapse dendrite connection all the nodes connected by links take in some data and use it to perform specific operations and tasks on the data each link has a weight determining the strength of one node s influence on another allowing weights to choose the signal between neurons artificial neurons anns are composed of artificial neurons which are conceptually derived from biological neurons each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons the inputs can be the feature values of a sample of external data such as images or documents or they can be the outputs of other neurons the outputs of the final output neurons of the neural net accomplish the task such as recognizing an object in an image to find the output of the neuron we take the weighted sum of all the inputs weighted by the weights of the connections from the inputs to the neuron we add a bias term to this sum this weighted sum is sometimes called the activation this weighted sum is then passed through a usually nonlinear activation function to produce the output the initial inputs are external data such as images and documents the ultimate outputs accomplish the task such as recognizing an object in an image organization the neurons are typically organized into multiple layers especially in deep learning neurons of one layer connect only to neurons of the immediately preceding and immediately following layers the layer that receives external data is the input layer the layer that produces the ultimate result is the output layer in between them are zero or more hidden layers single layer and unlayered networks are also used between two layers multiple connection patterns are possible they can be fully connected with every neuron in one layer connecting to every neuron in the next layer they can be pooling where a group of neurons in one layer connects to a single neuron in the next layer thereby reducing the number of neurons in that layer neurons with only such connections form a directed acyclic graph and are known as feedforward networks alternatively networks that allow connections between neurons in the same or previous layers are known as recurrent networks hyperparameter a hyperparameter is a constant parameter whose value is set before the learning process begins the values of parameters are derived via learning examples of hyperparameters include learning rate the number of hidden layers and batch size the values of some hyperparameters can be dependent on those of other hyperparameters for example the size of some layers can depend on the overall number of layers learning learning is the adaptation of the network to better handle a task by considering sample observations learning involves adjusting the weights and optional thresholds of the network to improve the accuracy of the result this is done by minimizing the observed errors learning is complete when examining additional observations does not usefully reduce the error rate even after learning the error rate typically does not reach 0 if after learning the error rate is too high the network typically must be redesigned practically this is done by defining a cost function that is evaluated periodically during learning as long as its output continues to decline learning continues the cost is frequently defined as a statistic whose value can only be approximated the outputs are actually numbers so when the error is low the difference between the output almost certainly a cat and the correct answer cat is small learning attempts to reduce the total of the differences across the observations most learning models can be viewed as a straightforward application of optimization theory and statistical estimation learning rate the learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation a high learning rate shortens the training time but with lower ultimate accuracy while a lower learning rate takes longer but with the potential for greater accuracy optimizations such as quickprop are primarily aimed at speeding up error minimization while other improvements mainly try to increase reliability in order to avoid oscillation inside the network such as alternating connection weights and to improve the rate of convergence refinements use an adaptive learning rate that increases or decreases as appropriate the concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change a momentum close to 0 emphasizes the gradient while a value close to 1 emphasizes the last change cost function while it is possible to define a cost function ad hoc frequently the choice is determined by the function s desirable properties such as convexity or because it arises from the model e g in a probabilistic model the model s posterior probability can be used as an inverse cost backpropagation backpropagation is a method used to adjust the connection weights to compensate for each error found during learning the error amount is effectively divided among the connections technically backprop calculates the gradient the derivative of the cost function associated with a given state with respect to the weights the weight updates can be done via stochastic gradient descent or other methods such as extreme learning machines no prop networks training without backtracking weightless networks and non connectionist neural networks learning paradigms machine learning is commonly separated into three main learning paradigms supervised learning unsupervised learning and reinforcement learning each corresponds to a particular learning task supervised learning supervised learning uses a set of paired inputs and desired outputs the learning task is to produce the desired output for each input in this case the cost function is related to eliminating incorrect deductions a commonly used cost is the mean squared error which tries to minimize the average squared error between the network s output and the desired output tasks suited for supervised learning are pattern recognition also known as classification and regression also known as function approximation supervised learning is also applicable to sequential data e g for handwriting speech and gesture recognition this can be thought of as learning with a teacher in the form of a function that provides continuous feedback on the quality of solutions obtained thus far unsupervised learning in unsupervised learning input data is given along with the cost function some function of the data x displaystyle textstyle x and the network s output the cost function is dependent on the task the model domain and any a priori assumptions the implicit properties of the model its parameters and the observed variables as a trivial example consider the model f x a displaystyle textstyle f x a where a displaystyle textstyle a is a constant and the cost c e x f x 2 displaystyle textstyle c e minimizing this cost produces a value of a displaystyle textstyle a that is equal to the mean of the data the cost function can be much more complicated its form depends on the application for example in compression it could be related to the mutual information between x displaystyle textstyle x and f x displaystyle textstyle f x whereas in statistical modeling it could be related to the posterior probability of the model given the data note that in both of those examples those quantities would be maximized rather than minimized tasks that fall within the paradigm of unsupervised learning are in general estimation problems the applications include clustering the estimation of statistical distributions compression and filtering reinforcement learning in applications such as playing video games an actor takes a string of actions receiving a generally unpredictable response from the environment after each one the goal is to win the game i e generate the most positive lowest cost responses in reinforcement learning the aim is to weight the network devise a policy to perform actions that minimize long term expected cumulative cost at each point in time the agent performs an action and the environment generates an observation and an instantaneous cost according to some usually unknown rules the rules and the long term cost usually only can be estimated at any juncture the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly formally the environment is modeled as a markov decision process mdp with states s 1 s n s displaystyle textstyle s_ 1 s_ n in s and actions a 1 a m a displaystyle textstyle a_ 1 a_ m in a because the state transitions are not known probability distributions are used instead the instantaneous cost distribution p c t s t displaystyle textstyle p c_ t s_ t the observation distribution p x t s t displaystyle textstyle p x_ t s_ t and the transition distribution p s t 1 s t a t displaystyle textstyle p s_ t 1 s_ t a_ t while a policy is defined as the conditional distribution over actions given the observations taken together the two define a markov chain mc the aim is to discover the lowest cost mc anns serve as the learning component in such applications dynamic programming coupled with anns giving neurodynamic programming has been applied to problems such as those involved in vehicle routing video games natural resource management and medicine because of anns ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems tasks that fall within the paradigm of reinforcement learning are control problems games and other sequential decision making tasks self learning self learning in neural networks was introduced in 1982 along with a neural network capable of self learning named crossbar adaptive array caa it is a system with only one input situation s and only one output action or behavior a it has neither external advice input nor external reinforcement input from the environment the caa computes in a crossbar fashion both decisions about actions and emotions feelings about encountered situations the system is driven by the interaction between cognition and emotion given the memory matrix w w a s the crossbar self learning algorithm in each iteration performs the following computation in situation s perform action a receive consequence situation s compute emotion of being in consequence situation v s update crossbar memory w a s w a s v s the backpropagated value secondary reinforcement is the emotion toward the consequence situation the caa exists in two environments one is behavioral environment where it behaves and the other is genetic environment where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment having received the genome vector species vector from the genetic environment the caa will learn a goal seeking behavior in the behavioral environment that contains both desirable and undesirable situations neuroevolution neuroevolution can create neural network topologies and weights using evolutionary computation it is competitive with sophisticated gradient descent approaches one advantage of neuroevolution is that it may be less prone to get caught in dead ends stochastic neural network stochastic neural networks originating from sherrington kirkpatrick models are a type of artificial neural network built by introducing random variations into the network either by giving the network s artificial neurons stochastic transfer functions or by giving them stochastic weights this makes them useful tools for optimization problems since the random fluctuations help the network escape from local minima stochastic neural networks trained using a bayesian approach are known as bayesian neural networks other in a bayesian framework a distribution over the set of allowed models is chosen to minimize the cost evolutionary methods gene expression programming simulated annealing expectation maximization non parametric methods and particle swarm optimization are other learning algorithms convergent recursion is a learning algorithm for cerebellar model articulation controller cmac neural networks modes two modes of learning are available stochastic and batch in stochastic learning each input creates a weight adjustment in batch learning weights are adjusted based on a batch of inputs accumulating errors over the batch stochastic learning introduces noise into the process using the local gradient calculated from one data point this reduces the chance of the network getting stuck in local minima however batch learning typically yields a faster more stable descent to a local minimum since each update is performed in the direction of the batch s average error a common compromise is to use mini batches small batches with samples in each batch selected stochastically from the entire data set types anns have evolved into a broad family of techniques that have advanced the state of the art across multiple domains the simplest types have one or more static components including number of units number of layers unit weights and topology dynamic types allow one or more of these to evolve via learning the latter is much more complicated but can shorten learning periods and produce better results some types allow require learning to be supervised by the operator while others operate independently some types operate purely in hardware while others are purely software and run on general purpose computers some of the main breakthroughs include convolutional neural networks that have proven particularly successful in processing visual and other two dimensional data where long short term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large vocabulary speech recognition text to speech synthesis and photo real talking heads competitive networks such as generative adversarial networks in which multiple networks of varying structure compete with each other on tasks such as winning a game or on deceiving the opponent about the authenticity of an input network design using artificial neural networks requires an understanding of their characteristics choice of model this depends on the data representation and the application model parameters include the number type and connectedness of network layers as well as the size of each and the connection type full pooling etc overly complex models learn slowly learning algorithm numerous trade offs exist between learning algorithms almost any algorithm will work well with the correct hyperparameters for training on a particular data set however selecting and tuning an algorithm for training on unseen data requires significant experimentation robustness if the model cost function and learning algorithm are selected appropriately the resulting ann can become robust neural architecture search nas uses machine learning to automate ann design various approaches to nas have designed networks that compare well with hand designed systems the basic search algorithm is to propose a candidate model evaluate it against a dataset and use the results as feedback to teach the nas network available systems include automl and autokeras scikit learn library provides functions to help with building a deep network from scratch we can then implement a deep network with tensorflow or keras hyperparameters must also be defined as part of the design they are not learned governing matters such as how many neurons are in each layer learning rate step stride depth receptive field and padding for cnns etc the python code snippet provides an overview of the training function which uses the training dataset number of hidden layer units learning rate and number of iterations as parameters applications because of their ability to reproduce and model nonlinear processes artificial neural networks have found applications in many disciplines these include function approximation or regression analysis including time series prediction fitness approximation and modeling data processing including filtering clustering blind source separation and compression nonlinear system identification and control including vehicle control trajectory prediction adaptive control process control and natural resource management pattern recognition including radar systems face identification signal classification novelty detection 3d reconstruction object recognition and sequential decision making sequence recognition including gesture speech and handwritten and printed text recognition sensor data analysis including image analysis robotics including directing manipulators and prostheses data mining including knowledge discovery in databases finance such as ex ante models for specific financial long run forecasts and artificial financial markets quantum chemistry general game playing generative ai data visualization machine translation social network filtering e mail spam filtering medical diagnosis anns have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information anns have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements it can also be useful to mitigate flood by the use of anns for modelling rainfall runoff anns have also been used for building black box models in geoscience hydrology ocean modelling and coastal engineering and geomorphology anns have been employed in cybersecurity with the objective to discriminate between legitimate activities and malicious ones for example machine learning has been used for classifying android malware for identifying domains belonging to threat actors and for detecting urls posing a security risk research is underway on ann systems designed for penetration testing for detecting botnets credit cards frauds and network intrusions anns have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many body open quantum systems in brain research anns have studied short term behavior of individual neurons the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems studies considered long and short term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level it is possible to create a profile of a user s interests from pictures using artificial neural networks trained for object recognition beyond their traditional applications artificial neural networks are increasingly being utilized in interdisciplinary research such as materials science for instance graph neural networks gnns have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals this application underscores the adaptability and potential of anns in tackling complex problems beyond the realms of predictive modeling and artificial intelligence opening new pathways for scientific discovery and innovation theoretical properties computational power the multilayer perceptron is a universal function approximator as proven by the universal approximation theorem however the proof is not constructive regarding the number of neurons required the network topology the weights and the learning parameters a specific recurrent architecture with rational valued weights as opposed to full precision real number valued weights has the power of a universal turing machine using a finite number of neurons and standard linear connections further the use of irrational values for weights results in a machine with super turing power capacity a model s capacity property corresponds to its ability to model any given function it is related to the amount of information that can be stored in the network and to the notion of complexity two notions of capacity are known by the community the information capacity and the vc dimension the information capacity of a perceptron is intensively discussed in sir david mackay s book which summarizes work by thomas cover the capacity of a network of standard neurons not convolutional can be derived by four rules that derive from understanding a neuron as an electrical element the information capacity captures the functions modelable by the network given any data as input the second notion is the vc dimension vc dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances this is given input data in a specific form as noted in the vc dimension for arbitrary inputs is half the information capacity of a perceptron the vc dimension for arbitrary points is sometimes referred to as memory capacity convergence models may not consistently converge on a single solution firstly because local minima may exist depending on the cost function and the model secondly the optimization method used might not guarantee to converge when it begins far from any local minimum thirdly for sufficiently large data or parameters some methods become impractical another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction the convergence behavior of certain types of ann architectures are more understood than others when the width of network approaches to infinity the ann is well described by its first order taylor expansion throughout training and so inherits the convergence behavior of affine models another example is when parameters are small it is observed that anns often fits target functions from low to high frequencies this behavior is referred to as the spectral bias or frequency principle of neural networks this phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as jacobi method deeper neural networks have been observed to be more biased towards low frequency functions generalization and statistics applications whose goal is to create a system that generalizes well to unseen examples face the possibility of over training this arises in convoluted or over specified systems when the network capacity significantly exceeds the needed free parameters two approaches address over training the first is to use cross validation and similar techniques to check for the presence of over training and to select hyperparameters to minimize the generalization error the second is to use some form of regularization this concept emerges in a probabilistic bayesian framework where regularization can be performed by selecting a larger prior probability over simpler models but also in statistical learning theory where the goal is to minimize over two quantities the empirical risk and the structural risk which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting supervised neural networks that use a mean squared error mse cost function can use formal statistical methods to determine the confidence of the trained model the mse on a validation set can be used as an estimate for variance this value can then be used to calculate the confidence interval of network output assuming a normal distribution a confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified by assigning a softmax activation function a generalization of the logistic function on the output layer of the neural network or a softmax component in a component based network for categorical target variables the outputs can be interpreted as posterior probabilities this is useful in classification as it gives a certainty measure on classifications the softmax activation function is y i e x i j 1 c e x j displaystyle y_ i frac e x_ i sum _ j 1 c e x_ j criticism training a common criticism of neural networks particularly in robotics is that they require too many training samples for real world operation any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases potential solutions include randomly shuffling training examples by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example grouping examples in so called mini batches and or introducing a recursive least squares algorithm for cmac dean pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads single lane multi lane dirt etc and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience and preserving past training diversity so that the system does not become overtrained if for example it is presented with a series of right turns it should not learn to always turn right theory a central claim of anns is that they embody new and powerful general principles for processing information these principles are ill defined it is often claimed that they are emergent from the network itself this allows simple statistical association the basic function of artificial neural networks to be described as learning or recognition in 1997 alexander dewdney a former scientific american columnist commented that as a result artificial neural networks have a something for nothing quality one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are no human hand or mind intervenes solutions are found as if by magic and no one it seems has learned anything one response to dewdney is that neural networks have been successfully used to handle many complex and diverse tasks ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of go technology writer roger bridgman commented neural networks for instance are in the dock not only because they have been hyped to high heaven what hasn t but also because you could create a successful net without understanding how it worked the bunch of numbers that captures its behaviour would in all probability be an opaque unreadable table valueless as a scientific resource in spite of his emphatic declaration that science is not technology dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers an unreadable table that a useful machine could read would still be well worth having although it is true that analyzing what has been learned by an artificial neural network is difficult it is much easier to do so than to analyze what has been learned by a biological neural network moreover recent emphasis on the explainability of ai has contributed towards the development of methods notably those based on attention mechanisms for visualizing and explaining learned neural networks furthermore researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful for example bengio and lecun 2007 wrote an article regarding local vs non local learning as well as shallow vs deep architecture biological brains use both shallow and deep circuits as reported by brain anatomy displaying a wide variety of invariance weng argued that the brain self wires largely according to signal statistics and therefore a serial cascade cannot catch all major statistical dependencies hardware large and effective neural networks require considerable computing resources while the brain has hardware tailored to the task of processing signals through a graph of neurons simulating even a simplified neuron on von neumann architecture may consume vast amounts of memory and storage furthermore the designer often needs to transmit signals through many of these connections and their associated neurons which require enormous cpu power and time some argue that the resurgence of neural networks in the twenty first century is largely attributable to advances in hardware from 1991 to 2015 computing power especially as delivered by gpgpus on gpus has increased around a million fold making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before the use of accelerators such as fpgas and gpus can reduce training times from months to days neuromorphic engineering or a physical neural network addresses the hardware difficulty directly by constructing non von neumann chips to directly implement neural networks in circuitry another type of chip optimized for neural network processing is called a tensor processing unit or tpu practical counterexamples analyzing what has been learned by an ann is much easier than analyzing what has been learned by a biological neural network furthermore researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful for example local vs non local learning and shallow vs deep architecture hybrid approaches advocates of hybrid models combining neural networks and symbolic approaches say that such a mixture can better capture the mechanisms of the human mind dataset bias neural networks are dependent on the quality of the data they are trained on thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases these inherited biases become especially critical when the anns are integrated into real world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race gender or other attribute this imbalance can result in the model having inadequate representation and understanding of underrepresented groups leading to discriminatory outcomes that exacerbate societal inequalities especially in applications like facial recognition hiring processes and law enforcement for example in 2018 amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field the program would penalize any resume with the word woman or the name of any women s college however the use of synthetic data can help reduce dataset bias and increase representation in datasets gallery recent advancements and future directions artificial neural networks anns have undergone significant advancements particularly in their ability to model complex systems handle large data sets and adapt to various types of applications their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing speech recognition natural language processing finance and medicine image processing in the realm of image processing anns are employed in tasks such as image classification object recognition and image segmentation for instance deep convolutional neural networks cnns have been important in handwritten digit recognition achieving state of the art performance this demonstrates the ability of anns to effectively process and interpret complex visual information leading to advancements in fields ranging from automated surveillance to medical imaging speech recognition by modeling speech signals anns are used for tasks like speaker identification and speech to text conversion deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition outperforming traditional techniques these advancements have enabled the development of more accurate and efficient voice activated systems enhancing user interfaces in technology products natural language processing in natural language processing anns are used for tasks such as text classification sentiment analysis and machine translation they have enabled the development of models that can accurately translate between languages understand the context and sentiment in textual data and categorize text based on content this has implications for automated customer service content moderation and language understanding technologies control systems in the domain of control systems anns are used to model dynamic systems for tasks such as system identification control design and optimization for instance deep feedforward neural networks are important in system identification and control applications finance anns are used for stock market prediction and credit scoring in investing anns can process vast amounts of financial data recognize complex patterns and forecast stock market trends aiding investors and risk managers in making informed decisions in credit scoring anns offer data driven personalized assessments of creditworthiness improving the accuracy of default predictions and automating the lending process anns require high quality data and careful tuning and their black box nature can pose challenges in interpretation nevertheless ongoing advancements suggest that anns continue to play a role in finance offering valuable insights and enhancing risk management strategies medicine anns are able to process and analyze vast medical datasets they enhance diagnostic accuracy especially by interpreting complex medical imaging for early disease detection and by predicting patient outcomes for personalized treatment planning in drug discovery anns speed up the identification of potential drug candidates and predict their efficacy and safety significantly reducing development time and costs additionally their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability as well as expanding the scope of ann applications in medicine content creation anns such as generative adversarial networks gan and transformers are used for content creation across numerous industries this is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions for instance dall e is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user in the field of music transformers are used to create original music for commercials and documentaries through companies such as aiva and jukedeck in the marketing industry generative models are used to create personalized advertisements for consumers additionally major film companies are partnering with technology companies to analyze the financial success of a film such as the partnership between warner bros and technology company cinelytic established in 2020 furthermore neural networks have found uses in video game creation where non player characters npcs can make decisions based on all the characters currently in the game see also references bibliography external links a brief introduction to neural networks d kriesel illustrated bilingual manuscript about artificial neural networks topics so far perceptrons backpropagation radial basis functions recurrent neural networks self organizing maps hopfield networks review of neural networks in materials science archived 7 june 2015 at the wayback machine artificial neural networks tutorial in three languages univ politécnica de madrid another introduction to ann next generation of neural networks archived 24 january 2011 at the wayback machine google tech talks performance of neural networks neural networks and information archived 9 july 2009 at the wayback machine sanderson g 5 october 2017 but what is a neural network 3blue1brown archived from the original on 7 november 2021 via youtube ",
            "total_words": 8153,
            "unique_words_percentage": 25.683797375199312,
            "stopwords_percentage": 37.15196860051515
        },
        {
            "title": "Adversarial machine learning",
            "link": "https://en.wikipedia.org/wiki/Adversarial_machine_learning",
            "content": "adversarial machine learning is the study of the attacks on machine learning algorithms and of the defenses against such attacks a survey from may 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications most machine learning techniques are mostly designed to work on specific problem sets under the assumption that the training and test data are generated from the same statistical distribution iid however this assumption is often dangerously violated in practical high stake applications where users may intentionally supply fabricated data that violates the statistical assumption most common attacks in adversarial machine learning include evasion attacks data poisoning attacks byzantine attacks and model extraction history at the mit spam conference in january 2004 john graham cumming showed that a machine learning spam filter could be used to defeat another machine learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam in 2004 nilesh dalvi and others noted that linear classifiers used in spam filters could be defeated by simple evasion attacks as spammers inserted good words into their spam emails around 2007 some spammers added random noise to fuzz words within image spam in order to defeat ocr based filters in 2006 marco barreno and others published can machine learning be secure outlining a broad taxonomy of attacks as late as 2013 many researchers continued to hope that non linear classifiers such as support vector machines and neural networks might be robust to adversaries until battista biggio and others demonstrated the first gradient based attacks on such machine learning models 2012 2013 in 2012 deep neural networks began to dominate computer vision problems starting in 2014 christian szegedy and others demonstrated that deep neural networks could be fooled by adversaries again using a gradient based attack to craft adversarial perturbations recently it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise for example any small rotation or slight illumination on an adversarial image can destroy the adversariality in addition researchers such as google brain s nicholas frosst point out that it is much easier to make self driving cars miss stop signs by physically removing the sign itself rather than creating adversarial examples frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution he suggests that a new approach to machine learning should be explored and is currently working on a unique neural network that has characteristics more similar to human perception than state of the art approaches while adversarial machine learning continues to be heavily rooted in academia large tech companies such as google microsoft and ibm have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks examples examples include attacks in spam filtering where spam messages are obfuscated through the misspelling of bad words or the insertion of good words attacks in computer security such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user or to compromise users template galleries that adapt to updated traits over time researchers showed that by changing only one pixel it was possible to fool deep learning algorithms others 3 d printed a toy turtle with a texture engineered to make google s object detection ai classify it as a rifle regardless of the angle from which the turtle was viewed creating the turtle required only low cost commercially available 3 d printing technology a machine tweaked image of a dog was shown to look like a cat to both computers and humans a 2019 study reported that humans can guess how machines will classify adversarial images researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign mcafee attacked tesla s former mobileye system fooling it into driving 50 mph over the speed limit simply by adding a two inch strip of black tape to a speed limit sign adversarial patterns on glasses or clothing designed to deceive facial recognition systems or license plate readers have led to a niche industry of stealth streetwear an adversarial attack on a neural network can allow an attacker to inject algorithms into the target system researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign seeming audio a parallel literature explores human perception of such stimuli clustering algorithms are used in security applications malware and computer virus analysis aims to identify malware families and to generate specific detection signatures attack modalities taxonomy attacks against supervised machine learning algorithms have been categorized along three primary axes influence on the classifier the security violation and their specificity classifier influence an attack can influence the classifier by disrupting the classification phase this may be preceded by an exploration phase to identify vulnerabilities the attacker s capabilities might be restricted by the presence of data manipulation constraints security violation an attack can supply malicious data that gets classified as legitimate malicious data supplied during training can cause legitimate data to be rejected after training specificity a targeted attack attempts to allow a specific intrusion disruption alternatively an indiscriminate attack creates general mayhem this taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary s goal knowledge of the attacked system capability of manipulating the input data system components and on attack strategy this taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks strategies below are some of the most commonly encountered attack scenarios data poisoning poisoning consists of contaminating the training dataset with data designed to increase errors in the output given that learning algorithms are shaped by their training datasets poisoning can effectively reprogram algorithms with potentially malicious intent concerns have been raised especially for user generated training data e g for content recommendation or natural language models the ubiquity of fake accounts offers many opportunities for poisoning facebook reportedly removes around 7 billion fake accounts per year poisoning has been reported as the leading concern for industrial applications on social medias disinformation campaigns attempt to bias recommendation and moderation algorithms to push certain content over others a particular case of data poisoning is the backdoor attack which aims to teach a specific behavior for inputs with a given trigger e g a small defect on images sounds videos or texts for instance intrusion detection systems are often trained using collected data an attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining data poisoning techniques can also be applied to text to image models to alter their output which can be used by artists to defend their copyrighted works or artistic style against imitation data poisoning can also happen unintentionally through model collapse where models are trained on synthetic data byzantine attacks as machine learning is scaled it often relies on multiple computing machines in federated learning for instance edge devices collaborate with a central server typically by sending gradients or model parameters however some of these devices may deviate from their expected behavior e g to harm the central server s model or to bias algorithms towards certain behaviors e g amplifying the recommendation of disinformation content on the other hand if the training is performed on a single machine then the model is very vulnerable to a failure of the machine or an attack on the machine the machine is a single point of failure in fact the machine owner may themselves insert provably undetectable backdoors the current leading solutions to make distributed learning algorithms provably resilient to a minority of malicious a k a byzantine participants are based on robust gradient aggregation rules the robust aggregation rules do not always work especially when the data across participants has a non iid distribution nevertheless in the context of heterogeneous honest participants such as users with different consumption habits for recommendation algorithms or writing styles for language models there are provable impossibility theorems on what any robust learning algorithm can guarantee evasion evasion attacks consist of exploiting the imperfection of a trained model for instance spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware samples are modified to evade detection that is to be classified as legitimate this does not involve influence over the training data a clear example of evasion is image based spam in which the spam content is embedded within an attached image to evade textual analysis by anti spam filters another example of evasion is given by spoofing attacks against biometric verification systems evasion attacks can be generally split into two different categories black box attacks and white box attacks model extraction model extraction involves an adversary probing a black box machine learning system in order to extract the data it was trained on this can cause issues when either the training data or the model itself is sensitive and confidential for example model extraction could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit in the extreme case model extraction can lead to model stealing which corresponds to extracting a sufficient amount of data from the model to enable the complete reconstruction of the model on the other hand membership inference is a targeted model extraction attack which infers the owner of a data point often by leveraging the overfitting resulting from poor machine learning practices concerningly this is sometimes achievable even without knowledge or access to a target model s parameters raising security concerns for models trained on sensitive data including but not limited to medical records and or personally identifiable information with the emergence of transfer learning and public accessibility of many state of the art machine learning models tech companies are increasingly drawn to create models based on public ones giving attackers freely accessible information to the structure and type of model being used categories adversarial deep reinforcement learning adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies in this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations while some methods have been proposed to overcome these susceptibilities in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies adversarial natural language processing adversarial attacks on speech recognition have been introduced for speech to text applications in particular for mozilla s implementation of deepspeech adversarial attacks and training in linear models there is a growing literature about adversarial attacks in linear models indeed since the seminal work from goodfellow at al studying these models in linear models has been an important tool to understand how adversarial attacks affect machine learning models the analysis of these models is simplified because the computation of adversarial attacks can be simplified in linear regression and classification problems moreover adversarial training is convex in this case linear models allow for analytical analysis while still reproducing phenomena observed in state of the art models one prime example of that is how this model can be used to explain the trade off between robustness and accuracy diverse work indeed provides analysis of adversarial attacks in linear models including asymptotic analysis for classification and for linear regression and finite sample analysis based on rademacher complexity specific attack types there are a large variety of different adversarial attacks that can be used against machine learning systems many of these work on both deep learning systems as well as traditional machine learning models such as svms and linear regression a high level sample of these attack types include adversarial examples trojan attacks backdoor attacks model inversion membership inference adversarial examples an adversarial example refers to specially crafted input that is designed to look normal to humans but causes misclassification to a machine learning model often a form of specially designed noise is used to elicit the misclassifications below are some current techniques for generating adversarial examples in the literature by no means an exhaustive list gradient based evasion attack fast gradient sign method fgsm projected gradient descent pgd carlini and wagner c w attack adversarial patch attack black box attacks black box attacks in adversarial machine learning assume that the adversary can only get outputs for provided inputs and has no knowledge of the model structure or parameters in this case the adversarial example is generated either using a model created from scratch or without any model at all excluding the ability to query the original model in either case the objective of these attacks is to create adversarial examples that are able to transfer to the black box model in question simple black box adversarial attacks simple black box adversarial attacks is a query efficient way to attack black box image classifiers take a random orthonormal basis v 1 v 2 v d displaystyle v_ 1 v_ 2 dots v_ d in r d displaystyle mathbb r d the authors suggested the discrete cosine transform of the standard basis the pixels for a correctly classified image x displaystyle x try x ϵ v 1 x ϵ v 1 displaystyle x epsilon v_ 1 x epsilon v_ 1 and compare the amount of error in the classifier upon x ϵ v 1 x x ϵ v 1 displaystyle x epsilon v_ 1 x x epsilon v_ 1 pick the one that causes the largest amount of error repeat this for v 2 v 3 displaystyle v_ 2 v_ 3 dots until the desired level of error in the classifier is reached it was discovered when the authors designed a simple baseline to compare with a previous black box adversarial attack algorithm based on gaussian processes and were surprised that the baseline worked even better square attack the square attack was introduced in 2020 as a black box evasion adversarial attack based on querying classification scores without the need of gradient information as a score based black box attack this adversarial approach is able to query probability distributions across model output classes but has no other access to the model itself according to the paper s authors the proposed square attack required fewer queries than when compared to state of the art score based black box attacks at the time to describe the function objective the attack defines the classifier as f 0 1 d r k textstyle f d rightarrow mathbb r k with d textstyle d representing the dimensions of the input and k textstyle k as the total number of output classes f k x textstyle f_ k x returns the score or a probability between 0 and 1 that the input x textstyle x belongs to class k textstyle k which allows the classifier s class output for any input x textstyle x to be defined as argmax k 1 k f k x textstyle text argmax _ k 1 k f_ k x the goal of this attack is as follows argmax k 1 k f k x y x x p ϵ and x 0 1 d displaystyle text argmax _ k 1 k f_ k hat x neq y hat x x _ p leq epsilon text and hat x in d in other words finding some perturbed adversarial example x textstyle hat x such that the classifier incorrectly classifies it to some other class under the constraint that x textstyle hat x and x textstyle x are similar the paper then defines loss l textstyle l as l f x y f y x max k y f k x textstyle l f hat x y f_ y hat x max _ k neq y f_ k hat x and proposes the solution to finding adversarial example x textstyle hat x as solving the below constrained optimization problem min x 0 1 d l f x y s t x x p ϵ displaystyle min _ hat x in d l f hat x y text s t hat x x _ p leq epsilon the result in theory is an adversarial example that is highly confident in the incorrect class but is also very similar to the original image to find such example square attack utilizes the iterative random search technique to randomly perturb the image in hopes of improving the objective function in each step the algorithm perturbs only a small square section of pixels hence the name square attack which terminates as soon as an adversarial example is found in order to improve query efficiency finally since the attack algorithm uses scores and not gradient information the authors of the paper indicate that this approach is not affected by gradient masking a common technique formerly used to prevent evasion attacks hopskipjump attack this black box attack was also proposed as a query efficient attack but one that relies solely on access to any input s predicted output class in other words the hopskipjump attack does not require the ability to calculate gradients or access to score values like the square attack and will require just the model s class prediction output for any given input the proposed attack is split into two different settings targeted and untargeted but both are built from the general idea of adding minimal perturbations that leads to a different model output in the targeted setting the goal is to cause the model to misclassify the perturbed image to a specific target label that is not the original label in the untargeted setting the goal is to cause the model to misclassify the perturbed image to any label that is not the original label the attack objectives for both are as follows where x textstyle x is the original image x textstyle x prime is the adversarial image d textstyle d is a distance function between images c textstyle c is the target label and c textstyle c is the model s classification class label function targeted min x d x x subject to c x c displaystyle textbf targeted min _ x prime d x prime x text subject to c x prime c untargeted min x d x x subject to c x c x displaystyle textbf untargeted min _ x prime d x prime x text subject to c x prime neq c x to solve this problem the attack proposes the following boundary function s textstyle s for both the untargeted and targeted setting s x max c c x f x c f x c x untargeted f x c max c c f x c targeted displaystyle s x prime begin cases max _ c neq c x f x prime _ c f x prime _ c x text untargeted f x prime _ c max _ c neq c f x prime _ c text targeted end cases this can be further simplified to better visualize the boundary between different potential adversarial examples s x 0 a r g m a x c f x c x untargeted a r g m a x c f x c targeted displaystyle s x prime 0 iff begin cases argmax_ c f x prime neq c x text untargeted argmax_ c f x prime c text targeted end cases with this boundary function the attack then follows an iterative algorithm to find adversarial examples x textstyle x prime for a given image x textstyle x that satisfies the attack objectives initialize x textstyle x to some point where s x 0 textstyle s x 0 iterate below boundary search gradient update compute the gradient find the step size boundary search uses a modified binary search to find the point in which the boundary as defined by s textstyle s intersects with the line between x textstyle x and x textstyle x prime the next step involves calculating the gradient for x textstyle x and update the original x textstyle x using this gradient and a pre chosen step size hopskipjump authors prove that this iterative algorithm will converge leading x textstyle x to a point right along the boundary that is very close in distance to the original image however since hopskipjump is a proposed black box attack and the iterative algorithm above requires the calculation of a gradient in the second iterative step which black box attacks do not have access to the authors propose a solution to gradient calculation that requires only the model s output predictions alone by generating many random vectors in all directions denoted as u b textstyle u_ b an approximation of the gradient can be calculated using the average of these random vectors weighted by the sign of the boundary function on the image x δ u b textstyle x prime delta _ u_ b where δ u b textstyle delta _ u_ b is the size of the random vector perturbation s x δ 1 b b 1 b ϕ x δ u b u b displaystyle nabla s x prime delta approx frac 1 b sum _ b 1 b phi x prime delta _ u_ b u_ b the result of the equation above gives a close approximation of the gradient required in step 2 of the iterative algorithm completing hopskipjump as a black box attack white box attacks white box attacks assumes that the adversary has access to model parameters on top of being able to get labels for provided inputs fast gradient sign method one of the first proposed attacks for generating adversarial examples was proposed by google researchers ian j goodfellow jonathon shlens and christian szegedy the attack was called fast gradient sign method fgsm and it consists of adding a linear amount of in perceivable noise to the image and causing a model to incorrectly classify it this noise is calculated by multiplying the sign of the gradient with respect to the image we want to perturb by a small constant epsilon as epsilon increases the model is more likely to be fooled but the perturbations become easier to identify as well shown below is the equation to generate an adversarial example where x textstyle x is the original image ϵ textstyle epsilon is a very small number δ x textstyle delta _ x is the gradient function j textstyle j is the loss function θ textstyle theta is the model weights and y textstyle y is the true label a d v x x ϵ s i g n δ x j θ x y displaystyle adv_ x x epsilon cdot sign delta _ x j theta x y one important property of this equation is that the gradient is calculated with respect to the input image since the goal is to generate an image that maximizes the loss for the original image of true label y textstyle y in traditional gradient descent for model training the gradient is used to update the weights of the model since the goal is to minimize the loss for the model on a ground truth dataset the fast gradient sign method was proposed as a fast way to generate adversarial examples to evade the model based on the hypothesis that neural networks cannot resist even linear amounts of perturbation to the input fgsm has shown to be effective in adversarial attacks for image classification and skeletal action recognition carlini wagner c w in an effort to analyze existing adversarial attacks and defenses researchers at the university of california berkeley nicholas carlini and david wagner in 2016 propose a faster and more robust method to generate adversarial examples the attack proposed by carlini and wagner begins with trying to solve a difficult non linear optimization equation min δ p subject to c x δ t x δ 0 1 n displaystyle min delta _ p text subject to c x delta t x delta in n here the objective is to minimize the noise δ textstyle delta added to the original input x textstyle x such that the machine learning algorithm c textstyle c predicts the original input with delta or x δ textstyle x delta as some other class t textstyle t however instead of directly the above equation carlini and wagner propose using a new function f textstyle f such that c x δ t f x δ 0 displaystyle c x delta t iff f x delta leq 0 this condenses the first equation to the problem below min δ p subject to f x δ 0 x δ 0 1 n displaystyle min delta _ p text subject to f x delta leq 0 x delta in n and even more to the equation below min δ p c f x δ x δ 0 1 n displaystyle min delta _ p c cdot f x delta x delta in n carlini and wagner then propose the use of the below function in place of f textstyle f using z textstyle z a function that determines class probabilities for given input x textstyle x when substituted in this equation can be thought of as finding a target class that is more confident than the next likeliest class by some constant amount f x max i t z x i z x t displaystyle f x z x _ t when solved using gradient descent this equation is able to produce stronger adversarial examples when compared to fast gradient sign method that is also able to bypass defensive distillation a defense that was once proposed to be effective against adversarial examples defenses researchers have proposed a multi step approach to protecting machine learning threat modeling formalize the attackers goals and capabilities with respect to the target system attack simulation formalize the optimization problem the attacker tries to solve according to possible attack strategies attack impact evaluation countermeasure design noise detection for evasion based attack information laundering alter the information received by adversaries for model stealing attacks mechanisms a number of defense mechanisms against evasion poisoning and privacy attacks have been proposed including secure learning algorithms byzantine resilient algorithms multiple classifier systems ai written algorithms ais that explore the training environment for example in image recognition actively navigating a 3d environment rather than passively scanning a fixed set of 2d images privacy preserving learning ladder algorithm for kaggle style competitions game theoretic models sanitizing training data adversarial training backdoor detection algorithms gradient masking obfuscation techniques to prevent the adversary exploiting the gradient in white box attacks this family of defenses is deemed unreliable as these models are still vulnerable to black box attacks or can be circumvented in other ways ensembles of models have been proposed in the literature but caution should be applied when relying on them usually ensembling weak classifiers results in a more accurate model but it does not seem to apply in the adversarial context see also pattern recognition fawkes image cloaking software generative adversarial network references external links mitre atlas adversarial threat landscape for artificial intelligence systems nist 8269 draft a taxonomy and terminology of adversarial machine learning nips 2007 workshop on machine learning in adversarial environments for computer security alfasvmlib archived 2020 09 24 at the wayback machine adversarial label flip attacks against support vector machines laskov pavel lippmann richard 2010 machine learning in adversarial environments machine learning 81 2 115 119 doi 10 1007 s10994 010 5207 6 s2cid 12567278 dagstuhl perspectives workshop on machine learning methods for computer security workshop on artificial intelligence and security aisec series",
            "total_words": 4672,
            "unique_words_percentage": 24.85017123287671,
            "stopwords_percentage": 33.66866438356164
        },
        {
            "title": "Artificial intelligence",
            "link": "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "content": "artificial intelligence ai in its broadest sense is intelligence exhibited by machines particularly computer systems it is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals such machines may be called ais high profile applications of ai include advanced web search engines e g google search recommendation systems used by youtube amazon and netflix virtual assistants e g google assistant siri and alexa autonomous vehicles e g waymo generative and creative tools e g chatgpt and ai art and superhuman play and analysis in strategy games e g chess and go however many ai applications are not perceived as ai a lot of cutting edge ai has filtered into general applications often without being called ai because once something becomes useful enough and common enough it s not labeled ai anymore various subfields of ai research are centered around particular goals and the use of particular tools the traditional goals of ai research include reasoning knowledge representation planning learning natural language processing perception and support for robotics general intelligence the ability to complete any task performed by a human on an at least equal level is among the field s long term goals to reach these goals ai researchers have adapted and integrated a wide range of techniques including search and mathematical optimization formal logic artificial neural networks and methods based on statistics operations research and economics ai also draws upon psychology linguistics philosophy neuroscience and other fields artificial intelligence was founded as an academic discipline in 1956 and the field went through multiple cycles of optimism followed by periods of disappointment and loss of funding known as ai winters funding and interest vastly increased after 2012 when deep learning outperformed previous ai techniques this growth accelerated further after 2017 with the transformer architecture and by the early 2020s many billions of dollars were being invested in ai known as the ai boom the widespread use of ai in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long term effects in the future prompting discussions about regulatory policies to ensure the safety and benefits of the technology goals the general problem of simulating or creating intelligence has been broken into subproblems these consist of particular traits or capabilities that researchers expect an intelligent system to display the traits described below have received the most attention and cover the scope of ai research reasoning and problem solving early researchers developed algorithms that imitated step by step reasoning that humans use when they solve puzzles or make logical deductions by the late 1980s and 1990s methods were developed for dealing with uncertain or incomplete information employing concepts from probability and economics many of these algorithms are insufficient for solving large reasoning problems because they experience a combinatorial explosion they become exponentially slower as the problems grow even humans rarely use the step by step deduction that early ai research could model they solve most of their problems using fast intuitive judgments accurate and efficient reasoning is an unsolved problem knowledge representation knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real world facts formal knowledge representations are used in content based indexing and retrieval scene interpretation clinical decision support knowledge discovery mining interesting and actionable inferences from large databases and other areas a knowledge base is a body of knowledge represented in a form that can be used by a program an ontology is the set of objects relations concepts and properties used by a particular domain of knowledge knowledge bases need to represent things such as objects properties categories and relations between objects situations events states and time causes and effects knowledge about knowledge what we know about what other people know default reasoning things that humans assume are true until they are told differently and will remain true even when other facts are changing and many other aspects and domains of knowledge among the most difficult problems in knowledge representation are the breadth of commonsense knowledge the set of atomic facts that the average person knows is enormous and the sub symbolic form of most commonsense knowledge much of what people know is not represented as facts or statements that they could express verbally there is also the difficulty of knowledge acquisition the problem of obtaining knowledge for ai applications planning and decision making an agent is anything that perceives and takes actions in the world a rational agent has goals or preferences and takes actions to make them happen in automated planning the agent has a specific goal in automated decision making the agent has preferences there are some situations it would prefer to be in and some situations it is trying to avoid the decision making agent assigns a number to each situation called the utility that measures how much the agent prefers it for each possible action it can calculate the expected utility the utility of all possible outcomes of the action weighted by the probability that the outcome will occur it can then choose the action with the maximum expected utility in classical planning the agent knows exactly what the effect of any action will be in most real world problems however the agent may not be certain about the situation they are in it is unknown or unobservable and it may not know for certain what will happen after each possible action it is not deterministic it must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked in some problems the agent s preferences may be uncertain especially if there are other agents or humans involved these can be learned e g with inverse reinforcement learning or the agent can seek information to improve its preferences information value theory can be used to weigh the value of exploratory or experimental actions the space of possible future actions and situations is typically intractably large so the agents must take actions and evaluate situations while being uncertain of what the outcome will be a markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action a policy associates a decision with each possible state the policy could be calculated e g by iteration be heuristic or it can be learned game theory describes the rational behavior of multiple interacting agents and is used in ai programs that make decisions that involve other agents learning machine learning is the study of programs that can improve their performance on a given task automatically it has been a part of ai from the beginning there are several kinds of machine learning unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance supervised learning requires labeling the training data with the expected answers and comes in two main varieties classification where the program must learn to predict what category the input belongs in and regression where the program must deduce a numeric function based on numeric input in reinforcement learning the agent is rewarded for good responses and punished for bad ones the agent learns to choose responses that are classified as good transfer learning is when the knowledge gained from one problem is applied to a new problem deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning computational learning theory can assess learners by computational complexity by sample complexity how much data is required or by other notions of optimization natural language processing natural language processing nlp allows programs to read write and communicate in human languages such as english specific problems include speech recognition speech synthesis machine translation information extraction information retrieval and question answering early work based on noam chomsky s generative grammar and semantic networks had difficulty with word sense disambiguation unless restricted to small domains called micro worlds due to the common sense knowledge problem margaret masterman believed that it was meaning and not grammar that was the key to understanding languages and that thesauri and not dictionaries should be the basis of computational language structure modern deep learning techniques for nlp include word embedding representing words typically as vectors encoding their meaning transformers a deep learning architecture using an attention mechanism and others in 2019 generative pre trained transformer or gpt language models began to generate coherent text and by 2023 these models were able to get human level scores on the bar exam sat test gre test and many other real world applications perception machine perception is the ability to use input from sensors such as cameras microphones wireless signals active lidar sonar radar and tactile sensors to deduce aspects of the world computer vision is the ability to analyze visual input the field includes speech recognition image classification facial recognition object recognition object tracking and robotic perception social intelligence affective computing is an interdisciplinary umbrella that comprises systems that recognize interpret process or simulate human feeling emotion and mood for example some virtual assistants are programmed to speak conversationally or even to banter humorously it makes them appear more sensitive to the emotional dynamics of human interaction or to otherwise facilitate human computer interaction however this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents moderate successes related to affective computing include textual sentiment analysis and more recently multimodal sentiment analysis wherein ai classifies the effects displayed by a videotaped subject general intelligence a machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence techniques ai research uses a wide variety of techniques to accomplish the goals above search and optimization ai can solve many problems by intelligently searching through many possible solutions there are two very different kinds of search used in ai state space search and local search state space search state space search searches through a tree of possible states to try to find a goal state for example planning algorithms search through trees of goals and subgoals attempting to find a path to a target goal a process called means ends analysis simple exhaustive searches are rarely sufficient for most real world problems the search space the number of places to search quickly grows to astronomical numbers the result is a search that is too slow or never completes heuristics or rules of thumb can help prioritize choices that are more likely to reach a goal adversarial search is used for game playing programs such as chess or go it searches through a tree of possible moves and countermoves looking for a winning position local search local search uses mathematical optimization to find a solution to a problem it begins with some form of guess and refines it incrementally gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function variants of gradient descent are commonly used to train neural networks another type of local search is evolutionary computation which aims to iteratively improve a set of candidate solutions by mutating and recombining them selecting only the fittest to survive each generation distributed search processes can coordinate via swarm intelligence algorithms two popular swarm algorithms used in search are particle swarm optimization inspired by bird flocking and ant colony optimization inspired by ant trails logic formal logic is used for reasoning and knowledge representation formal logic comes in two main forms propositional logic which operates on statements that are true or false and uses logical connectives such as and or not and implies and predicate logic which also operates on objects predicates and relations and uses quantifiers such as every x is a y and there are some xs that are ys deductive reasoning in logic is the process of proving a new statement conclusion from other statements that are given and assumed to be true the premises proofs can be structured as proof trees in which nodes are labelled by sentences and children nodes are connected to parent nodes by inference rules given a problem and a set of premises problem solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms in the case of horn clauses problem solving search can be performed by reasoning forwards from the premises or backwards from the problem in the more general case of the clausal form of first order logic resolution is a single axiom free rule of inference in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved inference in both horn clause logic and first order logic is undecidable and therefore intractable however backward reasoning with horn clauses which underpins computation in the logic programming language prolog is turing complete moreover its efficiency is competitive with computation in other symbolic programming languages fuzzy logic assigns a degree of truth between 0 and 1 it can therefore handle propositions that are vague and partially true non monotonic logics including logic programming with negation as failure are designed to handle default reasoning other specialized versions of logic have been developed to describe many complex domains probabilistic methods for uncertain reasoning many problems in ai including in reasoning planning learning perception and robotics require the agent to operate with incomplete or uncertain information ai researchers have devised a number of tools to solve these problems using methods from probability theory and economics precise mathematical tools have been developed that analyze how an agent can make choices and plan using decision theory decision analysis and information value theory these tools include models such as markov decision processes dynamic decision networks game theory and mechanism design bayesian networks are a tool that can be used for reasoning using the bayesian inference algorithm learning using the expectation maximization algorithm planning using decision networks and perception using dynamic bayesian networks probabilistic algorithms can also be used for filtering prediction smoothing and finding explanations for streams of data thus helping perception systems analyze processes that occur over time e g hidden markov models or kalman filters classifiers and statistical learning methods the simplest ai applications can be divided into two types classifiers e g if shiny then diamond on one hand and controllers e g if diamond then pick up on the other hand classifiers are functions that use pattern matching to determine the closest match they can be fine tuned based on chosen examples using supervised learning each pattern also called an observation is labeled with a certain predefined class all the observations combined with their class labels are known as a data set when a new observation is received that observation is classified based on previous experience there are many kinds of classifiers in use the decision tree is the simplest and most widely used symbolic machine learning algorithm k nearest neighbor algorithm was the most widely used analogical ai until the mid 1990s and kernel methods such as the support vector machine svm displaced k nearest neighbor in the 1990s the naive bayes classifier is reportedly the most widely used learner at google due in part to its scalability neural networks are also used as classifiers artificial neural networks an artificial neural network is based on a collection of nodes also known as artificial neurons which loosely model the neurons in a biological brain it is trained to recognise patterns once trained it can recognise those patterns in fresh data there is an input at least one hidden layer of nodes and an output each node applies a function and once the weight crosses its specified threshold the data is transmitted to the next layer a network is typically called a deep neural network if it has at least 2 hidden layers learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training the most common training technique is the backpropagation algorithm neural networks learn to model complex relationships between inputs and outputs and find patterns in data in theory a neural network can learn any function in feedforward neural networks the signal passes in only one direction recurrent neural networks feed the output signal back into the input which allows short term memories of previous input events long short term memory is the most successful network architecture for recurrent networks perceptrons use only a single layer of neurons deep learning uses multiple layers convolutional neural networks strengthen the connection between neurons that are close to each other this is especially important in image processing where a local set of neurons must identify an edge before the network can identify an object deep learning deep learning uses several layers of neurons between the network s inputs and outputs the multiple layers can progressively extract higher level features from the raw input for example in image processing lower layers may identify edges while higher layers may identify the concepts relevant to a human such as digits letters or faces deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence including computer vision speech recognition natural language processing image classification and others the reason that deep learning performs so well in so many applications is not known as of 2023 the sudden success of deep learning in 2012 2015 did not occur because of some new discovery or theoretical breakthrough deep neural networks and backpropagation had been described by many people as far back as the 1950s but because of two factors the incredible increase in computer power including the hundred fold increase in speed by switching to gpus and the availability of vast amounts of training data especially the giant curated datasets used for benchmark testing such as imagenet gpt generative pre trained transformers gpt are large language models llms that generate text based on the semantic relationships between words in sentences text based gpt models are pretrained on a large corpus of text that can be from the internet the pretraining consists of predicting the next token a token being usually a word subword or punctuation throughout this pretraining gpt models accumulate knowledge about the world and can then generate human like text by repeatedly predicting the next token typically a subsequent training phase makes the model more truthful useful and harmless usually with a technique called reinforcement learning from human feedback rlhf current gpt models are prone to generating falsehoods called hallucinations although this can be reduced with rlhf and quality data they are used in chatbots which allow people to ask a question or request a task in simple text current models and services include gemini formerly bard chatgpt grok claude copilot and llama multimodal gpt models can process different types of data modalities such as images videos sound and text hardware and software in the late 2010s graphics processing units gpus that were increasingly designed with ai specific enhancements and used with specialized tensorflow software had replaced previously used central processing unit cpus as the dominant means for large scale commercial and academic machine learning models training specialized programming languages such as prolog were used in early ai research but general purpose programming languages like python have become predominant the transistor density in integrated circuits has been observed to roughly double every 18 months a trend known as moore s law named after the intel co founder gordon moore who first identified it improvements in gpus have been even faster applications ai and machine learning technology is used in most of the essential applications of the 2020s including search engines such as google search targeting online advertisements recommendation systems offered by netflix youtube or amazon driving internet traffic targeted advertising adsense facebook virtual assistants such as siri or alexa autonomous vehicles including drones adas and self driving cars automatic language translation microsoft translator google translate facial recognition apple s face id or microsoft s deepface and google s facenet and image labeling used by facebook apple s iphoto and tiktok the deployment of ai may be overseen by a chief automation officer cao health and medicine the application of ai in medicine and medical research has the potential to increase patient care and quality of life through the lens of the hippocratic oath medical professionals are ethically compelled to use ai if applications can more accurately diagnose and treat patients for medical research ai is an important tool for processing and integrating big data this is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication it has been suggested that ai can overcome discrepancies in funding allocated to different fields of research new ai tools can deepen the understanding of biomedically relevant pathways for example alphafold 2 2021 demonstrated the ability to approximate in hours rather than months the 3d structure of a protein in 2023 it was reported that ai guided drug discovery helped find a class of antibiotics capable of killing two different types of drug resistant bacteria in 2024 researchers used machine learning to accelerate the search for parkinson s disease drug treatments their aim was to identify compounds that block the clumping or aggregation of alpha synuclein the protein that characterises parkinson s disease they were able to speed up the initial screening process ten fold and reduce the cost by a thousand fold sexuality applications of ai in this domain include ai enabled menstruation and fertility trackers that analyze user data to offer prediction ai integrated sex toys e g teledildonics ai generated sexual education content and ai agents that simulate sexual and romantic partners e g replika ai is also used for the production of non consensual deepfake pornography raising significant ethical and legal concerns ai technologies have also been used to attempt to identify online gender based violence and online sexual grooming of minors games game playing programs have been used since the 1950s to demonstrate and test ai s most advanced techniques deep blue became the first computer chess playing system to beat a reigning world chess champion garry kasparov on 11 may 1997 in 2011 in a jeopardy quiz show exhibition match ibm s question answering system watson defeated the two greatest jeopardy champions brad rutter and ken jennings by a significant margin in march 2016 alphago won 4 out of 5 games of go in a match with go champion lee sedol becoming the first computer go playing system to beat a professional go player without handicaps then in 2017 it defeated ke jie who was the best go player in the world other programs handle imperfect information games such as the poker playing program pluribus deepmind developed increasingly generalistic reinforcement learning models such as with muzero which could be trained to play chess go or atari games in 2019 deepmind s alphastar achieved grandmaster level in starcraft ii a particularly challenging real time strategy game that involves incomplete knowledge of what happens on the map in 2021 an ai agent competed in a playstation gran turismo competition winning against four of the world s best gran turismo drivers using deep reinforcement learning in 2024 google deepmind introduced sima a type of ai capable of autonomously playing nine previously unseen open world video games by observing screen output as well as executing short specific tasks in response to natural language instructions mathematics in mathematics special forms of formal step by step reasoning are used in contrast llms such as gpt 4 turbo gemini ultra claude opus llama 2 or mistral large are working with probabilistic models which can produce wrong answers in the form of hallucinations therefore they need not only a large database of mathematical problems to learn from but also methods such as supervised fine tuning or trained classifiers with human annotated data to improve answers for new problems and learn from corrections a 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low even for problems with only minor deviations from trained data alternatively dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as alpha tensor alpha geometry and alpha proof all from google deepmind llemma from eleuther or julius when natural language is used to describe mathematical problems converters transform such prompts into a formal language such as lean to define mathematical tasks some models have been developed to solve challenging problems and reach good results in benchmark tests others to serve as educational tools in mathematics finance finance is one of the fastest growing sectors where applied ai tools are being deployed from retail online banking to investment advice and insurance where automated robot advisers have been in use for some years world pensions experts like nicolas firzli insist it may be too early to see the emergence of highly innovative ai informed financial products and services the deployment of ai tools will simply further automatise things destroying tens of thousands of jobs in banking financial planning and pension advice in the process but i m not sure it will unleash a new wave of pension innovation military various countries are deploying ai military applications the main applications enhance command and control communications sensors integration and interoperability research is targeting intelligence collection and analysis logistics cyber operations information operations and semiautonomous and autonomous vehicles ai technologies enable coordination of sensors and effectors threat detection and identification marking of enemy positions target acquisition coordination and deconfliction of distributed joint fires between networked combat vehicles involving manned and unmanned teams ai has been used in military operations in iraq syria israel and ukraine generative ai in the early 2020s generative ai gained widespread prominence genai is ai capable of generating text images videos or other data using generative models often in response to prompts in march 2023 58 of u s adults had heard about chatgpt and 14 had tried it the increasing realism and ease of use of ai based text to image generators such as midjourney dall e and stable diffusion sparked a trend of viral ai generated photos widespread attention was gained by a fake photo of pope francis wearing a white puffer coat the fictional arrest of donald trump and a hoax of an attack on the pentagon as well as the usage in professional creative arts agents artificial intelligent ai agents are software entities designed to perceive their environment make decisions and take actions autonomously to achieve specific goals these agents can interact with users their environment or other agents ai agents are used in various applications including virtual assistants chatbots autonomous vehicles game playing systems and industrial robotics ai agents operate within the constraints of their programming available computational resources and hardware limitations this means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities in real world applications ai agents often face time constraints for decision making and action execution many ai agents incorporate learning algorithms enabling them to improve their performance over time through experience or training using machine learning ai agents can adapt to new situations and optimise their behaviour for their designated tasks other industry specific tasks there are also thousands of successful ai applications used to solve specific problems for specific industries or institutions in a 2017 survey one in five companies reported having incorporated ai in some offerings or processes a few examples are energy storage medical diagnosis military logistics applications that predict the result of judicial decisions foreign policy or supply chain management ai applications for evacuation and disaster management are growing ai has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from gps videos or social media further ai can provide real time information on the real time evacuation conditions in agriculture ai has helped farmers identify areas that need irrigation fertilization pesticide treatments or increasing yield agronomists use ai to conduct research and development ai has been used to predict the ripening time for crops such as tomatoes monitor soil moisture operate agricultural robots conduct predictive analytics classify livestock pig call emotions automate greenhouses detect diseases and pests and save water artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications mainly for classification regression clustering forecasting generation discovery and the development of new scientific insights for example it is used for discovering exoplanets forecasting solar activity and distinguishing between signals and instrumental effects in gravitational wave astronomy additionally it could be used for activities in space such as space exploration including the analysis of data from space missions real time science decisions of spacecraft space debris avoidance and more autonomous operation during the 2024 indian elections us 50 millions was spent on authorized ai generated content notably by creating deepfakes of allied including sometimes deceased politicians to better engage with voters and by translating speeches to various local languages ethics ai has potential benefits and potential risks ai may be able to advance science and find solutions for serious problems demis hassabis of deepmind hopes to solve intelligence and then use that to solve everything else however as the use of ai has become widespread several unintended consequences and risks have been identified in production systems can sometimes not factor ethics and bias into their ai training processes especially when the ai algorithms are inherently unexplainable in deep learning risks and harm privacy and copyright machine learning algorithms require large amounts of data the techniques used to acquire this data have raised concerns about privacy surveillance and copyright ai powered devices and services such as virtual assistants and iot products continuously collect personal information raising concerns about intrusive data gathering and unauthorized access by third parties the loss of privacy is further exacerbated by ai s ability to process and combine vast amounts of data potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency sensitive user data collected may include online activity records geolocation data video or audio for example in order to build speech recognition algorithms amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy ai developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data such as data aggregation de identification and differential privacy since 2016 some privacy experts such as cynthia dwork have begun to view privacy in terms of fairness brian christian wrote that experts have pivoted from the question of what they know to the question of what they re doing with it generative ai is often trained on unlicensed copyrighted works including in domains such as images or computer code the output is then used under the rationale of fair use experts disagree about how well and under what circumstances this rationale will hold up in courts of law relevant factors may include the purpose and character of the use of the copyrighted work and the effect upon the potential market for the copyrighted work website owners who do not wish to have their content scraped can indicate it in a robots txt file in 2023 leading authors including john grisham and jonathan franzen sued ai companies for using their work to train generative ai another discussed approach is to envision a separate sui generis system of protection for creations generated by ai to ensure fair attribution and compensation for human authors dominance by tech giants the commercial ai scene is dominated by big tech companies such as alphabet inc amazon apple inc meta platforms and microsoft some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers allowing them to entrench further in the marketplace power needs and environmental impacts in january 2024 the international energy agency iea released electricity 2024 analysis and forecast to 2026 forecasting electric power use this is the first iea report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency the report states that power demand for these uses might double by 2026 with additional electric power usage equal to electricity used by the whole japanese nation prodigious power consumption by ai is responsible for the growth of fossil fuels use and might delay closings of obsolete carbon emitting coal energy facilities there is a feverish rise in the construction of data centers throughout the us making large technology firms e g microsoft meta google amazon into voracious consumers of electric power projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source a chatgpt search involves the use of 10 times the electrical energy as a google search the large firms are in haste to find power sources from nuclear energy to geothermal to fusion the tech firms argue that in the long view ai will be eventually kinder to the environment but they need the energy now ai makes the power grid more efficient and intelligent will assist in the growth of nuclear power and track overall carbon emissions according to technology firms a 2024 goldman sachs research paper ai data centers and the coming us power demand surge found us power demand is likely to experience growth not seen in a generation and forecasts that by 2030 us data centers will consume 8 of us power as opposed to 3 in 2022 presaging growth for the electrical power generation industry by a variety of means data centers need for more and more electrical power is such that they might max out the electrical grid the big tech companies counter that ai can be used to maximize the utilization of the grid by all in 2024 the wall street journal reported that big ai companies have begun negotiations with the us nuclear power providers to provide electricity to the data centers in march 2024 amazon purchased a pennsylvania nuclear powered data center for 650 million us nvidia ceo jen hsun huang said nuclear power is a good option for the data centers in september 2024 microsoft announced an agreement with constellation energy to re open the three mile island nuclear power plant to provide microsoft with 100 of all electric power produced by the plant for 20 years reopening the plant which suffered a partial nuclear meltdown of its unit 2 reactor in 1979 will require constellation to get through strict regulatory processes which will include extensive safety scrutiny from the us nuclear regulatory commission if approved this will be the first ever us re commissioning of a nuclear plant over 835 megawatts of power enough for 800 000 homes of energy will be produced the cost for re opening and upgrading is estimated at 1 6 billion us and is dependent on tax breaks for nuclear power contained in the 2022 us inflation reduction act the us government and the state of michigan are investing almost 2 billion us to reopen the palisades nuclear reactor on lake michigan closed since 2022 the plant is planned to be reopened in october 2025 the three mile island facility will be renamed the crane clean energy center after chris crane a nuclear proponent and former ceo of exelon who was responsible for exelon spinoff of constellation after the last approval in september 2023 taiwan suspended the approval of data centers north of taoyuan with a capacity of more than 5 mw in 2024 due to power supply shortages taiwan aims to phase out nuclear power by 2025 on the other hand singapore imposed a ban on the opening of data centers in 2019 due to electric power but in 2022 lifted this ban although most nuclear plants in japan have been shut down after the 2011 fukushima nuclear accident according to an october 2024 bloomberg article in japanese cloud gaming services company ubitus in which nvidia has a stake is looking for land in japan near nuclear power plant for a new data center for generative ai ubitus ceo wesley kuo said nuclear power plants are the most efficient cheap and stable power for ai on 1 november 2024 the federal energy regulatory commission ferc rejected an application submitted by talen energy for approval to supply some electricity from the nuclear power station susquehanna to amazon s data center according to the commission chairman willie l phillips it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors misinformation youtube facebook and others use recommender systems to guide users to more content these ai programs were given the goal of maximizing user engagement that is the only goal was to keep people watching the ai learned that users tended to choose misinformation conspiracy theories and extreme partisan content and to keep them watching the ai recommended more of it users also tended to watch more content on the same subject so the ai led people into filter bubbles where they received multiple versions of the same misinformation this convinced many users that the misinformation was true and ultimately undermined trust in institutions the media and the government the ai program had correctly learned to maximize its goal but the result was harmful to society after the u s election in 2016 major technology companies took steps to mitigate the problem in 2022 generative ai began to create images audio video and text that are indistinguishable from real photographs recordings films or human writing it is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda ai pioneer geoffrey hinton expressed concern about ai enabling authoritarian leaders to manipulate their electorates on a large scale among other risks algorithmic bias and fairness machine learning applications will be biased if they learn from biased data the developers may not be aware that the bias exists bias can be introduced by the way training data is selected and by the way a model is deployed if a biased algorithm is used to make decisions that can seriously harm people as it can in medicine finance recruitment housing or policing then the algorithm may cause discrimination the field of fairness studies how to prevent harms from algorithmic biases on june 28 2015 google photos s new image labeling feature mistakenly identified jacky alcine and a friend as gorillas because they were black the system was trained on a dataset that contained very few images of black people a problem called sample size disparity google fixed this problem by preventing the system from labelling anything as a gorilla eight years later in 2023 google photos still could not identify a gorilla and neither could similar products from apple facebook microsoft and amazon compas is a commercial program widely used by u s courts to assess the likelihood of a defendant becoming a recidivist in 2016 julia angwin at propublica discovered that compas exhibited racial bias despite the fact that the program was not told the races of the defendants although the error rate for both whites and blacks was calibrated equal at exactly 61 the errors for each race were different the system consistently overestimated the chance that a black person would re offend and would underestimate the chance that a white person would not re offend in 2017 several researchers showed that it was mathematically impossible for compas to accommodate all possible measures of fairness when the base rates of re offense were different for whites and blacks in the data a program can make biased decisions even if the data does not explicitly mention a problematic feature such as race or gender the feature will correlate with other features like address shopping history or first name and the program will make the same decisions based on these features as it would on race or gender moritz hardt said the most robust fact in this research area is that fairness through blindness doesn t work criticism of compas highlighted that machine learning models are designed to make predictions that are only valid if we assume that the future will resemble the past if they are trained on data that includes the results of racist decisions in the past machine learning models must predict that racist decisions will be made in the future if an application then uses these predictions as recommendations some of these recommendations will likely be racist thus machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past it is descriptive rather than prescriptive bias and unfairness may go undetected because the developers are overwhelmingly white and male among ai engineers about 4 are black and 20 are women there are various conflicting definitions and mathematical models of fairness these notions depend on ethical assumptions and are influenced by beliefs about society one broad category is distributive fairness which focuses on the outcomes often identifying groups and seeking to compensate for statistical disparities representational fairness tries to ensure that ai systems do not reinforce negative stereotypes or render certain groups invisible procedural fairness focuses on the decision process rather than the outcome the most relevant notions of fairness may depend on the context notably the type of ai application and the stakeholders the subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them having access to sensitive attributes such as race or gender is also considered by many ai ethicists to be necessary in order to compensate for biases but it may conflict with anti discrimination laws at its 2022 conference on fairness accountability and transparency acm facct 2022 the association for computing machinery in seoul south korea presented and published findings that recommend that until ai and robotics systems are demonstrated to be free of bias mistakes they are unsafe and the use of self learning neural networks trained on vast unregulated sources of flawed internet data should be curtailed lack of transparency many ai systems are so complex that their designers cannot explain how they reach their decisions particularly with deep neural networks in which there are a large amount of non linear relationships between inputs and outputs but some popular explainability techniques exist it is impossible to be certain that a program is operating correctly if no one knows how exactly it works there have been many cases where a machine learning program passed rigorous tests but nevertheless learned something different than what the programmers intended for example a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as cancerous because pictures of malignancies typically include a ruler to show the scale another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at low risk of dying from pneumonia having asthma is actually a severe risk factor but since the patients having asthma would usually get much more medical care they were relatively unlikely to die according to the training data the correlation between asthma and low risk of dying from pneumonia was real but misleading people who have been harmed by an algorithm s decision have a right to an explanation doctors for example are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make early drafts of the european union s general data protection regulation in 2016 included an explicit statement that this right exists industry experts noted that this is an unsolved problem with no solution in sight regulators argued that nevertheless the harm is real if the problem has no solution the tools should not be used darpa established the xai explainable artificial intelligence program in 2014 to try to solve these problems several approaches aim to address the transparency problem shap enables to visualise the contribution of each feature to the output lime can locally approximate a model s outputs with a simpler interpretable model multitask learning provides a large number of outputs in addition to the target classification these other outputs can help developers deduce what the network has learned deconvolution deepdream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned and produce output that can suggest what the network is learning for generative pre trained transformers anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human understandable concepts bad actors and weaponized ai artificial intelligence provides a number of tools that are useful to bad actors such as authoritarian governments terrorists criminals or rogue states a lethal autonomous weapon is a machine that locates selects and engages human targets without human supervision widely available ai tools can be used by bad actors to develop inexpensive autonomous weapons and if produced at scale they are potentially weapons of mass destruction even when used in conventional warfare it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person in 2014 30 nations including china supported a ban on autonomous weapons under the united nations convention on certain conventional weapons however the united states and others disagreed by 2015 over fifty countries were reported to be researching battlefield robots ai tools make it easier for authoritarian governments to efficiently control their citizens in several ways face and voice recognition allow widespread surveillance machine learning operating this data can classify potential enemies of the state and prevent them from hiding recommendation systems can precisely target propaganda and misinformation for maximum effect deepfakes and generative ai aid in producing misinformation advanced ai can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets it lowers the cost and difficulty of digital warfare and advanced spyware all these technologies have been available since 2020 or earlier ai facial recognition systems are already being used for mass surveillance in china there many other ways that ai is expected to help bad actors some of which can not be foreseen for example machine learning ai is able to design tens of thousands of toxic molecules in a matter of hours technological unemployment economists have frequently highlighted the risks of redundancies from ai and speculated about unemployment if there is no adequate social policy for full employment in the past technology has tended to increase rather than reduce total employment but economists acknowledge that we re in uncharted territory with ai a survey of economists showed disagreement about whether the increasing use of robots and ai will cause a substantial increase in long term unemployment but they generally agree that it could be a net benefit if productivity gains are redistributed risk estimates vary for example in the 2010s michael osborne and carl benedikt frey estimated 47 of u s jobs are at high risk of potential automation while an oecd report classified only 9 of u s jobs as high risk the methodology of speculating about future employment levels has been criticised as lacking evidential foundation and for implying that technology rather than social policy creates unemployment as opposed to redundancies in april 2023 it was reported that 70 of the jobs for chinese video game illustrators had been eliminated by generative artificial intelligence unlike previous waves of automation many middle class jobs may be eliminated by artificial intelligence the economist stated in 2015 that the worry that ai could do to white collar jobs what steam power did to blue collar ones during the industrial revolution is worth taking seriously jobs at extreme risk range from paralegals to fast food cooks while job demand is likely to increase for care related professions ranging from personal healthcare to the clergy from the early days of the development of artificial intelligence there have been arguments for example those put forward by joseph weizenbaum about whether tasks that can be done by computers actually should be done by them given the difference between computers and humans and between quantitative calculation and qualitative value based judgement existential risk it has been argued ai will become so powerful that humanity may irreversibly lose control of it this could as physicist stephen hawking stated spell the end of the human race this scenario has been common in science fiction when a computer or robot suddenly develops a human like self awareness or sentience or consciousness and becomes a malevolent character these sci fi scenarios are misleading in several ways first ai does not require human like sentience to be an existential risk modern ai programs are given specific goals and use learning and intelligence to achieve them philosopher nick bostrom argued that if one gives almost any goal to a sufficiently powerful ai it may choose to destroy humanity to achieve it he used the example of a paperclip factory manager stuart russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged reasoning that you can t fetch the coffee if you re dead in order to be safe for humanity a superintelligence would have to be genuinely aligned with humanity s morality and values so that it is fundamentally on our side second yuval noah harari argues that ai does not require a robot body or physical control to pose an existential risk the essential parts of civilization are not physical things like ideologies law government money and the economy are built on language they exist because there are stories that billions of people believe the current prevalence of misinformation suggests that an ai could use language to convince people to believe anything even to take actions that are destructive the opinions amongst experts and industry insiders are mixed with sizable fractions both concerned and unconcerned by risk from eventual superintelligent ai personalities such as stephen hawking bill gates and elon musk as well as ai pioneers such as yoshua bengio stuart russell demis hassabis and sam altman have expressed concerns about existential risk from ai in may 2023 geoffrey hinton announced his resignation from google in order to be able to freely speak out about the risks of ai without considering how this impacts google he notably mentioned risks of an ai takeover and stressed that in order to avoid the worst outcomes establishing safety guidelines will require cooperation among those competing in use of ai in 2023 many leading ai experts endorsed the joint statement that mitigating the risk of extinction from ai should be a global priority alongside other societal scale risks such as pandemics and nuclear war some other researchers were more optimistic ai pioneer jürgen schmidhuber did not sign the joint statement emphasising that in 95 of all cases ai research is about making human lives longer and healthier and easier while the tools that are now being used to improve lives can also be used by bad actors they can also be used against the bad actors andrew ng also argued that it s a mistake to fall for the doomsday hype on ai and that regulators who do will only benefit vested interests yann lecun scoffs at his peers dystopian scenarios of supercharged misinformation and even eventually human extinction in the early 2010s experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine however after 2016 the study of current and future risks and possible solutions became a serious area of research ethical machines and alignment friendly ai are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans eliezer yudkowsky who coined the term argues that developing friendly ai should be a higher research priority it may require a large investment and it must be completed before ai becomes an existential risk machines with intelligence have the potential to use their intelligence to make ethical decisions the field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas the field of machine ethics is also called computational morality and was founded at an aaai symposium in 2005 other approaches include wendell wallach s artificial moral agents and stuart j russell s three principles for developing provably beneficial machines open source active organizations in the ai open source community include hugging face google eleutherai and meta various ai models such as llama 2 mistral or stable diffusion have been made open weight meaning that their architecture and trained parameters the weights are publicly available open weight models can be freely fine tuned which allows companies to specialize them with their own data and for their own use case open weight models are useful for research and innovation but can also be misused since they can be fine tuned any built in security measure such as objecting to harmful requests can be trained away until it becomes ineffective some researchers warn that future ai models may develop dangerous capabilities such as the potential to drastically facilitate bioterrorism and that once released on the internet they cannot be deleted everywhere if needed they recommend pre release audits and cost benefit analyses frameworks artificial intelligence projects can have their ethical permissibility tested while designing developing and implementing an ai system an ai framework such as the care and act framework containing the sum values developed by the alan turing institute tests projects in four main areas respect the dignity of individual people connect with other people sincerely openly and inclusively care for the wellbeing of everyone protect social values justice and the public interest other developments in ethical frameworks include those decided upon during the asilomar conference the montreal declaration for responsible ai and the ieee s ethics of autonomous systems initiative among others however these principles do not go without their criticisms especially regards to the people chosen contributes to these frameworks promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of ai system design development and implementation and collaboration between job roles such as data scientists product managers data engineers domain experts and delivery managers the uk ai safety institute released in 2024 a testing toolset called inspect for ai safety evaluations available under a mit open source licence which is freely available on github and can be improved with third party packages it can be used to evaluate ai models in a range of areas including core knowledge ability to reason and autonomous capabilities regulation the regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating ai it is therefore related to the broader regulation of algorithms the regulatory and policy landscape for ai is an emerging issue in jurisdictions globally according to ai index at stanford the annual number of ai related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone between 2016 and 2020 more than 30 countries adopted dedicated strategies for ai most eu member states had released national ai strategies as had canada china india japan mauritius the russian federation saudi arabia united arab emirates u s and vietnam others were in the process of elaborating their own ai strategy including bangladesh malaysia and tunisia the global partnership on artificial intelligence was launched in june 2020 stating a need for ai to be developed in accordance with human rights and democratic values to ensure public confidence and trust in the technology henry kissinger eric schmidt and daniel huttenlocher published a joint statement in november 2021 calling for a government commission to regulate ai in 2023 openai leaders published recommendations for the governance of superintelligence which they believe may happen in less than 10 years in 2023 the united nations also launched an advisory body to provide recommendations on ai governance the body comprises technology company executives governments officials and academics in 2024 the council of europe created the first international legally binding treaty on ai called the framework convention on artificial intelligence and human rights democracy and the rule of law it was adopted by the european union the united states the united kingdom and other signatories in a 2022 ipsos survey attitudes towards ai varied greatly by country 78 of chinese citizens but only 35 of americans agreed that products and services using ai have more benefits than drawbacks a 2023 reuters ipsos poll found that 61 of americans agree and 22 disagree that ai poses risks to humanity in a 2023 fox news poll 35 of americans thought it very important and an additional 41 thought it somewhat important for the federal government to regulate ai versus 13 responding not very important and 8 responding not at all important in november 2023 the first global ai safety summit was held in bletchley park in the uk to discuss the near and far term risks of ai and the possibility of mandatory and voluntary regulatory frameworks 28 countries including the united states china and the european union issued a declaration at the start of the summit calling for international co operation to manage the challenges and risks of artificial intelligence in may 2024 at the ai seoul summit 16 global ai tech companies agreed to safety commitments on the development of ai history the study of mechanical or formal reasoning began with philosophers and mathematicians in antiquity the study of logic led directly to alan turing s theory of computation which suggested that a machine by shuffling symbols as simple as 0 and 1 could simulate any conceivable form of mathematical reasoning this along with concurrent discoveries in cybernetics information theory and neurobiology led researchers to consider the possibility of building an electronic brain they developed several areas of research that would become part of ai such as mccullouch and pitts design for artificial neurons in 1943 and turing s influential 1950 paper computing machinery and intelligence which introduced the turing test and showed that machine intelligence was plausible the field of ai research was founded at a workshop at dartmouth college in 1956 the attendees became the leaders of ai research in the 1960s they and their students produced programs that the press described as astonishing computers were learning checkers strategies solving word problems in algebra proving logical theorems and speaking english artificial intelligence laboratories were set up at a number of british and u s universities in the latter 1950s and early 1960s researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field in 1965 herbert simon predicted machines will be capable within twenty years of doing any work a man can do in 1967 marvin minsky agreed writing that within a generation the problem of creating artificial intelligence will substantially be solved they had however underestimated the difficulty of the problem in 1974 both the u s and british governments cut off exploratory research in response to the criticism of sir james lighthill and ongoing pressure from the u s congress to fund more productive projects minsky s and papert s book perceptrons was understood as proving that artificial neural networks would never be useful for solving real world tasks thus discrediting the approach altogether the ai winter a period when obtaining funding for ai projects was difficult followed in the early 1980s ai research was revived by the commercial success of expert systems a form of ai program that simulated the knowledge and analytical skills of human experts by 1985 the market for ai had reached over a billion dollars at the same time japan s fifth generation computer project inspired the u s and british governments to restore funding for academic research however beginning with the collapse of the lisp machine market in 1987 ai once again fell into disrepute and a second longer lasting winter began up to this point most of ai s funding had gone to projects that used high level symbols to represent mental objects like plans goals beliefs and known facts in the 1980s some researchers began to doubt that this approach would be able to imitate all the processes of human cognition especially perception robotics learning and pattern recognition and began to look into sub symbolic approaches rodney brooks rejected representation in general and focussed directly on engineering machines that move and survive judea pearl lofti zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic but the most important development was the revival of connectionism including neural network research by geoffrey hinton and others in 1990 yann lecun successfully showed that convolutional neural networks can recognize handwritten digits the first of many successful applications of neural networks ai gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems this narrow and formal focus allowed researchers to produce verifiable results and collaborate with other fields such as statistics economics and mathematics by 2000 solutions developed by ai researchers were being widely used although in the 1990s they were rarely described as artificial intelligence a tendency known as the ai effect however several academic researchers became concerned that ai was no longer pursuing its original goal of creating versatile fully intelligent machines beginning around 2002 they founded the subfield of artificial general intelligence or agi which had several well funded institutions by the 2010s deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field for many specific tasks other methods were abandoned deep learning s success was based on both hardware improvements faster computers graphics processing units cloud computing and access to large amounts of data including curated datasets such as imagenet deep learning s success led to an enormous increase in interest and funding in ai the amount of machine learning research measured by total publications increased by 50 in the years 2015 2019 in 2016 issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences publications vastly increased funding became available and many researchers re focussed their careers on these issues the alignment problem became a serious field of academic study in the late teens and early 2020s agi companies began to deliver programs that created enormous interest in 2015 alphago developed by deepmind beat the world champion go player the program was taught only the rules of the game and developed strategy by itself gpt 3 is a large language model that was released in 2020 by openai and is capable of generating high quality human like text chatgpt launched on november 30 2022 became the fastest growing consumer software application in history gaining over 100 million users in two months it marked what is widely regarded as ai s breakout year bringing it into the public consciousness these programs and others inspired an aggressive ai boom where large companies began investing billions of dollars in ai research according to ai impacts about 50 billion annually was invested in ai around 2022 in the u s alone and about 20 of the new u s computer science phd graduates have specialized in ai about 800 000 ai related u s job openings existed in 2022 philosophy philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines another major focus has been whether machines can be conscious and the associated ethical implications many other topics in philosophy are relevant to ai such as epistemology and free will rapid advancements have intensified public discussions on the philosophy and ethics of ai defining artificial intelligence alan turing wrote in 1950 i propose to consider the question can machines think he advised changing the question from whether a machine thinks to whether or not it is possible for machinery to show intelligent behaviour he devised the turing test which measures the ability of a machine to simulate human conversation since we can only observe the behavior of the machine it does not matter if it is actually thinking or literally has a mind turing notes that we can not determine these things about other people but it is usual to have a polite convention that everyone thinks russell and norvig agree with turing that intelligence must be defined in terms of external behavior not internal structure however they are critical that the test requires the machine to imitate humans aeronautical engineering texts they wrote do not define the goal of their field as making machines that fly so exactly like pigeons that they can fool other pigeons ai founder john mccarthy agreed writing that artificial intelligence is not by definition simulation of human intelligence mccarthy defines intelligence as the computational part of the ability to achieve goals in the world another ai founder marvin minsky similarly describes it as the ability to solve hard problems the leading ai textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals these definitions view intelligence in terms of well defined problems with well defined solutions where both the difficulty of the problem and the performance of the program are direct measures of the intelligence of the machine and no other philosophical discussion is required or may not even be possible another definition has been adopted by google a major practitioner in the field of ai this definition stipulates the ability of systems to synthesize information as the manifestation of intelligence similar to the way it is defined in biological intelligence some authors have suggested in practice that the definition of ai is vague and difficult to define with contention as to whether classical algorithms should be categorised as ai with many companies during the early 2020s ai boom using the term as a marketing buzzword often even if they did not actually use ai in a material way evaluating approaches to ai no established unifying theory or paradigm has guided ai research for most of its history the unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches so much so that some sources especially in the business world use the term artificial intelligence to mean machine learning with neural networks this approach is mostly sub symbolic soft and narrow critics argue that these questions may have to be revisited by future generations of ai researchers symbolic ai and its limits symbolic ai or gofai simulated the high level conscious reasoning that people use when they solve puzzles express legal reasoning and do mathematics they were highly successful at intelligent tasks such as algebra or iq tests in the 1960s newell and simon proposed the physical symbol systems hypothesis a physical symbol system has the necessary and sufficient means of general intelligent action however the symbolic approach failed on many tasks that humans solve easily such as learning recognizing an object or commonsense reasoning moravec s paradox is the discovery that high level intelligent tasks were easy for ai but low level instinctive tasks were extremely difficult philosopher hubert dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a feel for the situation rather than explicit symbolic knowledge although his arguments had been ridiculed and ignored when they were first presented eventually ai research came to agree with him the issue is not resolved sub symbolic reasoning can make many of the same inscrutable mistakes that human intuition does such as algorithmic bias critics such as noam chomsky argue continuing research into symbolic ai will still be necessary to attain general intelligence in part because sub symbolic ai is a move away from explainable ai it can be difficult or impossible to understand why a modern statistical ai program made a particular decision the emerging field of neuro symbolic artificial intelligence attempts to bridge the two approaches neat vs scruffy neats hope that intelligent behavior is described using simple elegant principles such as logic optimization or neural networks scruffies expect that it necessarily requires solving a large number of unrelated problems neats defend their programs with theoretical rigor scruffies rely mainly on incremental testing to see if they work this issue was actively discussed in the 1970s and 1980s but eventually was seen as irrelevant modern ai has elements of both soft vs hard computing finding a provably correct or optimal solution is intractable for many important problems soft computing is a set of techniques including genetic algorithms fuzzy logic and neural networks that are tolerant of imprecision uncertainty partial truth and approximation soft computing was introduced in the late 1980s and most successful ai programs in the 21st century are examples of soft computing with neural networks narrow vs general ai ai researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible narrow ai in hopes these solutions will lead indirectly to the field s long term goals general intelligence is difficult to define and difficult to measure and modern ai has had more verifiable successes by focusing on specific problems with specific solutions the sub field of artificial general intelligence studies this area exclusively machine consciousness sentience and mind the philosophy of mind does not know whether a machine can have a mind consciousness and mental states in the same sense that human beings do this issue considers the internal experiences of the machine rather than its external behavior mainstream ai research considers this issue irrelevant because it does not affect the goals of the field to build machines that can solve problems using intelligence russell and norvig add that he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on however the question has become central to the philosophy of mind it is also typically the central question at issue in artificial intelligence in fiction consciousness david chalmers identified two problems in understanding the mind which he named the hard and easy problems of consciousness the easy problem is understanding how the brain processes signals makes plans and controls behavior the hard problem is explaining how this feels or why it should feel like anything at all assuming we are right in thinking that it truly does feel like something dennett s consciousness illusionism says this is an illusion while human information processing is easy to explain human subjective experience is difficult to explain for example it is easy to imagine a color blind person who has learned to identify which objects in their field of view are red but it is not clear what would be required for the person to know what red looks like computationalism and functionalism computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind body problem this philosophical position was inspired by the work of ai researchers and cognitive scientists in the 1960s and was originally proposed by philosophers jerry fodor and hilary putnam philosopher john searle characterized this position as strong ai the appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds searle challenges this claim with his chinese room argument which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind ai welfare and rights it is difficult or impossible to reliably evaluate whether an advanced ai is sentient has the ability to feel and if so to what degree but if there is a significant chance that a given machine can feel and suffer then it may be entitled to certain rights or welfare protection measures similarly to animals sapience a set of capacities related to high intelligence such as discernment or self awareness may provide another moral basis for ai rights robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society in 2017 the european union considered granting electronic personhood to some of the most capable ai systems similarly to the legal status of companies it would have conferred rights but also responsibilities critics argued in 2018 that granting rights to ai systems would downplay the importance of human rights and that legislation should focus on user needs rather than speculative futuristic scenarios they also noted that robots lacked the autonomy to take part to society on their own progress in ai increased interest in the topic proponents of ai welfare and rights often argue that ai sentience if it emerges would be particularly easy to deny they warn that this may be a moral blind spot analogous to slavery or factory farming which could lead to large scale suffering if sentient ai is created and carelessly exploited future superintelligence and the singularity a superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind if research into artificial general intelligence produced sufficiently intelligent software it might be able to reprogram and improve itself the improved software would be even better at improving itself leading to what i j good called an intelligence explosion and vernor vinge called a singularity however technologies cannot improve exponentially indefinitely and typically follow an s shaped curve slowing when they reach the physical limits of what the technology can do transhumanism robot designer hans moravec cyberneticist kevin warwick and inventor ray kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either this idea called transhumanism has roots in the writings of aldous huxley and robert ettinger edward fredkin argues that artificial intelligence is the next step in evolution an idea first proposed by samuel butler s darwin among the machines as far back as 1863 and expanded upon by george dyson in his 1998 book darwin among the machines the evolution of global intelligence in fiction thought capable artificial beings have appeared as storytelling devices since antiquity and have been a persistent theme in science fiction a common trope in these works began with mary shelley s frankenstein where a human creation becomes a threat to its masters this includes such works as arthur c clarke s and stanley kubrick s 2001 a space odyssey both 1968 with hal 9000 the murderous computer in charge of the discovery one spaceship as well as the terminator 1984 and the matrix 1999 in contrast the rare loyal robots such as gort from the day the earth stood still 1951 and bishop from aliens 1986 are less prominent in popular culture isaac asimov introduced the three laws of robotics in many stories most notably with the multivac super intelligent computer asimov s laws are often brought up during lay discussions of machine ethics while almost all artificial intelligence researchers are familiar with asimov s laws through popular culture they generally consider the laws useless for many reasons one of which is their ambiguity several works use ai to force us to confront the fundamental question of what makes us human showing us artificial beings that have the ability to feel and thus to suffer this appears in karel čapek s r u r the films a i artificial intelligence and ex machina as well as the novel do androids dream of electric sheep by philip k dick dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence see also artificial intelligence and elections use and impact of ai on political elections artificial intelligence content detection software to detect ai generated content behavior selection algorithm algorithm that selects actions for intelligent agents business process automation automation of business processes case based reasoning process of solving new problems based on the solutions of similar past problems computational intelligence ability of a computer to learn a specific task from data or experimental observation digital immortality hypothetical concept of storing a personality in digital form emergent algorithm algorithm exhibiting emergent behavior female gendering of ai technologies gender biases in digital technologypages displaying short descriptions of redirect targets glossary of artificial intelligence list of definitions of terms and concepts commonly used in the study of artificial intelligence intelligence amplification use of information technology to augment human intelligence intelligent agent software agent which acts autonomously mind uploading hypothetical process of digitally emulating a brain organoid intelligence use of brain cells and brain organoids for intelligent computing robotic process automation form of business process automation technology wetware computer computer composed of organic material explanatory notes references ai textbooks the two most widely used textbooks in 2023 see the open syllabus russell stuart j norvig peter 2021 artificial intelligence a modern approach 4th ed hoboken pearson isbn 978 0 1346 1099 3 lccn 20190474 rich elaine knight kevin nair shivashankar b 2010 artificial intelligence 3rd ed new delhi tata mcgraw hill india isbn 978 0 0700 8770 5 the four most widely used ai textbooks in 2008 other textbooks ertel wolfgang 2017 introduction to artificial intelligence 2nd ed springer isbn 978 3 3195 8486 7 ciaramella alberto ciaramella marco 2024 introduction to artificial intelligence from data analysis to generative ai 1st ed intellisemantic editions isbn 978 8 8947 8760 3 history of ai other sources further reading external links artificial intelligence internet encyclopedia of philosophy ",
            "total_words": 13168,
            "unique_words_percentage": 24.027946537059538,
            "stopwords_percentage": 37.91008505467801
        },
        {
            "title": "Boosting (machine learning)",
            "link": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)",
            "content": "in machine learning ml boosting is an ensemble metaheuristic for primarily reducing bias as opposed to variance it can also improve the stability and accuracy of ml classification and regression algorithms hence it is prevalent in supervised learning for converting weak learners to strong learners the concept of boosting is based on the question posed by kearns and valiant 1988 1989 can a set of weak learners create a single strong learner a weak learner is defined as a classifier that is only slightly correlated with the true classification a strong learner is a classifier that is arbitrarily well correlated with the true classification robert schapire answered the question in the affirmative in a paper published in 1990 this has had significant ramifications in machine learning and statistics most notably leading to the development of boosting initially the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner algorithms that achieve this quickly became known as boosting freund and schapire s arcing adaptive resampling and combining as a general technique is more or less synonymous with boosting algorithms while boosting is not algorithmically constrained most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier when they are added they are weighted in a way that is related to the weak learners accuracy after a weak learner is added the data weights are readjusted known as re weighting misclassified input data gain a higher weight and examples that are classified correctly lose weight thus future weak learners focus more on the examples that previous weak learners misclassified there are many boosting algorithms the original ones proposed by robert schapire a recursive majority gate formulation and yoav freund boost by majority were not adaptive and could not take full advantage of the weak learners schapire and freund then developed adaboost an adaptive boosting algorithm that won the prestigious gödel prize only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms other algorithms that are similar in spirit to boosting algorithms are sometimes called leveraging algorithms although they are also sometimes incorrectly called boosting algorithms the main variation between many boosting algorithms is their method of weighting training data points and hypotheses adaboost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners it is often the basis of introductory coverage of boosting in university machine learning courses there are many more recent algorithms such as lpboost totalboost brownboost xgboost madaboost logitboost and others many boosting algorithms fit into the anyboost framework which shows that boosting performs gradient descent in a function space using a convex cost function object categorization in computer vision given images containing various known objects in the world a classifier can be learned from them to automatically classify the objects in future images simple classifiers built based on some image feature of the object tend to be weak in categorization performance using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization problem of object categorization object categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object the idea is closely related with recognition identification and detection appearance based object categorization typically contains feature extraction learning a classifier and applying the classifier to new examples there are many ways to represent a category of objects e g from shape analysis bag of words models or local descriptors such as sift etc examples of supervised classifiers are naive bayes classifiers support vector machines mixtures of gaussians and neural networks however research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well status quo for object categorization the recognition of object categories in images is a challenging problem in computer vision especially when the number of categories is large this is due to high intra class variability and the need for generalization across variations of objects within the same category objects within one category may look quite different even the same object may appear unalike under different viewpoint scale and illumination background clutter and partial occlusion add difficulties to recognition as well humans are able to recognize thousands of object types whereas most of the existing object recognition systems are trained to recognize only a few e g human faces cars simple objects etc research has been very active on dealing with more categories and enabling incremental additions of new categories and although the general problem remains unsolved several multi category objects detectors for up to hundreds or thousands of categories have been developed one means is by feature sharing and boosting boosting for binary categorization adaboost can be used for face detection as an example of binary categorization the two categories are faces versus background the general algorithm is as follows form a large set of simple features initialize weights for training images for t rounds normalize the weights for available features from the set train a classifier using a single feature and evaluate the training error choose the classifier with the lowest error update the weights of the training images increase if classified wrongly by this classifier decrease if correctly form the final strong classifier as the linear combination of the t classifiers coefficient larger if training error is small after boosting a classifier constructed from 200 features could yield a 95 detection rate under a 10 5 displaystyle 10 5 false positive rate another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance this work is the first to combine both motion information and appearance information as features to detect a walking person it takes a similar approach to the viola jones object detection framework boosting for multi class categorization compared with binary categorization multi class categorization looks for common features that can be shared across the categories at the same time they turn to be more generic edge like features during learning the detectors for each category can be trained jointly compared with training separately it generalizes better needs less training data and requires fewer features to achieve the same performance the main flow of the algorithm is similar to the binary case what is different is that a measure of the joint training error shall be defined in advance during each iteration the algorithm chooses a classifier of a single feature features that can be shared by more categories shall be encouraged this can be done via converting multi class classification into a binary one a set of categories versus the rest or by introducing a penalty error from the categories that do not have the feature of the classifier in the paper sharing visual features for multiclass and multiview object detection a torralba et al used gentleboost for boosting and showed that when training data is limited learning via sharing features does a much better job than no sharing given same boosting rounds also for a given performance level the total number of features required and therefore the run time cost of the classifier for the feature sharing detectors is observed to scale approximately logarithmically with the number of class i e slower than linear growth in the non sharing case similar results are shown in the paper incremental learning of object detectors using a visual shape alphabet yet the authors used adaboost for boosting convex vs non convex boosting algorithms boosting algorithms can be based on convex or non convex optimization algorithms convex algorithms such as adaboost and logitboost can be defeated by random noise such that they can t learn basic and learnable combinations of weak hypotheses this limitation was pointed out by long servedio in 2008 however by 2009 multiple authors demonstrated that boosting algorithms based on non convex optimization such as brownboost can learn from noisy datasets and can specifically learn the underlying classifier of the long servedio dataset see also implementations scikit learn an open source machine learning library for python orange a free data mining software suite module orange ensemble weka is a machine learning set of tools that offers variate implementations of boosting algorithms like adaboost and logitboost r package gbm generalized boosted regression models implements extensions to freund and schapire s adaboost algorithm and friedman s gradient boosting machine jboost adaboost logitboost robustboost boostexter and alternating decision trees r package adabag applies multiclass adaboost m1 adaboost samme and bagging r package xgboost an implementation of gradient boosting for linear and tree based models notes references further reading freund yoav schapire robert e 1997 a decision theoretic generalization of on line learning and an application to boosting pdf journal of computer and system sciences 55 1 119 139 doi 10 1006 jcss 1997 1504 schapire robert e 1990 the strength of weak learnability machine learning 5 2 197 227 doi 10 1007 bf00116037 s2cid 6207294 schapire robert e singer yoram 1999 improved boosting algorithms using confidence rated predictors machine learning 37 3 297 336 doi 10 1023 a 1007614523901 s2cid 2329907 zhou zhihua 2008 on the margin explanation of boosting algorithm pdf in proceedings of the 21st annual conference on learning theory colt 08 479 490 zhou zhihua 2013 on the doubt about margin explanation of boosting pdf artificial intelligence 203 1 18 arxiv 1009 3613 doi 10 1016 j artint 2013 07 002 s2cid 2828847 external links robert e schapire 2003 the boosting approach to machine learning an overview msri mathematical sciences research institute workshop on nonlinear estimation and classification zhou zhi hua 2014 boosting 25 years archived 2016 08 20 at the wayback machine ccl 2014 keynote ",
            "total_words": 1645,
            "unique_words_percentage": 38.358662613981764,
            "stopwords_percentage": 35.379939209726444
        },
        {
            "title": "Attention (machine learning)",
            "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
            "content": "attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence in natural language processing importance is represented by soft weights assigned to each word in a sentence more generally attention encodes vectors called token embeddings across a fixed width sequence that can range from tens to millions of tokens in size unlike hard weights which are computed during the backwards training pass soft weights exist only in the forward pass and therefore change with every step of the input earlier designs implemented the attention mechanism in a serial recurrent neural network rnn language translation system but a more recent design namely the transformer removed the slower sequential rnn and relied more heavily on the faster parallel attention scheme inspired by ideas about attention in humans the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks recurrent neural networks favor more recent information contained in words at the end of a sentence while information earlier in the sentence tends to be attenuated attention allows a token equal access to any part of a sentence directly rather than only through the previous state history academic reviews of the history of the attention mechanism are provided in niu et al and soydaner predecessors selective attention in humans had been well studied in neuroscience and cognitive psychology in 1953 colin cherry studied selective attention in the context of audition known as the cocktail party effect in 1958 donald broadbent proposed the filter model of attention selective attention of vision was studied in the 1960s by george sperling s partial report paradigm it was also noticed that saccade control is modulated by cognitive processes insofar as the eye moves preferentially towards areas of high salience as the fovea of the eye is small the eye cannot sharply resolve the entire visual field at once the use of saccade control allows the eye to quickly scan important features of a scene these research developments inspired algorithms such as the neocognitron and its variants meanwhile developments in neural networks had inspired circuit models of biological visual attention one well cited network from 1998 for example was inspired by the low level primate visual system it produced saliency maps of images using handcrafted not learned features which were then used to guide a second neural network in processing patches of the image in order of reducing saliency a key aspect of attention mechanism can be written schematically as i query i key i value i displaystyle sum _ i langle text query _ i text key _ i rangle text value _ i where the angled brackets denote dot product this shows that it involves a multiplicative operation multiplicative operations within artificial neural networks had been studied under the names of group method of data handling 1965 where kolmogorov gabor polynomials implement multiplicative units or gates higher order neural networks multiplication units sigma pi units fast weight controllers and hyper networks in fast weight controller schmidhuber 1992 one of its two networks has fast weights or dynamic links 1981 a slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries this was later shown to be equivalent to the unnormalized linear transformer a follow up paper developed a similar system with active weight changing recurrent attention during the deep learning era attention mechanism was developed to solve similar problems in encoding decoding in machine translation the seq2seq model as it was proposed in 2014 would encode an input text into a fixed length vector which would then be decoded into an output text if the input text is long the fixed length vector would be unable to carry enough information for accurate decoding an attention mechanism was proposed to solve this problem an image captioning model was proposed in 2015 citing inspiration from the seq2seq model that would encode an input image into a fixed length vector xu et al 2015 citing bahdanau et al 2014 applied the attention mechanism as used in the seq2seq model to image captioning transformer one problem with seq2seq models was their use of recurrent neural networks which are not parallelizable as both the encoder and the decoder must process the sequence token by token decomposable attention attempted to solve this problem by processing the input sequence in parallel before computing a soft alignment matrix alignment is the terminology used by bahdanau et al in order to allow for parallel processing the idea of using the attention mechanism for self attention instead of in an encoder decoder cross attention was also proposed during this period such as in differentiable neural computers and neural turing machines it was termed intra attention where an lstm is augmented with a memory network as it encodes an input sequence these strands of development were brought together in 2017 with the transformer architecture published in the attention is all you need paper overview the attention network was designed to identify high correlations patterns amongst words in a given sentence assuming that it has learned word correlation patterns from the training data this correlation is captured as neuronal weights learned during training with backpropagation this attention scheme has been compared to the query key analogy of relational databases that comparison suggests an asymmetric role for the query and key vectors where one item of interest the query vector that is matched against all possible items the key vectors of each word in the sentence however both self and cross attentions parallel calculations matches all tokens of the k matrix with all tokens of the q matrix therefore the roles of these vectors are symmetric possibly because the simplistic database analogy is flawed much effort has gone into understand attention further by studying their roles in focused settings such as in context learning masked language tasks stripped down transformers bigram statistics n gram statistics pairwise convolutions and arithmetic factoring machine translation the seq2seq method developed in the early 2010s uses two neural networks an encoder network converts an input sentence into numerical vectors and a decoder network converts those vectors to sentences in the target language the attention mechanism was grafted onto this structure in 2014 and shown below later it was refined into the transformer design 2017 interpreting attention weights in translating between languages alignment is the process of matching words from the source sentence to words of the translated sentence in the i love you example above the second word love is aligned with the third word aime stacking soft row vectors together for je t and aime yields an alignment matrix sometimes alignment can be multiple to multiple for example the english phrase look it up corresponds to cherchez le thus soft attention weights work better than hard attention weights setting one attention weight to 1 and the others to 0 as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors rather than the best one as there may not be a best hidden vector this view of the attention weights addresses some of the neural network explainability problem networks that perform verbatim translation without regard to word order would show the highest scores along the dominant diagonal of the matrix the off diagonal dominance shows that the attention mechanism is more nuanced on the first pass through the decoder 94 of the attention weight is on the first english word i so the network offers the word je on the second pass of the decoder 88 of the attention weight is on the third english word you so it offers t on the last pass 95 of the attention weight is on the second english word love so it offers aime seq2seq problem statement consider the seq2seq language english to french translation task to be concrete let us consider the translation of the zone of international control end which should translate to la zone de contrôle international end here we use the special end token as a control character to delimit the end of input for both the encoder and the decoder an input sequence of text x 0 x 1 displaystyle x_ 0 x_ 1 dots is processed by a neural network which can be an lstm a transformer encoder or some other network into a sequence of real valued vectors h 0 h 1 displaystyle h_ 0 h_ 1 dots where h displaystyle h stands for hidden vector after the encoder has finished processing the decoder starts operating over the hidden vectors to produce an output sequence y 0 y 1 displaystyle y_ 0 y_ 1 dots autoregressively that is it always takes as input both the hidden vectors produced by the encoder and what the decoder itself has produced before to produce the next output word h 0 h 1 displaystyle h_ 0 h_ 1 dots start la h 0 h 1 displaystyle h_ 0 h_ 1 dots start la la zone h 0 h 1 displaystyle h_ 0 h_ 1 dots start la zone la zone de h 0 h 1 displaystyle h_ 0 h_ 1 dots start la zone de contrôle international la zone de contrôle international end here we use the special start token as a control character to delimit the start of input for the decoder the decoding terminates as soon as end appears in the decoder output attention weights as hand crafting weights defeats the purpose of machine learning the model must compute the attention weights on its own taking analogy from the language of database queries we make the model construct a triple of vectors key query and value the rough idea is that we have a database in the form of a list of key value pairs the decoder sends in a query and obtains a reply in the form of a weighted sum of the values where the weight is proportional to how closely the query resembles each key the decoder first processes the start input partially to obtain an intermediate vector h 0 d displaystyle h_ 0 d the 0th hidden vector of decoder then the intermediate vector is transformed by a linear map w q displaystyle w q into a query vector q 0 h 0 d w q displaystyle q_ 0 h_ 0 d w q meanwhile the hidden vectors outputted by the encoder are transformed by another linear map w k displaystyle w k into key vectors k 0 h 0 w k k 1 h 1 w k displaystyle k_ 0 h_ 0 w k k_ 1 h_ 1 w k dots the linear maps are useful for providing the model with enough freedom to find the best way to represent the data now the query and keys are compared by taking dot products q 0 k 0 t q 0 k 1 t displaystyle q_ 0 k_ 0 t q_ 0 k_ 1 t dots ideally the model should have learned to compute the keys and values such that q 0 k 0 t displaystyle q_ 0 k_ 0 t is large q 0 k 1 t displaystyle q_ 0 k_ 1 t is small and the rest are very small this can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder a little to the 1st and essentially none to the rest in order to make a properly weighted sum we need to transform this list of dot products into a probability distribution over 0 1 displaystyle 0 1 dots this can be accomplished by the softmax function thus giving us the attention weights w 00 w 01 s o f t m a x q 0 k 0 t q 0 k 1 t displaystyle w_ 00 w_ 01 dots mathrm softmax q_ 0 k_ 0 t q_ 0 k_ 1 t dots this is then used to compute the context vector c 0 w 00 v 0 w 01 v 1 displaystyle c_ 0 w_ 00 v_ 0 w_ 01 v_ 1 cdots where v 0 h 0 w v v 1 h 1 w v displaystyle v_ 0 h_ 0 w v v_ 1 h_ 1 w v dots are the value vectors linearly transformed by another matrix to provide the model with freedom to find the best way to represent values without the matrices w q w k w v displaystyle w q w k w v the model would be forced to use the same hidden vector for both key and value which might not be appropriate as these two tasks are not the same this is the dot attention mechanism the particular version described in this section is decoder cross attention as the output context vector is used by the decoder and the input keys and values come from the encoder but the query comes from the decoder thus cross attention more succinctly we can write it as c 0 a t t e n t i o n h 0 d w q h w k h w v s o f t m a x h 0 d w q h w k t h w v displaystyle c_ 0 mathrm attention h_ 0 d w q hw k hw v mathrm softmax h_ 0 d w q hw k t hw v where the matrix h displaystyle h is the matrix whose rows are h 0 h 1 displaystyle h_ 0 h_ 1 dots note that the querying vector h 0 d displaystyle h_ 0 d is not necessarily the same as the key value vector h 0 displaystyle h_ 0 in fact it is theoretically possible for query key and value vectors to all be different though that is rarely done in practice variants many variants of attention implement soft weights such as fast weight programmers or fast weight controllers 1992 a slow neural network outputs the fast weights of another neural network through outer products the slow network learns by gradient descent it was later renamed as linearized self attention bahdanau style attention also referred to as additive attention luong style attention which is known as multiplicative attention highly parallelizable self attention introduced in 2016 as decomposable attention and successfully used in transformers a year later positional attention and factorized positional attention for convolutional neural networks attention mechanisms can be distinguished by the dimension on which they operate namely spatial attention channel attention or combinations much effort has gone into understand attention further by studying their roles in focused settings such as in context learning masked language tasks stripped down transformers bigram statistics n gram statistics pairwise convolutions and arithmetic factoring these variants recombine the encoder side inputs to redistribute those effects to each target output often a correlation style matrix of dot products provides the re weighting coefficients in the figures below w is the matrix of context attention weights similar to the formula in core calculations section above self attention self attention is essentially the same as cross attention except that query key and value vectors all come from the same model both encoder and decoder can use self attention but with subtle differences for encoder self attention we can start with a simple encoder without self attention such as an embedding layer which simply converts each input word into a vector by a fixed lookup table this gives a sequence of hidden vectors h 0 h 1 displaystyle h_ 0 h_ 1 dots these can then be applied to a dot product attention mechanism to obtain h 0 a t t e n t i o n h 0 w q h w k h w v h 1 a t t e n t i o n h 1 w q h w k h w v displaystyle begin aligned h_ 0 mathrm attention h_ 0 w q hw k hw v h_ 1 mathrm attention h_ 1 w q hw k hw v cdots end aligned or more succinctly h a t t e n t i o n h w q h w k h w v displaystyle h mathrm attention hw q hw k hw v this can be applied repeatedly to obtain a multilayered encoder this is the encoder self attention sometimes called the all to all attention as the vector at every position can attend to every other masking for decoder self attention all to all attention is inappropriate because during the autoregressive decoding process the decoder cannot attend to future outputs that has yet to be decoded this can be solved by forcing the attention weights w i j 0 displaystyle w_ ij 0 for all i j displaystyle i j called causal masking this attention mechanism is the causally masked self attention optimizations flash attention the size of the attention matrix is proportional to the square of the number of input tokens therefore when the input is long calculating the attention matrix requires a lot of gpu memory flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy it achieves this by partitioning the attention computation into smaller blocks that fit into the gpu s faster on chip memory reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency mathematical representation standard scaled dot product attention for matrices q r m d k k r n d k displaystyle mathbf q in mathbb r m times d_ k mathbf k in mathbb r n times d_ k and v r n d v displaystyle mathbf v in mathbb r n times d_ v the scaled dot product or qkv attention is defined as attention q k v softmax q k t d k v r m d v displaystyle text attention mathbf q mathbf k mathbf v text softmax left frac mathbf q mathbf k t sqrt d_ k right mathbf v in mathbb r m times d_ v where t displaystyle t denotes transpose and the softmax function is applied independently to every row of its argument the matrix q displaystyle mathbf q contains m displaystyle m queries while matrices k v displaystyle mathbf k mathbf v jointly contain an unordered set of n displaystyle n key value pairs value vectors in matrix v displaystyle mathbf v are weighted using the weights resulting from the softmax operation so that the rows of the m displaystyle m by d v displaystyle d_ v output matrix are confined to the convex hull of the points in r d v displaystyle mathbb r d_ v given by the rows of v displaystyle mathbf v to understand the permutation invariance and permutation equivariance properties of qkv attention let a r m m displaystyle mathbf a in mathbb r m times m and b r n n displaystyle mathbf b in mathbb r n times n be permutation matrices and d r m n displaystyle mathbf d in mathbb r m times n an arbitrary matrix the softmax function is permutation equivariant in the sense that softmax a d b a softmax d b displaystyle text softmax mathbf a mathbf d mathbf b mathbf a text softmax mathbf d mathbf b by noting that the transpose of a permutation matrix is also its inverse it follows that attention a q b k b v a attention q k v displaystyle text attention mathbf a mathbf q mathbf b mathbf k mathbf b mathbf v mathbf a text attention mathbf q mathbf k mathbf v which shows that qkv attention is equivariant with respect to re ordering the queries rows of q displaystyle mathbf q and invariant to re ordering of the key value pairs in k v displaystyle mathbf k mathbf v these properties are inherited when applying linear transforms to the inputs and outputs of qkv attention blocks for example a simple self attention function defined as x attention x t q x t k x t v displaystyle mathbf x mapsto text attention mathbf x mathbf t _ q mathbf x mathbf t _ k mathbf x mathbf t _ v is permutation equivariant with respect to re ordering the rows of the input matrix x displaystyle x in a non trivial way because every row of the output is a function of all the rows of the input similar properties hold for multi head attention which is defined below masked attention when qkv attention is used as a building block for an autoregressive decoder and when at training time all input and output matrices have n displaystyle n rows a masked attention variant is used attention q k v softmax q k t d k m v displaystyle text attention mathbf q mathbf k mathbf v text softmax left frac mathbf q mathbf k t sqrt d_ k mathbf m right mathbf v where the mask m r n n displaystyle mathbf m in mathbb r n times n is a strictly upper triangular matrix with zeros on and below the diagonal and displaystyle infty in every element above the diagonal the softmax output also in r n n displaystyle mathbb r n times n is then lower triangular with zeros in all elements above the diagonal the masking ensures that for all 1 i j n displaystyle 1 leq i j leq n row i displaystyle i of the attention output is independent of row j displaystyle j of any of the three input matrices the permutation invariance and equivariance properties of standard qkv attention do not hold for the masked variant multi head attention multi head attention multihead q k v concat head 1 head h w o displaystyle text multihead mathbf q mathbf k mathbf v text concat text head _ 1 text head _ h mathbf w o where each head is computed with qkv attention as head i attention q w i q k w i k v w i v displaystyle text head _ i text attention mathbf q mathbf w _ i q mathbf k mathbf w _ i k mathbf v mathbf w _ i v and w i q w i k w i v displaystyle mathbf w _ i q mathbf w _ i k mathbf w _ i v and w o displaystyle mathbf w o are parameter matrices the permutation properties of standard unmasked qkv attention apply here also for permutation matrices a b displaystyle mathbf a mathbf b multihead a q b k b v a multihead q k v displaystyle text multihead mathbf a mathbf q mathbf b mathbf k mathbf b mathbf v mathbf a text multihead mathbf q mathbf k mathbf v from which we also see that multi head self attention x multihead x t q x t k x t v displaystyle mathbf x mapsto text multihead mathbf x mathbf t _ q mathbf x mathbf t _ k mathbf x mathbf t _ v is equivariant with respect to re ordering of the rows of input matrix x displaystyle x bahdanau additive attention attention q k v softmax e v displaystyle text attention q k v text softmax e v where e tanh w q q w k k displaystyle e tanh w_ q q w_ k k and w q displaystyle w_ q and w k displaystyle w_ k are learnable weight matrices luong attention general attention q k v softmax q w a k t v displaystyle text attention q k v text softmax qw_ a k t v where w a displaystyle w_ a is a learnable weight matrix see also recurrent neural network seq2seq transformer deep learning architecture attention dynamic neural network references external links olah chris carter shan september 8 2016 attention and augmented recurrent neural networks distill 1 9 distill working group doi 10 23915 distill 00001 dan jurafsky and james h martin 2022 speech and language processing 3rd ed draft january 2022 ch 10 4 attention and ch 9 7 self attention networks transformers alex graves 4 may 2020 attention and memory in deep learning video lecture deepmind ucl via youtube",
            "total_words": 4056,
            "unique_words_percentage": 21.474358974358974,
            "stopwords_percentage": 33.13609467455622
        },
        {
            "title": "Ensemble learning",
            "link": "https://en.wikipedia.org/wiki/Ensemble_learning",
            "content": "in statistics and machine learning ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone unlike a statistical ensemble in statistical mechanics which is usually infinite a machine learning ensemble consists of only a concrete finite set of alternative models but typically allows for much more flexible structure to exist among those alternatives overview supervised learning algorithms search through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem even if this space contains hypotheses that are very well suited for a particular problem it may be very difficult to find a good one ensembles combine multiple hypotheses to form one which should be theoretically better ensemble learning trains two or more machine learning algorithms on a specific classification or regression task the algorithms within the ensemble model are generally referred as base models base learners or weak learners in literature these base models can be constructed using a single modelling algorithm or several different algorithms the idea is to train a diverse set of weak models on the same modelling task such that the outputs of each weak learner have poor predictive ability i e high bias and among all weak learners the outcome and error values exhibit high variance fundamentally an ensemble learning model trains at least two high bias weak and high variance diverse models to be combined into a better performing model the set of weak models which would not produce satisfactory predictive results individually are combined or averaged to produce a single high performing accurate and low variance model to fit the task as required ensemble learning typically refers to bagging bootstrap aggregating boosting or stacking blending techniques to induce high variance among the base models bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample also known as homogeneous parallel ensembles boosting follows an iterative process by sequentially training each base model on the up weighted errors of the previous base model producing an additive model to reduce the final model errors also known as sequential ensemble learning stacking or blending consists of different base models each trained independently i e diverse high variance to be combined into the ensemble model producing a heterogeneous parallel ensemble common applications of ensemble learning include random forests an extension of bagging boosted tree models and gradient boosted tree models models in applications of stacking are generally more task specific such as combining clustering techniques with other parametric and or non parametric techniques the broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model in one sense ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation on the other hand the alternative is to do a lot more learning with one non ensemble model an ensemble may be more efficient at improving overall accuracy for the same increase in compute storage or communication resources by using that increase on two or more methods than would have been improved by increasing resource use for a single method fast algorithms such as decision trees are commonly used in ensemble methods e g random forests although slower algorithms can benefit from ensemble techniques as well by analogy ensemble techniques have been used also in unsupervised learning scenarios for example in consensus clustering or in anomaly detection ensemble theory empirically ensembles tend to yield better results when there is a significant diversity among the models many ensemble methods therefore seek to promote diversity among the models they combine although perhaps non intuitive more random algorithms like random decision trees can be used to produce a stronger ensemble than very deliberate algorithms like entropy reducing decision trees using a variety of strong learning algorithms however has been shown to be more effective than using techniques that attempt to dumb down the models in order to promote diversity it is possible to increase diversity in the training stage of the model using correlation for regression tasks or using information measures such as cross entropy for classification tasks theoretically one can justify the diversity concept because the lower bound of the error rate of an ensemble system can be decomposed into accuracy diversity and the other term the geometric framework ensemble learning including both regression and classification tasks can be explained using a geometric framework within this framework the output of each individual classifier or regressor for the entire dataset can be viewed as a point in a multi dimensional space additionally the target result is also represented as a point in this space referred to as the ideal point the euclidean distance is used as the metric to measure both the performance of a single classifier or regressor the distance between its point and the ideal point and the dissimilarity between two classifiers or regressors the distance between their respective points this perspective transforms ensemble learning into a deterministic problem for example within this geometric framework it can be proved that the averaging of the outputs scores of all base classifiers or regressors can lead to equal or better results than the average of all the individual models it can also be proved that if the optimal weighting scheme is used then a weighted averaging approach can outperform any of the individual classifiers or regressors that make up the ensemble or as good as the best performer at least ensemble size while the number of component classifiers of an ensemble has a great impact on the accuracy of prediction there is a limited number of studies addressing this problem a priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers mostly statistical tests were used for determining the proper number of components more recently a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy it is called the law of diminishing returns in ensemble construction their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy common types of ensembles bayes optimal classifier the bayes optimal classifier is a classification technique it is an ensemble of all the hypotheses in the hypothesis space on average no other ensemble can outperform it the naive bayes classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true to facilitate training data of finite size the vote of each hypothesis is also multiplied by the prior probability of that hypothesis the bayes optimal classifier can be expressed with the following equation y a r g m a x c j c h i h p c j h i p t h i p h i displaystyle y underset c_ j in c mathrm argmax sum _ h_ i in h p c_ j h_ i p t h_ i p h_ i where y displaystyle y is the predicted class c displaystyle c is the set of all possible classes h displaystyle h is the hypothesis space p displaystyle p refers to a probability and t displaystyle t is the training data as an ensemble the bayes optimal classifier represents a hypothesis that is not necessarily in h displaystyle h the hypothesis represented by the bayes optimal classifier however is the optimal hypothesis in ensemble space the space of all possible ensembles consisting only of hypotheses in h displaystyle h this formula can be restated using bayes theorem which says that the posterior is proportional to the likelihood times the prior p h i t p t h i p h i displaystyle p h_ i t propto p t h_ i p h_ i hence y a r g m a x c j c h i h p c j h i p h i t displaystyle y underset c_ j in c mathrm argmax sum _ h_ i in h p c_ j h_ i p h_ i t bootstrap aggregating bagging bootstrap aggregation bagging involves training an ensemble on bootstrapped data sets a bootstrapped set is created by selecting from original training data set with replacement thus a bootstrap set may contain a given example zero one or multiple times ensemble members can also have limits on the features e g nodes of a decision tree to encourage exploring of diverse features the variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble and can strengthen the ensemble to reduce overfitting a member can be validated using the out of bag set the examples that are not in its bootstrap set inference is done by voting of predictions of ensemble members called aggregation it is illustrated below with an ensemble of four decision trees the query example is classified by each tree because three of the four predict the positive class the ensemble s overall classification is positive random forests like the one shown are a common application of bagging boosting boosting involves training successive models by emphasizing training data mis classified by previously learned models initially all data d1 has equal weight and is used to learn a base model m1 the examples mis classified by m1 are assigned a weight greater than correctly classified examples this boosted data d2 is used to train a second base model m2 and so on inference is done by voting in some cases boosting has yielded better accuracy than bagging but tends to over fit more the most common implementation of boosting is adaboost but some newer algorithms are reported to achieve better results bayesian model averaging bayesian model averaging bma makes predictions by averaging the predictions of models weighted by their posterior probabilities given the data bma is known to generally give better answers than a single model obtained e g via stepwise regression especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently the question with any use of bayes theorem is the prior i e the probability perhaps subjective that each model is the best to use for a given purpose conceptually bma can be used with any prior r packages ensemblebma and bma use the prior implied by the bayesian information criterion bic following raftery 1995 r package bas supports the use of the priors implied by akaike information criterion aic and other criteria over the alternative models as well as priors over the coefficients the difference between bic and aic is the strength of preference for parsimony bic s penalty for model complexity is ln n k displaystyle ln n k while aic s is 2 k displaystyle 2k large sample asymptotic theory establishes that if there is a best model then with increasing sample sizes bic is strongly consistent i e will almost certainly find it while aic may not because aic may continue to place excessive posterior probability on models that are more complicated than they need to be on the other hand aic and aicc are asymptotically efficient i e minimum mean square prediction error while bic is not haussler et al 1994 showed that when bma is used for classification its expected error is at most twice the expected error of the bayes optimal classifier burnham and anderson 1998 2002 contributed greatly to introducing a wider audience to the basic ideas of bayesian model averaging and popularizing the methodology the availability of software including other free open source packages for r beyond those mentioned above helped make the methods accessible to a wider audience bayesian model combination bayesian model combination bmc is an algorithmic correction to bayesian model averaging bma instead of sampling each model in the ensemble individually it samples from the space of possible ensembles with model weights drawn randomly from a dirichlet distribution having uniform parameters this modification overcomes the tendency of bma to converge toward giving all the weight to a single model although bmc is somewhat more computationally expensive than bma it tends to yield dramatically better results bmc has been shown to be better on average with statistical significance than bma and bagging use of bayes law to compute model weights requires computing the probability of the data given each model typically none of the models in the ensemble are exactly the distribution from which the training data were generated so all of them correctly receive a value close to zero for this term this would work well if the ensemble were big enough to sample the entire model space but this is rarely possible consequently each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data it essentially reduces to an unnecessarily complex method for doing model selection the possible weightings for an ensemble can be visualized as lying on a simplex at each vertex of the simplex all of the weight is given to a single model in the ensemble bma converges toward the vertex that is closest to the distribution of the training data by contrast bmc converges toward the point where this distribution projects onto the simplex in other words instead of selecting the one model that is closest to the generating distribution it seeks the combination of models that is closest to the generating distribution the results from bma can often be approximated by using cross validation to select the best model from a bucket of models likewise the results from bmc may be approximated by using cross validation to select the best ensemble combination from a random sampling of possible weightings bucket of models a bucket of models is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem when tested with only one problem a bucket of models can produce no better results than the best model in the set but when evaluated across many problems it will typically produce much better results on average than any model in the set the most common approach used for model selection is cross validation selection sometimes called a bake off contest it is described with the following pseudo code for each model m in the bucket do c times where c is some constant randomly divide the training dataset into two sets a and b train m with a test m with b select the model that obtains the highest average score cross validation selection can be summed up as try them all with the training set and pick the one that works best gating is a generalization of cross validation selection it involves training another learning model to decide which of the models in the bucket is best suited to solve the problem often a perceptron is used for the gating model it can be used to pick the best model or it can be used to give a linear weight to the predictions from each model in the bucket when a bucket of models is used with a large set of problems it may be desirable to avoid training some of the models that take a long time to train landmark learning is a meta learning approach that seeks to solve this problem it involves training only the fast but imprecise algorithms in the bucket and then using the performance of these algorithms to help determine which slow but accurate algorithm is most likely to do best amended cross entropy cost an approach for encouraging diversity in classification ensemble the most common approach for training classifier is using cross entropy cost function however one would like to train an ensemble of models that have diversity so when we combine them it would provide best results assuming we use a simple ensemble of averaging k displaystyle k classifiers then the amended cross entropy cost is e k h p q k λ k j k h q j q k displaystyle e k h p q k frac lambda k sum _ j neq k h q j q k where e k displaystyle e k is the cost function of the k t h displaystyle k th classifier q k displaystyle q k is the probability of the k t h displaystyle k th classifier p displaystyle p is the true probability that we need to estimate and λ displaystyle lambda is a parameter between 0 and 1 that define the diversity that we would like to establish when λ 0 displaystyle lambda 0 we want each classifier to do its best regardless of the ensemble and when λ 1 displaystyle lambda 1 we would like the classifier to be as diverse as possible stacking stacking sometimes called stacked generalization involves training a model to combine the predictions of several other learning algorithms first all of the other algorithms are trained using the available data then a combiner algorithm final estimator is trained to make a final prediction using all the predictions of the other algorithms base estimators as additional inputs or using cross validated predictions from the base estimators which can prevent overfitting if an arbitrary combiner algorithm is used then stacking can theoretically represent any of the ensemble techniques described in this article although in practice a logistic regression model is often used as the combiner stacking typically yields performance better than any single one of the trained models it has been successfully used on both supervised learning tasks regression classification and distance learning and unsupervised learning density estimation it has also been used to estimate bagging s error rate it has been reported to out perform bayesian model averaging the two top performers in the netflix competition utilized blending which may be considered a form of stacking voting voting is another form of ensembling see e g weighted majority algorithm machine learning implementations in statistics packages r at least three packages offer bayesian model averaging tools including the bms an acronym for bayesian model selection package the bas an acronym for bayesian adaptive sampling package and the bma package python scikit learn a package for machine learning in python offers packages for ensemble learning including packages for bagging voting and averaging methods matlab classification ensembles are implemented in statistics and machine learning toolbox ensemble learning applications in recent years due to growing computational power which allows for training in large ensemble learning in a reasonable time frame the number of ensemble learning applications has grown increasingly some of the applications of ensemble classifiers include remote sensing land cover mapping land cover mapping is one of the major applications of earth observation satellite sensors using remote sensing and geospatial data to identify the materials and objects which are located on the surface of target areas generally the classes of target materials include roads buildings rivers lakes and vegetation some different ensemble learning approaches based on artificial neural networks kernel principal component analysis kpca decision trees with boosting random forest and automatic design of multiple classifier systems are proposed to efficiently identify land cover objects change detection change detection is an image analysis problem consisting of the identification of places where the land cover has changed over time change detection is widely used in fields such as urban growth forest and vegetation dynamics land use and disaster monitoring the earliest applications of ensemble classifiers in change detection are designed with the majority voting bayesian model averaging and the maximum posterior probability given the growth of satellite data over time the past decade sees more use of time series methods for continuous change detection from image stacks one example is a bayesian ensemble changepoint detection method called beast with the software available as a package rbeast in r python and matlab computer security distributed denial of service distributed denial of service is one of the most threatening cyber attacks that may happen to an internet service provider by combining the output of single classifiers ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds malware detection classification of malware codes such as computer viruses computer worms trojans ransomware and spywares with the usage of machine learning techniques is inspired by the document categorization problem ensemble learning systems have shown a proper efficacy in this area intrusion detection an intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process ensemble learning successfully aids such monitoring systems to reduce their total error face recognition face recognition which recently has become one of the most popular research areas of pattern recognition copes with identification or verification of a person by their digital images hierarchical ensembles based on gabor fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field emotion recognition while speech recognition is mainly based on deep learning because most of the industry players in this field like google microsoft and ibm reveal that the core technology of their speech recognition is based on this approach speech based emotion recognition can also have a satisfactory performance with ensemble learning it is also being successfully used in facial emotion recognition fraud detection fraud detection deals with the identification of bank fraud such as money laundering credit card fraud and telecommunication fraud which have vast domains of research and applications of machine learning because ensemble learning improves the robustness of the normal behavior modelling it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems financial decision making the accuracy of prediction of business failure is a very crucial issue in financial decision making therefore different ensemble classifiers are proposed to predict financial crises and financial distress also in the trade based manipulation problem where traders attempt to manipulate stock prices by buying and selling activities ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation medicine ensemble classifiers have been successfully applied in neuroscience proteomics and medical diagnosis like in neuro cognitive disorder i e alzheimer or myotonic dystrophy detection based on mri datasets and cervical cytology classification see also ensemble averaging machine learning bayesian structural time series bsts references further reading zhou zhihua 2012 ensemble methods foundations and algorithms chapman and hall crc isbn 978 1 439 83003 1 robert schapire yoav freund 2012 boosting foundations and algorithms mit isbn 978 0 262 01718 3 external links robi polikar ed ensemble learning scholarpedia the waffles machine learning toolkit contains implementations of bagging boosting bayesian model averaging bayesian model combination bucket of models and other ensemble techniques",
            "total_words": 3878,
            "unique_words_percentage": 25.399690562145437,
            "stopwords_percentage": 38.44765342960289
        },
        {
            "title": "Transformer (deep learning architecture)",
            "link": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
            "content": "a transformer is a deep learning architecture that was developed by researchers at google and is based on the multi head attention mechanism which was proposed in the 2017 paper attention is all you need text is converted to numerical representations called tokens and each token is converted into a vector via lookup from a word embedding table at each layer each token is then contextualized within the scope of the context window with other unmasked tokens via a parallel multi head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished transformers have the advantage of having no recurrent units therefore requiring less training time than earlier recurrent neural architectures rnns such as long short term memory lstm later variations have been widely adopted for training large language models llm on large language datasets such as the wikipedia corpus and common crawl transformers were first developed as an improvement over previous architectures for machine translation but have found many applications since they are used in large scale natural language processing computer vision vision transformers reinforcement learning audio multimodal learning robotics and even playing chess it has also led to the development of pre trained systems such as generative pre trained transformers gpts and bert bidirectional encoder representations from transformers history predecessors for many years sequence modelling and generation was done by using plain recurrent neural networks rnns a well cited early example was the elman network 1990 in theory the information from one token can propagate arbitrarily far down the sequence but in practice the vanishing gradient problem leaves the model s state at the end of a long sentence without precise extractable information about preceding tokens a key breakthrough was lstm 1995 a rnn which used various innovations to overcome the vanishing gradient problem allowing efficient learning of long sequence modelling one key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons so called multiplicative units neural networks using multiplicative units were later called sigma pi networks or higher order networks lstm became the standard architecture for long sequence modelling until the 2017 publication of transformers however lstm still used sequential processing like most other rnns specifically rnns operate one token at a time from first to last they cannot operate in parallel over all tokens in a sequence modern transformers overcome this problem but unlike rnns they require computation time that is quadratic in the size of the context window the linearly scaling fast weight controller 1992 learns to compute a weight matrix for further processing depending on the input one of its two networks has fast weights or dynamic links 1981 a slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries this was later shown to be equivalent to the unnormalized linear transformer attention with seq2seq the idea of encoder decoder sequence transduction had been developed in the early 2010s see previous papers the papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014 a 380m parameter model for machine translation uses two long short term memories lstm its architecture consists of two parts the encoder is an lstm that takes in a sequence of tokens and turns it into a vector the decoder is another lstm that converts the vector into a sequence of tokens similarly another 130m parameter model used gated recurrent units gru instead of lstm later research showed that grus are neither better nor worse than lstms for seq2seq these early seq2seq models had no attention mechanism and the state vector is accessible only after the last word of the source text was processed although in theory such a vector retains the information about the whole original sentence in practice the information is poorly preserved this is because the input is processed sequentially by one recurrent network into a fixed size output vector which is then processed by another recurrent network into an output if the input is long then the output vector would not be able to contain all relevant information degrading the output as evidence reversing the input sentence improved seq2seq translation the rnnsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem of the fixed size output vector allowing the model to process long distance dependencies more easily the name is because it emulates searching through a source sentence during decoding a translation the relative performances were compared between global that of rnnsearch and local sliding window attention model architectures for machine translation finding that mixed attention had higher quality than global attention while local attention reduced translation time in 2016 google translate was revamped to google neural machine translation which replaced the previous model based on statistical machine translation the new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional lstm it took nine months to develop and it outperformed the statistical approach which took ten years to develop parallelizing attention seq2seq models with attention including self attention still suffered from the same issue with recurrent networks which is that they are hard to parallelize which prevented them from being accelerated on gpus in 2016 decomposable attention applied a self attention mechanism to feedforward networks which are easy to parallelize and achieved sota result in textual entailment with an order of magnitude less parameters than lstms one of its authors jakob uszkoreit suspected that attention without recurrence is sufficient for language translation thus the title attention is all you need that hypothesis was against conventional wisdom at the time and even his father hans uszkoreit a well known computational linguist was skeptical in the same year self attention called intra attention or intra sentence attention was proposed for lstms in 2017 the original 100m sized encoder decoder transformer model was proposed in the attention is all you need paper at the time the focus of the research was on improving seq2seq for machine translation by removing its recurrence to process all tokens in parallel but preserving its dot product attention mechanism to keep its text processing performance this led to the introduction of a multi head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence its parallelizability was an important factor to its widespread use in large neural networks ai boom era already in spring 2017 even before the attention is all you need preprint was published one of the co authors applied the decoder only variation of the architecture to generate fictitious wikipedia articles transformer architecture is now used in many generative models that contribute to the ongoing ai boom in language modelling elmo 2018 was a bi directional lstm that produces contextualized word embeddings improving upon the line of research from bag of words and word2vec it was followed by bert 2018 an encoder only transformer model in 2019 october google started using bert to process search queries in 2020 google translate replaced the previous rnn encoder rnn decoder model by a transformer encoder rnn decoder model starting in 2018 the openai gpt series of decoder only transformers became state of the art in natural language generation in 2022 a chatbot based on gpt 3 chatgpt became unexpectedly popular triggering a boom around large language models since 2020 transformers have been applied in modalities beyond text including the vision transformer speech recognition robotics and multimodal the vision transformer in turn stimulated new developments in convolutional neural networks image and video generators like dall e 2021 stable diffusion 3 2024 and sora 2024 are based on the transformer architecture training methods for stabilizing training the plain transformer architecture had difficulty converging in the original paper the authors recommended using learning rate warmup that is the learning rate should linearly scale up from 0 to maximal value for the first part of the training usually recommended to be 2 of the total number of training steps before decaying again a 2020 paper found that using layer normalization before instead of after multiheaded attention and feedforward layers stabilizes training not requiring learning rate warmup pretrain finetune transformers typically are first pretrained by self supervised learning on a large generic dataset followed by supervised fine tuning on a small task specific dataset the pretrain dataset is typically an unlabeled large corpus such as the pile tasks for pretraining and fine tuning commonly include language modeling next sentence prediction question answering reading comprehension sentiment analysis paraphrasing the t5 transformer report documents a large number of natural language pretraining tasks some examples are restoring or repairing incomplete or corrupted text for example the input thank you me to your party week might generate the output thank you for inviting me to your party last week translation between natural languages machine translation judging the pragmatic acceptability of natural language for example the following sentence might be judged not acceptable because even though it is syntactically well formed it is improbable in ordinary human usage the course is jumping well note that while each of these tasks is trivial or obvious for human native speakers of the language or languages they have typically proved challenging for previous generations of machine learning architecture tasks in general there are 3 classes of language modelling tasks masked autoregressive and prefixlm these classes are independent of a specific modeling architecture such as transformer but they are often discussed in the context of transformer in a masked task one or more of the tokens is masked out and the model would produce a probability distribution predicting what the masked out tokens are based on the context the loss function for the task is typically sum of log perplexities for the masked out tokens loss t masked tokens ln probability of t conditional on its context displaystyle text loss sum _ t in text masked tokens ln text probability of t text conditional on its context and the model is trained to minimize this loss function the bert series of models are trained for masked token prediction and another task in an autoregressive task the entire sequence is masked at first and the model produces a probability distribution for the first token then the first token is revealed and the model predicts the second token and so on the loss function for the task is still typically the same the gpt series of models are trained by autoregressive tasks in a prefixlm task the sequence is divided into two parts the first part is presented as context and the model predicts the first token of the second part then that would be revealed and the model predicts the second token and so on the loss function for the task is still typically the same the t5 series of models are trained by prefixlm tasks note that masked as in masked language modelling is not masked as in masked attention and prefixlm prefix language modeling is not prefixlm prefix language model architecture all transformers have the same primary components tokenizers which convert text into tokens embedding layer which converts tokens and positions of the tokens into vector representations transformer layers which carry out repeated transformations on the vector representations extracting more and more linguistic information these consist of alternating attention and feedforward layers there are two major types of transformer layers encoder layers and decoder layers with further variants un embedding layer which converts the final vector representations back to a probability distribution over the tokens the following description follows exactly the transformer as described in the original paper there are variants described in the following section by convention we write all vectors as row vectors this for example means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right as x w displaystyle xw tokenization as the transformer architecture natively processes numerical data not text there must be a translation between text and tokens a token is an integer that represents a character or a short segment of characters on the input side the input text is parsed into a token sequence similarly on the output side the output tokens are parsed back to text the module doing the conversion between texts and token sequences is a tokenizer the set of all tokens is the vocabulary of the tokenizer and its size is the vocabulary size n vocabulary displaystyle n_ text vocabulary when faced with tokens outside the vocabulary typically a special token is used written as for unknown some commonly used tokenizers are byte pair encoding wordpiece and sentencepiece embedding each token is converted into an embedding vector via a lookup table equivalently stated it multiplies a one hot representation of the token by an embedding matrix m displaystyle m for example if the input token is 3 displaystyle 3 then the one hot representation is 0 0 0 1 0 0 displaystyle and its embedding vector is e m b e d 3 0 0 0 1 0 0 m displaystyle mathrm embed 3 m the token embedding vectors are added to their respective positional encoding vectors see below producing the sequence of input vectors the number of dimensions in an embedding vector is called hidden size or embedding size and written as d emb displaystyle d_ text emb this size is written as d model displaystyle d_ text model in the original transformer paper un embedding an un embedding layer is almost the reverse of an embedding layer whereas an embedding layer converts a token into a vector an un embedding layer converts a vector into a probability distribution over tokens the un embedding layer is a linear softmax layer u n e m b e d x s o f t m a x x w b displaystyle mathrm unembed x mathrm softmax xw b the matrix has shape d emb n vocabulary displaystyle d_ text emb n_ text vocabulary the embedding matrix m displaystyle m and the un embedding matrix w displaystyle w are sometimes required to be transposes of each other a practice called weight tying positional encoding a positional encoding is a fixed size vector representation of the relative positions of tokens within a sequence it provides the transformer model with information about where the words are in the input sequence this shall induce a bias towards the order of the input sequence so that for example the input sequence man bites dog is processed differently from dog bites man the positional encoding is defined as a function of type f r r d d z d 0 displaystyle f mathbb r to mathbb r d d in mathbb z d 0 where d displaystyle d is a positive even integer the full positional encoding defined in the original paper is f t 2 k f t 2 k 1 sin θ cos θ k 0 1 d 2 1 displaystyle f t _ 2k f t _ 2k 1 sin theta cos theta quad forall k in 0 1 ldots d 2 1 where θ t r k r n 2 d displaystyle theta frac t r k r n 2 d here n displaystyle n is a free parameter that should be significantly larger than the biggest k displaystyle k that would be input into the positional encoding function the original paper uses n 10000 displaystyle n 10000 the function is in a simpler form when written as a complex function of type f r c d 2 displaystyle f mathbb r to mathbb c d 2 f t e i t r k k 0 1 d 2 1 displaystyle f t left e it r k right _ k 0 1 ldots frac d 2 1 where r n 2 d displaystyle r n 2 d the main reason for using this positional encoding function is that using it shifts are linear transformations f t δ t d i a g f δ t f t displaystyle f t delta t mathrm diag f delta t f t where δ t r displaystyle delta t in mathbb r is the distance one wishes to shift this allows the transformer to take any encoded position and find the encoding of the position n steps ahead or n steps behind by a matrix multiplication by taking a linear sum any convolution can also be implemented as linear transformations j c j f t δ t j j c j d i a g f δ t j f t displaystyle sum _ j c_ j f t delta t_ j left sum _ j c_ j mathrm diag f delta t_ j right f t for any constants c j displaystyle c_ j this allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors this sum of encoded positions when fed into the attention mechanism would create attention weights on its neighbors much like what happens in a convolutional neural network language model in the author s words we hypothesized it would allow the model to easily learn to attend by relative position in typical implementations all operations are done over the real numbers not the complex numbers but since complex multiplication can be implemented as real 2 by 2 matrix multiplication this is a mere notational difference encoder decoder overview like earlier seq2seq models the original transformer model used an encoder decoder architecture the encoder consists of encoding layers that process all the input tokens together one layer after another while the decoder consists of decoding layers that iteratively process the encoder s output and the decoder s output tokens so far the purpose of each encoder layer is to create contextualized representations of the tokens where each representation corresponds to a token that mixes information from other input tokens via self attention mechanism each decoder layer contains two attention sublayers 1 cross attention for incorporating the output of encoder contextualized input token representations and 2 self attention for mixing information among the input tokens to the decoder i e the tokens generated so far during inference time both the encoder and decoder layers have a feed forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps these feed forward layers contain most of the parameters in a transformer model feedforward network the feedforward network ffn modules in a transformer are 2 layered multilayer perceptrons f f n x ϕ x w 1 b 1 w 2 b 2 displaystyle mathrm ffn x phi xw 1 b 1 w 2 b 2 where ϕ displaystyle phi is its activation function the original transformer used relu activation the number of neurons in the middle layer is called intermediate size gpt filter size bert or feedforward size bert it is typically larger than the embedding size for example in both gpt 2 series and bert series the intermediate size of a model is 4 times its embedding size d ffn 4 d emb displaystyle d_ text ffn 4d_ text emb scaled dot product attention attention head the attention mechanism used in the transformer architecture are scaled dot product attention units for each unit the transformer model learns three weight matrices the query weights w q displaystyle w q the key weights w k displaystyle w k and the value weights w v displaystyle w v the module takes three sequences a query sequence a key sequence and a value sequence the query sequence is a sequence of length ℓ seq query displaystyle ell _ text seq query and each entry is a vector of dimension d emb query displaystyle d_ text emb query similarly for the key and value sequences for each vector x i query displaystyle x_ i text query in the query sequence it is multiplied by a matrix w q displaystyle w q to produce a query vector q i x i query w q displaystyle q_ i x_ i text query w q the matrix of all query vectors is the query matrix q x query w q displaystyle q x_ text query w q similarly we construct the key matrix k x key w k displaystyle k x_ text key w k and the value matrix v x value w v displaystyle v x_ text value w v it is usually the case that all w q w k w v displaystyle w q w k w v are square matrices meaning d emb query d query displaystyle d_ text emb query d_ text query etc attention weights are calculated using the query and key vectors the attention weight a i j displaystyle a_ ij from token i displaystyle i to token j displaystyle j is the dot product between q i displaystyle q_ i and k j displaystyle k_ j the attention weights are divided by the square root of the dimension of the key vectors d k displaystyle sqrt d_ k which stabilizes gradients during training and passed through a softmax which normalizes the weights the fact that w q displaystyle w q and w k displaystyle w k are different matrices allows attention to be non symmetric if token i displaystyle i attends to token j displaystyle j i e q i k j displaystyle q_ i cdot k_ j is large this does not necessarily mean that token j displaystyle j will attend to token i displaystyle i i e q j k i displaystyle q_ j cdot k_ i could be small the output of the attention unit for token i displaystyle i is the weighted sum of the value vectors of all tokens weighted by a i j displaystyle a_ ij the attention from token i displaystyle i to each token the attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations the matrices q displaystyle q k displaystyle k and v displaystyle v are defined as the matrices where the i displaystyle i th rows are vectors q i displaystyle q_ i k i displaystyle k_ i and v i displaystyle v_ i respectively then we can represent the attention as attention q k v softmax q k t d k v displaystyle begin aligned text attention q k v text softmax left frac qk mathrm t sqrt d_ k right v end aligned where the softmax is applied over each of the rows of the matrix the number of dimensions in a query vector is query size d query displaystyle d_ text query and similarly for the key size d key displaystyle d_ text key and value size d value displaystyle d_ text value the output dimension of an attention head is its head dimension d head displaystyle d_ text head the attention mechanism requires the following three equalities to hold ℓ seq key ℓ seq value d query d key d value d head displaystyle ell _ text seq key ell _ text seq value d_ text query d_ text key d_ text value d_ text head but is otherwise unconstrained if the attention head is used in a self attention fashion then x query x key x value displaystyle x_ text query x_ text key x_ text value if the attention head is used in a cross attention fashion then usually x query x key x value displaystyle x_ text query neq x_ text key x_ text value it is theoretically possible for all three to be different but that is rarely the case in practice multiheaded attention one set of w q w k w v displaystyle left w q w k w v right matrices is called an attention head and each layer in a transformer model has multiple attention heads while each attention head attends to the tokens that are relevant to each token multiple attention heads allow the model to do this for different definitions of relevance specifically the query and key projection matrices w q displaystyle w q and w k displaystyle w k which are involved in the attention score computation defines the relevance meanwhile the value projection matrix w v displaystyle w v in combination with the part of the output projection matrix w o displaystyle w o determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits in addition the scope of attention or the range of token relationships captured by each attention head can expand as tokens pass through successive layers this allows the model to capture more complex and long range dependencies in deeper layers many transformer attention heads encode relevance relations that are meaningful to humans for example some attention heads can attend mostly to the next word while others mainly attend from verbs to their direct objects the computations for each attention head can be performed in parallel which allows for fast processing the outputs for the attention layer are concatenated to pass into the feed forward neural network layers concretely let the multiple attention heads be indexed by i displaystyle i then we have multiheadedattention q k v concat i n heads attention x w i q x w i k x w i v w o displaystyle text multiheadedattention q k v text concat _ i in text attention xw_ i q xw_ i k xw_ i v w o where the matrix x displaystyle x is the concatenation of word embeddings and the matrices w i q w i k w i v displaystyle w_ i q w_ i k w_ i v are projection matrices owned by individual attention head i displaystyle i and w o displaystyle w o is a final projection matrix owned by the whole multi headed attention head it is theoretically possible for each attention head to have a different head dimension d head displaystyle d_ text head but that is rarely the case in practice as an example in the smallest gpt 2 model there are only self attention mechanisms it has the following dimensions d emb 768 n head 12 d head 64 displaystyle d_ text emb 768 n_ text head 12 d_ text head 64 since 12 64 768 displaystyle 12 times 64 768 its output projection matrix w o r 12 64 768 displaystyle w o in mathbb r 12 times 64 times 768 is a square matrix masked attention the transformer architecture is constructed to calculate output tokens iteratively assuming t 0 displaystyle t 0 refers to the calculation of the first output token i 0 displaystyle i 0 for step t 0 displaystyle t 0 the output token i 0 displaystyle i 0 shall remain constant this ensures properties of the model similar to autoregressive models therefore at every time step t displaystyle t the calculation for all outputs i displaystyle i should not have access to tokens at position j displaystyle j for j i displaystyle j i as it naturally is the case for time step t i displaystyle t i when tokens j t displaystyle j t are not yet calculated this behavior may be accomplished before the softmax stage by adding a mask matrix m displaystyle m that is displaystyle infty at entries where the attention link must be cut and 0 displaystyle 0 at other places maskedattention q k v softmax m q k t d k v displaystyle begin aligned text maskedattention q k v text softmax left m frac qk mathrm t sqrt d_ k right v end aligned the following matrix is commonly used in decoder self attention modules called causal masking m causal 0 0 0 0 0 0 0 0 0 0 displaystyle m_ text causal begin bmatrix 0 infty infty dots infty 0 0 infty dots infty 0 0 0 dots infty vdots vdots vdots ddots vdots 0 0 0 dots 0 end bmatrix in words it means that each token can pay attention to itself and every token before it but not any after it a non masked attention module can be thought of as a masked attention module where the mask has all entries zero as an example of an uncommon use of mask matrix the xlnet considers all masks of the form p m causal p 1 displaystyle pm_ text causal p 1 where p displaystyle p is a random permutation matrix encoder an encoder consists of an embedding layer followed by multiple encoder layers each encoder layer consists of two major components a self attention mechanism and a feed forward layer it takes an input as a sequence of input vectors applies the self attention mechanism to produce an intermediate sequence of vectors then applies the feed forward layer for each vector individually schematically we have given input vectors h 0 h 1 combine them into a matrix h h 0 h 1 encoderlayer h ffn multiheadedattention h h h 0 ffn multiheadedattention h h h 1 displaystyle begin aligned text given input vectors h_ 0 h_ 1 dots text combine them into a matrix h begin bmatrix h_ 0 h_ 1 vdots end bmatrix text encoderlayer h begin bmatrix text ffn text multiheadedattention h h h _ 0 text ffn text multiheadedattention h h h _ 1 vdots end bmatrix end aligned where ffn displaystyle text ffn stands for feed forward network we can more succinctly write it as encoderlayer h ffn multiheadedattention h h h displaystyle text encoderlayer h text ffn text multiheadedattention h h h with the implicit convention that the ffn displaystyle text ffn is applied to each row of the matrix individually the encoder layers are stacked the first encoder layer takes the sequence of input vectors from the embedding layer producing a sequence of vectors this sequence of vectors is processed by the second encoder and so on the output from the final encoder layer is then used by the decoder as the encoder processes the entire input all at once every token can attend to every other token all to all attention so there is no need for causal masking decoder a decoder consists of an embedding layer followed by multiple decoder layers followed by an un embedding layer each decoder consists of three major components a causally masked self attention mechanism a cross attention mechanism and a feed forward neural network the decoder functions in a similar fashion to the encoder but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders this mechanism can also be called the encoder decoder attention like the first encoder the first decoder takes positional information and embeddings of the output sequence as its input rather than encodings the transformer must not use the current or future output to predict an output so the output sequence must be partially masked to prevent this reverse information flow this allows for autoregressive text generation for decoding all to all attention is inappropriate because a token cannot attend to tokens not yet generated thus the self attention module in the decoder is causally masked in contrast the cross attention mechanism attends to the output vectors of the encoder which is computed before the decoder starts decoding consequently there is no need for masking in the cross attention mechanism schematically we have h maskedmultiheadedattention h h h decoderlayer h ffn multiheadedattention h h e h e displaystyle begin aligned h text maskedmultiheadedattention h h h text decoderlayer h text ffn text multiheadedattention h h e h e end aligned where h e displaystyle h e is the matrix with rows being the output vectors from the encoder the last decoder is followed by a final un embedding layer to produce the output probabilities over the vocabulary then one of the tokens is sampled according to the probability and the decoder can be run again to produce the next token etc autoregressively generating output text full transformer architecture sublayers each encoder layer contains 2 sublayers the self attention and the feedforward network each decoder layer contains 3 sublayers the causally masked self attention the cross attention and the feedforward network the final points of detail are the residual connections and layer normalization layernorm or ln which while conceptually unnecessary are necessary for numerical stability and convergence similarly to how the feedforward network modules are applied individually to each vector the layernorm is also applied individually to each vector there are two common conventions in use the post ln and the pre ln convention in the post ln convention the output of each sublayer is l a y e r n o r m x s u b l a y e r x displaystyle mathrm layernorm x mathrm sublayer x where s u b l a y e r x displaystyle mathrm sublayer x is the function implemented by the sublayer itself in the pre ln convention the output of each sublayer is x s u b l a y e r l a y e r n o r m x displaystyle x mathrm sublayer mathrm layernorm x the original 2017 transformer used the post ln convention it was difficult to train and required careful hyperparameter tuning and a warm up in learning rate where it starts small and gradually increases the pre ln convention proposed several times in 2018 was found to be easier to train requiring no warm up leading to faster convergence pseudocode the following is the pseudocode for a standard pre ln encoder decoder transformer adapted from input encoder input t_e decoder input t_d output array of probability distributions with shape decoder vocabulary size x length decoder output sequence encoder z_e encoder tokenizer t_e for each t in 1 length z_e do z_e encoder embedding z_e encoder positional_embedding t for each l in 1 length encoder layers do layer encoder layers first sublayer z_e_copy copy z_e for each t in 1 length z_e do z_e layer layer_norm z_e z_e layer multiheaded_attention z_e z_e z_e for each t in 1 length z_e do z_e z_e z_e_copy second sublayer z_e_copy copy z_e for each t in 1 length z_e do z_e layer layer_norm z_e z_e layer feedforward z_e for each t in 1 length z_e do z_e z_e z_e_copy for each t in 1 length z_e do z_e encoder final_layer_norm z_e decoder z_d decoder tokenizer t_d for each t in 1 length z_d do z_d decoder embedding z_d decoder positional_embedding t for each l in 1 length decoder layers do layer decoder layers first sublayer z_d_copy copy z_d for each t in 1 length z_d do z_d layer layer_norm z_d z_d layer masked_multiheaded_attention z_d z_d z_d for each t in 1 length z_d do z_d z_d z_d_copy second sublayer z_d_copy copy z_d for each t in 1 length z_d do z_d layer layer_norm z_d z_d layer multiheaded_attention z_d z_e z_e for each i in 1 length z_d do z_d z_d z_d_copy third sublayer z_d_copy copy z_d for each t in 1 length z_d do z_d layer layer_norm z_d z_d layer feedforward z_d for each t in 1 length z_d do z_d z_d z_d_copy z_d decoder final_layer_norm z_d output_distributions for each t in 1 length z_d do output_distributions append decoder unembed z_d return output_distributions terminology the transformer architecture being modular allows variations several common variations are described here an encoder only transformer applies the encoder to map an input text into a sequence of vectors that represent the input text this is usually used for text embedding and representation learning for downstream applications bert is encoder only they are less often used currently as they were found to be not significantly better than training an encoder decoder transformer then taking just the encoder a decoder only transformer is not literally decoder only since without an encoder the cross attention mechanism has nothing to attend to thus the decoder layers in a decoder only transformer is composed of just two sublayers the causally masked self attention and the feedforward network this is usually used for text generation and instruction following the models in the gpt series and chinchilla series are decoder only an encoder decoder transformer is generally the same as the original transformer with 2 sublayers per encoder layer and 3 sublayers per decoder layer etc they might have minor architectural improvements such as alternative activation functions changing the location of normalization etc this is also usually used for text generation and instruction following the models in the t5 series are encoder decoder a prefixlm prefix language model is a decoder only architecture but with prefix masking which is different from causal masking specifically it has mask of the form figure 3 m prefixlm 0 0 m causal displaystyle m_ text prefixlm begin bmatrix mathbf 0 infty mathbf 0 m_ text causal end bmatrix where the first columns correspond to the prefix and the subsequent columns correspond to the autoregressively generated text based on the prefix they resemble encoder decoder models but has less sparsity such models are rarely used though they are cited as theoretical possibilities and benchmarked comparisons there are also mixed seq2seq models for example in 2020 google translate replaced the previous rnn encoder rnn decoder model by a transformer encoder rnn decoder model on the argument that an rnn decoder runs much faster than transformer decoder when run autoregressively subsequent work alternative activation functions the original transformer uses relu activation function other activation functions were developed the llama series used swiglu both gpt 1 and bert used gelu alternative activation functions are often used in combination with gated linear units in the feedforward module alternative normalizations the normalization used in the transformer can be different from layernorm one example is rmsnorm which is used in the llama series other examples include capsulenorm scalenorm or fixnorm alternative positional encodings transformers may use other positional encoding methods than sinusoidal the original transformer paper reported using a learned positional encoding but finding it not superior to the sinusoidal one later found that causal masking itself provides enough signal to a transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module rope rope rotary positional embedding is best explained by considering a list of 2 dimensional vectors x 1 1 x 1 2 x 2 1 x 2 2 x 3 1 x 3 2 displaystyle now pick some angle θ displaystyle theta then rope encoding is rope x m 1 x m 2 m cos m θ sin m θ sin m θ cos m θ x m 1 x m 2 x m 1 cos m θ x m 2 sin m θ x m 2 cos m θ x m 1 sin m θ displaystyle text rope big x_ m 1 x_ m 2 m big begin pmatrix cos m theta sin m theta sin m theta cos m theta end pmatrix begin pmatrix x_ m 1 x_ m 2 end pmatrix begin pmatrix x_ m 1 cos m theta x_ m 2 sin m theta x_ m 2 cos m theta x_ m 1 sin m theta end pmatrix equivalently if we write the 2 dimensional vectors as complex numbers z m x m 1 i x m 2 displaystyle z_ m x_ m 1 ix_ m 2 then rope encoding is just multiplication by an angle rope z m m e i m θ z m displaystyle text rope big z_ m m big e im theta z_ m for a list of 2 n displaystyle 2n dimensional vectors a rope encoder is defined by a sequence of angles θ 1 θ n displaystyle theta 1 theta n then the rope encoding is applied to each pair of coordinates the benefit of rope is that the dot product between two vectors depends on their relative location only rope x m t rope y n rope x m k t rope y n k displaystyle text rope big x m big t text rope big y n big text rope big x m k big t text rope big y n k big for any integer k displaystyle k alibi alibi attention with linear biases is not a replacement for the positional encoder on the original transformer instead it is an additional positional encoder that is directly plugged into the attention mechanism specifically the alibi attention mechanism is attention q k v softmax q k t d k s b v displaystyle begin aligned text attention q k v text softmax left frac qk mathrm t sqrt d_ k sb right v end aligned here s displaystyle s is a real number scalar and b displaystyle b is the linear bias matrix defined by b 0 1 2 3 1 0 1 2 2 1 0 1 3 2 1 0 displaystyle b begin pmatrix 0 1 2 3 cdots 1 0 1 2 cdots 2 1 0 1 cdots 3 2 1 0 cdots vdots vdots vdots vdots ddots end pmatrix in other words b i j j i displaystyle b_ i j j i the idea being that the linear bias matrix is a softened mask just as 0 displaystyle 0 represent full attention paid and displaystyle infty represents no attention paid the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction alibi allows pretraining on short context windows then finetuning on longer context windows since it is directly plugged into the attention mechanism it can be combined with any positional encoder that is plugged into the bottom of the entire network which is where the sinusoidal encoder on the original transformer as well as rope and many others are located relative position encodings relative position encodings is similar to alibi but more generic attention q k v softmax q k t d k b v displaystyle begin aligned text attention q k v text softmax left frac qk mathrm t sqrt d_ k b right v end aligned where b displaystyle b is a toeplitz matrix that is b i j b i j displaystyle b_ i j b_ i j whenever i j i j displaystyle i j i j this is contrasted with the original sinusoidal positional encoding which is an absolute positional encoding efficient implementation the transformer model has been implemented in standard deep learning frameworks such as tensorflow and pytorch transformers is a library produced by hugging face that supplies transformer based architectures and pretrained models flashattention flashattention is an algorithm that implements the transformer attention mechanism efficiently on a gpu it performs matrix multiplications in blocks such that each block fits within the cache of a gpu and by careful management of the blocks it minimizes data copying between gpu caches as data movement is slow an improved version flashattention 2 was developed to cater to the rising demand for language models capable of handling longer context lengths it offers enhancements in work partitioning and parallelism enabling it to achieve up to 230 tflops s on a100 gpus fp16 bf16 a 2x speed increase over the original flashattention key advancements in flashattention 2 include the reduction of non matmul flops improved parallelism over the sequence length dimension better work partitioning between gpu warps and added support for head dimensions up to 256 and multi query attention mqa and grouped query attention gqa benchmarks revealed flashattention 2 to be up to 2x faster than flashattention and up to 9x faster than a standard attention implementation in pytorch future developments include optimization for new hardware like h100 gpus and new data types like fp8 multi query attention multi query attention changes the multiheaded attention mechanism whereas normally multiheadedattention q k v concat i n heads attention x w i q x w i k x w i v w o displaystyle text multiheadedattention q k v text concat _ i in left text attention xw_ i q xw_ i k xw_ i v right w o with multi query attention there is just one w k w v displaystyle w k w v thus multiqueryattention q k v concat i n heads attention x w i q x w k x w v w o displaystyle text multiqueryattention q k v text concat _ i in left text attention xw_ i q xw k xw v right w o this has a neutral effect on model quality and training speed but increases inference speed more generally grouped query attention gqa partitions attention heads into groups each of which shares the key value pair mqa is gqa with one group while standard multiheaded atteniton is gqa with the maximal number of groups caching when an autoregressive transformer is used for inference such as generating text the query vector is different at each step but the already computed key and value vectors are always the same the kv caching method saves the computed key and value vectors at each attention block so that they are not recomputed at each new token pagedattention applies memory paging to kv caching if a transformer is used with a baked in prompt such as then the key and value vectors can be computed for the prompt and saved on disk the saving in compute is significant when the model is used for many short interactions such as in online chatbots speculative decoding transformers are used in large language models for autoregressive sequence generation generating a stream of text one token at a time however in most settings decoding from language models is memory bound meaning that we have spare compute power available speculative decoding uses this spare compute power by computing several tokens in parallel similarly to speculative execution in cpus future tokens are computed concurrently by speculating on the value of previous tokens and are later discarded if it turns out the speculation was incorrect specifically consider a transformer model like gpt 3 with a context window size of 512 to generate an entire context window autoregressively with greedy decoding it must be run for 512 times each time generating a token x 1 x 2 x 512 displaystyle x_ 1 x_ 2 x_ 512 however if we had some educated guess for the values of these tokens we could verify all of them in parallel in one run of the model by checking that each x t displaystyle x_ t is indeed the token with the largest log likelihood in the t displaystyle t th output in speculative decoding a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model for example suppose a small model generated four speculative tokens x 1 x 2 x 3 x 4 displaystyle tilde x _ 1 tilde x _ 2 tilde x _ 3 tilde x _ 4 these tokens are run through the larger model and only x 1 displaystyle tilde x _ 1 and x 2 displaystyle tilde x _ 2 are accepted the same run of the large model already generated a new token x 3 displaystyle x_ 3 to replace x 3 displaystyle tilde x _ 3 and x 4 displaystyle tilde x _ 4 is completely discarded the process then repeats starting from the 4th token until all tokens are generated for non greedy decoding similar ideas apply except the speculative tokens are accepted or rejected stochastically in a way that guarantees the final output distribution is the same as if speculative decoding was not used sub quadratic transformers training transformer based architectures can be expensive especially for long inputs many methods have been developed to attempt to address the issue long range arena 2020 is a standard benchmark for comparing the behavior of transformer architectures over long inputs alternative attention graphs the standard attention graph is either all to all or causal both of which scales as o n 2 displaystyle o n 2 where n displaystyle n is the number of tokens in a sequence reformer 2020 reduces the computational load from o n 2 displaystyle o n 2 to o n ln n displaystyle o n ln n by using locality sensitive hashing and reversible layers sparse attention uses attention graphs that grows slower than o n 2 displaystyle o n 2 for example bigbird 2020 uses random small world networks which grows as o n displaystyle o n ordinary transformers require a memory size that is quadratic in the size of the context window attention free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value random feature attention random feature attention 2021 uses fourier random features φ x 1 d cos w 1 x sin w 1 x cos w d x sin w d x t displaystyle varphi x frac 1 sqrt d t where w 1 w d displaystyle w_ 1 w_ d are independent samples from the normal distribution n 0 σ 2 i displaystyle n 0 sigma 2 i this choice of parameters satisfy e φ x φ y e x y 2 2 σ 2 displaystyle mathbb e e frac x y 2 2 sigma 2 or e x y σ 2 e e x 2 2 σ 2 φ x e y 2 2 σ 2 φ y e x 2 2 σ 2 φ x e y 2 2 σ 2 φ y displaystyle e langle x y rangle sigma 2 mathbb e approx langle e x 2 2 sigma 2 varphi x e y 2 2 sigma 2 varphi y rangle consequently the one headed attention with one query can be written as attention q k v softmax q k t d k v φ q t i e k i 2 2 σ 2 φ k i v i t φ q t i e k i 2 2 σ 2 φ k i displaystyle text attention q k v text softmax left frac qk mathrm t sqrt d_ k right v approx frac varphi q t sum _ i e k_ i 2 2 sigma 2 varphi k_ i v_ i t varphi q t sum _ i e k_ i 2 2 sigma 2 varphi k_ i where σ d k 1 4 displaystyle sigma d_ k 1 4 similarly for multiple queries and for multiheaded attention this approximation can be computed in linear time as we can compute the matrix φ k i v i t displaystyle varphi k_ i v_ i t first then multiply it with the query in essence we have managed to obtain a more precise version of attention q k v softmax q k t d k v q k t v d k displaystyle text attention q k v text softmax left frac qk mathrm t sqrt d_ k right v approx q k t v sqrt d_ k performer 2022 uses the same random feature attention but w 1 w d displaystyle w_ 1 w_ d are first independently sampled from the normal distribution n 0 σ 2 i displaystyle n 0 sigma 2 i then they are gram schmidt processed multimodality transformers can also be used adapted for modalities input or output beyond just text usually by finding a way to tokenize the modality multimodal models can either be trained from scratch or by finetuning a 2022 study found that transformers pretrained only on natural language can be finetuned on only 0 03 of parameters and become competitive with lstms on a variety of logical and visual tasks demonstrating transfer learning the llava was a vision language model composed of a language model vicuna 13b and a vision model vit l 14 connected by a linear layer only the linear layer is finetuned vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches turning them into vectors and treating them like tokens in a standard transformer conformer and later whisper follow the same pattern for speech recognition first turning the speech signal into a spectrogram which is then treated like an image i e broken down into a series of patches turned into vectors and treated like tokens in a standard transformer perceivers are a variant of transformers designed for multimodality for image generation notable architectures are dall e 1 2021 parti 2022 phenaki 2023 and muse 2023 unlike later models dall e is not a diffusion model instead it uses a decoder only transformer that autoregressively generates a text followed by the token representation of an image which is then converted by a variational autoencoder to an image parti is an encoder decoder transformer where the encoder processes a text prompt and the decoder generates a token representation of an image muse is an encoder only transformer that is trained to predict masked image tokens from unmasked image tokens during generation all input tokens are masked and the highest confidence predictions are included for the next iteration until all tokens are predicted phenaki is a text to video model it is a bidirectional masked transformer conditioned on pre computed text tokens the generated tokens are then decoded to a video applications the transformer has had great success in natural language processing nlp many large language models such as gpt 2 gpt 3 gpt 4 albertagpt claude bert xlnet roberta and chatgpt demonstrate the ability of transformers to perform a wide variety of nlp related subtasks and their related real world or practical applications including machine translation time series prediction document summarization document generation named entity recognition ner writing computer code based on requirements expressed in natural language speech to text beyond traditional nlp the transformer architecture has had success in other applications such as biological sequence analysis video understanding protein folding such as alphafold evaluating chess board positions using static evaluation alone that is with no minimax search transformer achieved an elo of 2895 putting it at grandmaster level see also seq2seq family of machine learning approaches perceiver variant of transformer designed for multimodal data vision transformer machine learning model for vision processing large language model comparatively large scale natural language processing systems bert language model series of language models developed by google ai generative pre trained transformer type of large language model t5 language model series of large language models developed by google ai mean field type transformers mftt notes references further reading ",
            "total_words": 9121,
            "unique_words_percentage": 15.85352483280342,
            "stopwords_percentage": 34.29448525380989
        }
    ],
    "Data Science": [
        {
            "title": "Data science",
            "link": "https://en.wikipedia.org/wiki/Data_science",
            "content": "data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processing scientific visualization algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy structured or unstructured data data science also integrates domain knowledge from the underlying application domain e g natural sciences information technology and medicine data science is multifaceted and can be described as a science a research paradigm a research method a discipline a workflow and a profession data science is a concept to unify statistics data analysis informatics and their related methods to understand and analyze actual phenomena with data it uses techniques and theories drawn from many fields within the context of mathematics statistics computer science information science and domain knowledge however data science is different from computer science and information science turing award winner jim gray imagined data science as a fourth paradigm of science empirical theoretical computational and now data driven and asserted that everything about science is changing because of the impact of information technology and the data deluge a data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data foundations data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains the field encompasses preparing data for analysis formulating data science problems analyzing data developing data driven solutions and presenting findings to inform high level decisions in a broad range of application domains as such it incorporates skills from computer science statistics information science mathematics data visualization information visualization data sonification data integration graphic design complex systems communication and business statistician nathan yau drawing on ben fry also links data science to human computer interaction users should be able to intuitively control and explore data in 2015 the american statistical association identified database management statistics and machine learning and distributed and parallel systems as the three emerging foundational professional communities relationship to statistics many statisticians including nate silver have argued that data science is not a new field but rather another name for statistics others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data vasant dhar writes that statistics emphasizes quantitative data and description in contrast data science deals with quantitative and qualitative data e g from images text sensors transactions customer information etc and emphasizes prediction and action andrew gelman of columbia university has described statistics as a non essential part of data science stanford professor david donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program he describes data science as an applied field growing out of traditional statistics etymology early usage in 1962 john tukey described a field he called data analysis which resembles modern data science in 1985 in a lecture given to the chinese academy of sciences in beijing c f jeff wu used the term data science for the first time as an alternative name for statistics later attendees at a 1992 statistics symposium at the university of montpellier ii acknowledged the emergence of a new discipline focused on data of various origins and forms combining established concepts and principles of statistics and data analysis with computing the term data science has been traced back to 1974 when peter naur proposed it as an alternative name to computer science in 1996 the international federation of classification societies became the first conference to specifically feature data science as a topic however the definition was still in flux after the 1985 lecture at the chinese academy of sciences in beijing in 1997 c f jeff wu again suggested that statistics should be renamed data science he reasoned that a new name would help statistics shed inaccurate stereotypes such as being synonymous with accounting or limited to describing data in 1998 hayashi chikio argued for data science as a new interdisciplinary concept with three aspects data design collection and analysis during the 1990s popular terms for the process of finding patterns in datasets which were increasingly large included knowledge discovery and data mining modern usage in 2012 technologists thomas h davenport and dj patil declared data scientist the sexiest job of the 21st century a catchphrase that was picked up even by major city newspapers like the new york times and the boston globe a decade later they reaffirmed it stating that the job is more in demand than ever with employers the modern conception of data science as an independent discipline is sometimes attributed to william s cleveland in a 2001 paper he advocated an expansion of statistics beyond theory into technical areas because this would significantly change the field it warranted a new name data science became more widely used in the next few years in 2002 the committee on data for science and technology launched the data science journal in 2003 columbia university launched the journal of data science in 2014 the american statistical association s section on statistical learning and data mining changed its name to the section on statistical learning and data science reflecting the ascendant popularity of data science the professional title of data scientist has been attributed to dj patil and jeff hammerbacher in 2008 though it was used by the national science board in their 2005 report long lived digital data collections enabling research and education in the 21st century it referred broadly to any key role in managing a digital data collection there is still no consensus on the definition of data science and it is considered by some to be a buzzword big data is a related marketing term data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations data science and data analysis data science and data analysis are both important disciplines in the field of data management and analysis but they differ in several key ways while both fields involve working with data data science is more of an interdisciplinary field that involves the application of statistical computational and machine learning methods to extract insights from data and make predictions while data analysis is more focused on the examination and interpretation of data to identify patterns and trends data analysis typically involves working with smaller structured datasets to answer specific questions or solve specific problems this can involve tasks such as data cleaning data visualization and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data for example a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies data science on the other hand is a more complex and iterative process that involves working with larger more complex datasets that often require advanced computational and statistical methods to analyze data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data driven decisions in addition to statistical analysis data science often involves tasks such as data preprocessing feature engineering and model selection for instance a data scientist might develop a recommendation system for an e commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences while data analysis focuses on extracting insights from existing data data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions data scientists are often responsible for collecting and cleaning data selecting appropriate analytical techniques and deploying models in real world scenarios they work at the intersection of mathematics computer science and domain expertise to solve complex problems and uncover hidden patterns in large datasets despite these differences data science and data analysis are closely related fields and often require similar skill sets both fields require a solid foundation in statistics programming and data visualization as well as the ability to communicate findings effectively to both technical and non technical audiences both fields benefit from critical thinking and domain knowledge as understanding the context and nuances of the data is essential for accurate analysis and modeling in summary data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis data analysis focuses on extracting insights and drawing conclusions from structured data while data science involves a more comprehensive approach that combines statistical analysis computational methods and machine learning to extract insights build predictive models and drive data driven decision making both fields use data to understand patterns make informed decisions and solve complex problems across various domains data science as an academic discipline as illustrated in the previous sections there is substantially some considerable differences between data science data analysis and statistics consequently just like statistics grew into an independent field from applied mathematics similarly data science has emerged as a independent field and has gained traction over the recent years the unique demand for professional skills on computerized data analysis skills has exploded due to the increasing amounts of data emanating from a variety of independent sources whereas some of these highly sought skills can be provided by statisticians the lack of high algorithmic writing skills makes them less preferred than trained data scientists who provide unique expertise on skills such as nosql apache hadoop cloud computing platforms and use of complex networks this paradigm shift has seen various institution craft academic programmes to prepare skilled labor for the market some of the institutions offering degree programmes in data science include stanford university harvard university university of oxford eth zurich meru university among many others cloud computing for data science cloud computing can offer access to large amounts of computational power and storage in big data where volumes of information are continually generated and processed these platforms can be used to handle complex and resource intensive analytical tasks some distributed computing frameworks are designed to handle big data workloads these frameworks can enable data scientists to process and analyze large datasets in parallel which can reducing processing times ethical consideration in data science data science involve collecting processing and analyzing data which often including personal and sensitive information ethical concerns include potential privacy violations bias perpetuation and negative societal impacts machine learning models can amplify existing biases present in training data leading to discriminatory or unfair outcomes see also python programming language r programming language data engineering big data machine learning references ",
            "total_words": 1799,
            "unique_words_percentage": 36.68704836020011,
            "stopwords_percentage": 33.407448582545854
        },
        {
            "title": "Data (computer science)",
            "link": "https://en.wikipedia.org/wiki/Data_(computer_science)",
            "content": "in computer science data treated as singular plural or as a mass noun is any sequence of one or more symbols datum is a single symbol of data data requires interpretation to become information digital data is data that is represented using the binary number system of ones 1 and zeros 0 instead of analog representation in modern post 1960 computer systems all data is digital data exists in three states data at rest data in transit and data in use data within a computer in most cases moves as parallel data data moving to or from a computer in most cases moves as serial data data sourced from an analog device such as a temperature sensor may be converted to digital using an analog to digital converter data representing quantities characters or symbols on which operations are performed by a computer are stored and recorded on magnetic optical electronic or mechanical recording media and transmitted in the form of digital electrical or optical signals data pass in and out of computers via peripheral devices physical computer memory elements consist of an address and a byte word of data storage digital data are often stored in relational databases like tables or sql databases and can generally be represented as abstract key value pairs data can be organized in many different types of data structures including arrays graphs and objects data structures can store data of many different types including numbers strings and even other data structures characteristics metadata helps translate data to information metadata is data about the data metadata may be implied specified or given data relating to physical events or processes will have a temporal component this temporal component may be implied this is the case when a device such as a temperature logger receives data from a temperature sensor when the temperature is received it is assumed that the data has a temporal reference of now so the device records the date time and temperature together when the data logger communicates temperatures it must also report the date and time as metadata for each temperature reading fundamentally computers follow a sequence of instructions they are given in the form of data a set of instructions to perform a given task or tasks is called a program a program is data in the form of coded instructions to control the operation of a computer or other machine in the nominal case the program as executed by the computer will consist of machine code the elements of storage manipulated by the program but not actually executed by the central processing unit cpu are also data at its most essential a single datum is a value stored at a specific location therefore it is possible for computer programs to operate on other computer programs by manipulating their programmatic data to store data bytes in a file they have to be serialized in a file format typically programs are stored in special file types different from those used for other data executable files contain programs all other files are also data files however executable files may also contain data used by the program which is built into the program in particular some executable files have a data segment which nominally contains constants and initial values for variables both of which can be considered data the line between program and data can become blurry an interpreter for example is a program the input data to an interpreter is itself a program just not one expressed in native machine language in many cases the interpreted program will be a human readable text file which is manipulated with a text editor program metaprogramming similarly involves programs manipulating other programs as data programs like compilers linkers debuggers program updaters virus scanners and such use other programs as their data for example a user might first instruct the operating system to load a word processor program from one file and then use the running program to open and edit a document stored in another file in this example the document would be considered data if the word processor also features a spell checker then the dictionary word list for the spell checker would also be considered data the algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language in an alternate usage binary files which are not human readable are sometimes called data as distinguished from human readable text the total amount of digital data in 2007 was estimated to be 281 billion gigabytes 281 exabytes data keys and values structures and persistence keys in data provide the context for values regardless of the structure of data there is always a key component present keys in data and data structures are essential for giving meaning to data values without a key that is directly or indirectly associated with a value or collection of values in a structure the values become meaningless and cease to be data that is to say there has to be a key component linked to a value component in order for it to be considered data data can be represented in computers in multiple ways as per the following examples ram random access memory ram holds data that the cpu has direct access to a cpu may only manipulate data within its processor registers or memory this is as opposed to data storage where the cpu must direct the transfer of data between the storage device disk tape and memory ram is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation the processor may operate on any location in memory at any time in any order in ram the smallest element of data is the binary bit the capabilities and limitations of accessing ram are processor specific in general main memory is arranged as an array of locations beginning at address 0 hexadecimal 0 each location can store usually 8 or 32 bits depending on the computer architecture keys data keys need not be a direct hardware address in memory indirect abstract and logical keys codes can be stored in association with values to form a data structure data structures have predetermined offsets or links or paths from the start of the structure in which data values are stored therefore the data key consists of the key to the structure plus the offset or links or paths into the structure when such a structure is repeated storing variations of the data values and the data keys within the same repeating structure the result can be considered to resemble a table in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table in such an organization of data the data key is usually a value in one or a composite of the values in several of the columns organised recurring data structures the tabular view of repeating data structures is only one of many possibilities repeating data structures can be organised hierarchically such that nodes are linked to each other in a cascade of parent child relationships values and potentially more complex data structures are linked to the nodes thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes this representation can be thought of as an inverted tree modern computer operating system file systems are a common example and xml is another sorted or ordered data data has some inherent features when it is sorted on a key all the values for subsets of the key appear together when passing sequentially through groups of the data with the same key or a subset of the key changes this is referred to in data processing circles as a break or a control break it particularly facilitates the aggregation of data values on subsets of a key peripheral storage until the advent of bulk non volatile memory like flash persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives these devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size in this case the seek location on the media is the data key and the blocks are the data values early used raw disk data file systems or disc operating systems reserved contiguous blocks on the disc drive for data files in those systems the files could be filled up running out of data space before all the data had been written to them thus much unused data space was reserved unproductively to ensure adequate free space for each file later file systems introduced partitions they reserved blocks of disc data space for partitions and used the allocated blocks more economically by dynamically assigning blocks of a partition to a file as needed to achieve this the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table though this made better use of the disc data space it resulted in fragmentation of files across the disc and a concomitant performance overhead due additional seek time to read the data modern file systems reorganize fragmented files dynamically to optimize file access times further developments in file systems resulted in virtualization of disc drives i e where a logical drive can be defined as partitions from a number of physical drives indexed data retrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially indexes are a way to copy out keys and location addresses from data structures in files tables and data sets then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data in order to do this the key of the subset of data to be retrieved must be known before retrieval begins the most popular indexes are the b tree and the dynamic hash key indexing methods indexing is overhead for filing and retrieving data there are other ways of organizing indexes e g sorting the keys and using a binary search algorithm abstraction and indirection object oriented programming uses two basic concepts for understanding data and software the taxonomic rank structure of classes which is an example of a hierarchical data structure and at run time the creation of references to in memory data structures of objects that have been instantiated from a class library it is only after instantiation that an object of a specified class exists after an object s reference is cleared the object also ceases to exist the memory locations where the object s data was stored are garbage and are reclassified as unused memory available for reuse database data the advent of databases introduced a further layer of abstraction for persistent data storage databases use metadata and a structured query language protocol between client and server systems communicating over a computer network using a two phase commit logging system to ensure transactional completeness when saving data parallel distributed data processing modern scalable and high performance data persistence technologies such as apache hadoop rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network in such systems the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data either directly or indirectly this enables the differentiation between two identical sets of data each being processed on a different computer at the same time see also references ",
            "total_words": 1958,
            "unique_words_percentage": 30.643513789581206,
            "stopwords_percentage": 41.67517875383044
        },
        {
            "title": "Data",
            "link": "https://en.wikipedia.org/wiki/Data",
            "content": "data day tə us also dat ə are a collection of discrete or continuous values that convey information describing the quantity quality fact statistics other basic units of meaning or simply sequences of symbols that may be further interpreted formally a datum is an individual value in a collection of data data are usually organized into structures such as tables that provide additional context and meaning and may themselves be used as data in larger structures data may be used as variables in a computational process data may represent abstract ideas or concrete measurements data are commonly used in scientific research economics and virtually every other form of human organizational activity examples of data sets include price indices such as the consumer price index unemployment rates literacy rates and census data in this context data represent the raw facts and figures from which useful information can be extracted data are collected using techniques such as measurement observation query or analysis and are typically represented as numbers or characters that may be further processed field data are data that are collected in an uncontrolled in situ environment experimental data are data that are generated in the course of a controlled scientific experiment data are analyzed using techniques such as calculation reasoning discussion presentation visualization or other forms of post analysis prior to analysis raw data or unprocessed data is typically cleaned outliers are removed and obvious instrument or data entry errors are corrected data can be seen as the smallest units of factual information that can be used as a basis for calculation reasoning or discussion data can range from abstract ideas to concrete measurements including but not limited to statistics thematically connected data presented in some relevant context can be viewed as information contextually connected pieces of information can then be described as data insights or intelligence the stock of insights and intelligence that accumulate over time resulting from the synthesis of data into information can then be described as knowledge data has been described as the new oil of the digital economy data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing advances in computing technologies have led to the advent of big data which usually refers to very large quantities of data usually at the petabyte scale using traditional data analysis methods and computing working with such large and growing datasets is difficult even impossible theoretically speaking infinite data would yield infinite information which would render extracting insights or intelligence impossible in response the relatively new field of data science uses machine learning and other artificial intelligence ai methods that allow for efficient applications of analytic methods to big data etymology and terminology the latin word data is the plural of datum thing given and the neuter past participle of dare to give the first english use of the word data is from the 1640s the word data was first used to mean transmissible and storable computer information in 1946 the expression data processing was first used in 1954 when data is used more generally as a synonym for information it is treated as a mass noun in singular form this usage is common in everyday language and in technical and scientific fields such as software development and computer science one example of this usage is the term big data when used more specifically to refer to the processing and analysis of sets of data the term retains its plural form this usage is common in the natural sciences life sciences social sciences software development and computer science and grew in popularity in the 20th and 21st centuries some style guides do not recognize the different meanings of the term and simply recommend the form that best suits the target audience of the guide for example apa style as of the 7th edition requires data to be treated as a plural form meaning data information knowledge and wisdom are closely related concepts but each has its role concerning the other and each term has its meaning according to a common view data is collected and analyzed data only becomes information suitable for making decisions once it has been analyzed in some fashion one can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person the amount of information contained in a data stream may be characterized by its shannon entropy knowledge is the awareness of its environment that some entity possesses whereas data merely communicates that knowledge for example the entry in a database specifying the height of mount everest is a datum that communicates a precisely measured value this measurement may be included in a book along with other data on mount everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it awareness of the characteristics represented by this data is knowledge data are often assumed to be the least abstract concept information the next least and knowledge the most abstract in this view data becomes information by interpretation e g the height of mount everest is generally considered data a book on mount everest geological characteristics may be considered information and a climber s guidebook containing practical information on the best way to reach mount everest s peak may be considered knowledge information bears a diversity of meanings that range from everyday usage to technical use this view however has also been argued to reverse how data emerges from information and information from knowledge generally speaking the concept of information is closely related to notions of constraint communication control data form instruction knowledge meaning mental stimulus pattern perception and representation beynon davies uses the concept of a sign to differentiate between data and information data is a series of symbols while information occurs when the symbols are used to refer to something before the development of computing devices and machines people had to manually collect data and impose patterns on it with the development of computing devices and machines these devices can also collect data in the 2010s computers were widely used in many fields to collect data and sort or process it in disciplines ranging from marketing analysis of social service usage by citizens to scientific research these patterns in the data are seen as information that can be used to enhance knowledge these patterns may be interpreted as truth though truth can be a subjective concept and may be authorized as aesthetic and ethical criteria in some disciplines or cultures events that leave behind perceivable physical or virtual remains can be traced back through data marks are no longer considered data once the link between the mark and observation is broken mechanical computing devices are classified according to how they represent data an analog computer represents a datum as a voltage distance position or other physical quantity a digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet the most common digital computers use a binary alphabet that is an alphabet of two characters typically denoted 0 and 1 more familiar representations such as numbers or letters are then constructed from the binary alphabet some special forms of data are distinguished a computer program is a collection of data that can be interpreted as instructions most computer languages make a distinction between programs and the other data on which programs operate but in some languages notably lisp and similar languages programs are essentially indistinguishable from other data it is also useful to distinguish metadata that is a description of other data a similar yet earlier term for metadata is ancillary data the prototypical example of metadata is the library catalog which is a description of the contents of books data documents whenever data needs to be registered data exists in the form of a data document kinds of data documents include data repository data study data set software data paper database data handbook data journal some of these data documents data repositories data studies data sets and software are indexed in data citation indexes while data papers are indexed in traditional bibliographic databases e g science citation index data collection gathering data can be accomplished through a primary source the researcher is the first person to obtain the data or a secondary source the researcher obtains the data that has already been collected by other sources such as data disseminated in a scientific journal data analysis methodologies vary and include data triangulation and data percolation the latter offers an articulate method of collecting classifying and analyzing data using five possible angles of analysis at least three to maximize the research s objectivity and permit an understanding of the phenomena under investigation as complete as possible qualitative and quantitative methods literature reviews including scholarly articles interviews with experts and computer simulation the data is thereafter percolated using a series of pre determined steps so as to extract the most relevant information data longevity and accessibility an important field in computer science technology and library science is the longevity of data scientific research generates huge amounts of data especially in genomics and astronomy but also in the medical sciences e g in medical imaging in the past scientific data has been published in papers and books stored in libraries but more recently practically all data is stored on hard drives or optical discs however in contrast to paper these storage devices may become unreadable after a few decades scientific publishers and libraries have been struggling with this problem for a few decades and there is still no satisfactory solution for the long term storage of data over centuries or even for eternity data accessibility another problem is that much scientific data is never published or deposited in data repositories such as databases in a recent survey data was requested from 516 studies that were published between 2 and 22 years earlier but less than one out of five of these studies were able or willing to provide the requested data overall the likelihood of retrieving data dropped by 17 each year after publication similarly a survey of 100 datasets in dryad found that more than half lacked the details to reproduce the research results from these studies this shows the dire situation of access to scientific data that is not published or does not have enough details to be reproduced a solution to the problem of reproducibility is the attempt to require fair data that is data that is findable accessible interoperable and reusable data that fulfills these requirements can be used in subsequent research and thus advances science and technology in other fields although data is also increasingly used in other fields it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as given peter checkland introduced the term capta from the latin capere to take to distinguish between an immense number of possible data and a sub set of them to which attention is oriented johanna drucker has argued that since the humanities affirm knowledge production as situated partial and constitutive using data may introduce assumptions that are counterproductive for example that phenomena are discrete or are observer independent the term capta which emphasizes the act of observation as constitutive is offered as an alternative to data for visual representations in the humanities the term data driven is a neologism applied to an activity which is primarily compelled by data over all other factors data driven applications include data driven programming and data driven journalism see also references external links data is a singular noun a detailed assessment ",
            "total_words": 1967,
            "unique_words_percentage": 35.43467208947636,
            "stopwords_percentage": 41.73868835790544
        },
        {
            "title": "Data type",
            "link": "https://en.wikipedia.org/wiki/Data_type",
            "content": "in computer science and computer programming a data type or simply type is a collection or grouping of data values usually specified by a set of possible values a set of allowed operations on these values and or a representation of these values as machine types a data type specification in a program constrains the possible values that an expression such as a variable or a function call might take on literal data it tells the compiler or interpreter how the programmer intends to use the data most programming languages support basic data types of integer numbers of varying sizes floating point numbers which approximate real numbers characters and booleans concept a data type may be specified for many reasons similarity convenience or to focus the attention it is frequently a matter of good organization that aids the understanding of complex definitions almost all programming languages explicitly include the notion of data type though the possible data types are often restricted by considerations of simplicity computability or regularity an explicit data type declaration typically allows the compiler to choose an efficient machine representation but the conceptual organization offered by data types should not be discounted different languages may use different data types or similar types with different semantics for example in the python programming language int represents an arbitrary precision integer which has the traditional numeric operations such as addition subtraction and multiplication however in the java programming language the type int represents the set of 32 bit integers ranging in value from 2 147 483 648 to 2 147 483 647 with arithmetic operations that wrap on overflow in rust this 32 bit integer type is denoted i32 and panics on overflow in debug mode most programming languages also allow the programmer to define additional data types usually by combining multiple elements of other types and defining the valid operations of the new data type for example a programmer might create a new data type named complex number that would include real and imaginary parts or a color data type represented by three bytes denoting the amounts each of red green and blue and a string representing the color s name data types are used within type systems which offer various ways of defining implementing and using them in a type system a data type represents a constraint placed upon the interpretation of data describing representation interpretation and structure of values or objects stored in computer memory the type system uses data type information to check correctness of computer programs that access or manipulate the data a compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value in many c compilers the float data type for example is represented in 32 bits in accord with the ieee specification for single precision floating point numbers they will thus use floating point specific microprocessor operations on those values floating point addition multiplication etc most data types in statistics have comparable types in computer programming and vice versa as shown in the following table definition parnas shore weiss 1976 identified five definitions of a type that were used sometimes implicitly in the literature syntactic a type is a purely syntactic label associated with a variable when it is declared although useful for advanced type systems such as substructural type systems such definitions provide no intuitive meaning of the types representation a type is defined in terms of a composition of more primitive types often machine types representation and behaviour a type is defined as its representation and a set of operators manipulating these representations value space a type is a set of possible values which a variable can possess such definitions make it possible to speak about disjoint unions or cartesian products of types value space and behaviour a type is a set of values which a variable can possess and a set of functions that one can apply to these values the definition in terms of a representation was often done in imperative languages such as algol and pascal while the definition in terms of a value space and behaviour was used in higher level languages such as simula and clu types including behavior align more closely with object oriented models whereas a structured programming model would tend to not include code and are called plain old data structures classification data types may be categorized according to several factors primitive data types or built in data types are types that are built in to a language implementation user defined data types are non primitive types for example java s numeric types are primitive while classes are user defined a value of an atomic type is a single data item that cannot be broken into component parts a value of a composite type or aggregate type is a collection of data items that can be accessed individually for example an integer is generally considered atomic although it consists of a sequence of bits while an array of integers is certainly composite basic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements generated data types or derived data types are specified and partly defined in terms of other data types all basic types are atomic for example integers are a basic type defined in mathematics while an array of integers is the result of applying an array type generator to the integer type the terminology varies in the literature primitive built in basic atomic and fundamental may be used interchangeably examples machine data types all data in computers based on digital electronics is represented as bits alternatives 0 and 1 on the lowest level the smallest addressable unit of data is usually a group of bits called a byte usually an octet which is 8 bits the unit processed by machine code instructions is called a word as of 2011 typically 32 or 64 bits machine data types expose or make available fine grained control over hardware but this can also expose implementation details that make code less portable hence machine types are mainly used in systems programming or low level programming languages in higher level languages most data types are abstracted in that they do not have a language defined machine representation the c programming language for instance supplies types such as booleans integers floating point numbers etc but the precise bit representations of these types are implementation defined the only c type with a precise machine representation is the char type that represents a byte boolean type the boolean type represents the values true and false although only two values are possible they are more often represented as a byte or word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit many programming languages do not have an explicit boolean type instead using an integer type and interpreting for instance 0 as false and other values as true boolean data refers to the logical structure of how the language is interpreted to the machine language in this case a boolean 0 refers to the logic false true is always a non zero especially a one which is known as boolean 1 numeric types almost all programming languages supply one or more integer data types they may either supply a small number of predefined subtypes restricted to certain ranges such as short and long and their corresponding unsigned variants in c c or allow users to freely define subranges such as 1 12 e g pascal ada if a corresponding native type does not exist on the target platform the compiler will break them down into code using types that do exist for instance if a 32 bit integer is requested on a 16 bit platform the compiler will tacitly treat it as an array of two 16 bit integers floating point data types represent certain fractional values rational numbers mathematically although they have predefined limits on both their maximum values and their precision they are sometimes misleadingly called reals evocative of mathematical real numbers they are typically stored internally in the form a 2b where a and b are integers but displayed in familiar decimal form fixed point data types are convenient for representing monetary values they are often implemented internally as integers leading to predefined limits for independence from architecture details a bignum or arbitrary precision numeric type might be supplied this represents an integer or rational to a precision limited only by the available memory and computational resources on the system bignum implementations of arithmetic operations on machine sized values are significantly slower than the corresponding machine operations enumerations the enumerated type has distinct values which can be compared and assigned but which do not necessarily have any particular concrete representation in the computer s memory compilers and interpreters can represent them arbitrarily for example the four suits in a deck of playing cards may be four enumerators named club diamond heart spade belonging to an enumerated type named suit if a variable v is declared having suit as its data type one can assign any of those four values to it some implementations allow programmers to assign integer values to the enumeration values or even treat them as type equivalent to integers string and text types strings are a sequence of characters used to store words or plain text most often textual markup languages representing formatted text characters may be a letter of some alphabet a digit a blank space a punctuation mark etc characters are drawn from a character set such as ascii or unicode character and string types can have different subtypes according to the character encoding the original 7 bit wide ascii was found to be limited and superseded by 8 16 and 32 bit sets which can encode a wide variety of non latin alphabets such as hebrew and chinese and other symbols strings may be of either variable length or fixed length and some programming languages have both types they may also be subtyped by their maximum size since most character sets include the digits it is possible to have a numeric string such as 1234 these numeric strings are usually considered distinct from numeric values such as 1234 although some languages automatically convert between them union types a union type definition will specify which of a number of permitted subtypes may be stored in its instances e g float or long integer in contrast with a record which could be defined to contain a float and an integer a union may only contain one subtype at a time a tagged union also called a variant variant record discriminated union or disjoint union contains an additional field indicating its current type for enhanced type safety algebraic data types an algebraic data type adt is a possibly recursive sum type of product types a value of an adt consists of a constructor tag together with zero or more field values with the number and type of the field values fixed by the constructor the set of all possible values of an adt is the set theoretic disjoint union sum of the sets of all possible values of its variants product of fields values of algebraic types are analyzed with pattern matching which identifies a value s constructor and extracts the fields it contains if there is only one constructor then the adt corresponds to a product type similar to a tuple or record a constructor with no fields corresponds to the empty product unit type if all constructors have no fields then the adt corresponds to an enumerated type one common adt is the option type defined in haskell as data maybe a nothing just a data structures some types are very useful for storing and retrieving data and are called data structures common data structures include an array also called vector list or sequence stores a number of elements and provides random access to individual elements the elements of an array are typically but not in all contexts required to be of the same type arrays may be fixed length or expandable indices into an array are typically required to be integers if not one may stress this relaxation by speaking about an associative array from a specific range if not all indices in that range correspond to elements it may be a sparse array record also called tuple or struct records are among the simplest data structures a record is a value that contains other values typically in fixed number and sequence and typically indexed by names the elements of records are usually called fields or members an object contains a number of data fields like a record and also offers a number of subroutines for accessing or modifying them called methods the singly linked list which can be used to implement a queue and is defined in haskell as the adt data list a nil cons a list a and the binary tree which allows fast searching and can be defined in haskell as the adt data btree a nil node btree a a btree a abstract data types an abstract data type is a data type that does not specify the concrete representation of the data instead a formal specification based on the data type s operations is used to describe it any implementation of a specification must fulfill the rules given for example a stack has push pop operations that follow a last in first out rule and can be concretely implemented using either a list or an array abstract data types are used in formal semantics and program verification and less strictly in design pointers and references the main non composite derived type is the pointer a data type whose value refers directly to or points to another value stored elsewhere in the computer memory using its address it is a primitive kind of reference in everyday terms a page number in a book could be considered a piece of data that refers to another one pointers are often stored in a format similar to an integer however attempting to dereference or look up a pointer whose value was never a valid memory address would cause a program to crash to ameliorate this potential problem a pointer type is typically considered distinct from the corresponding integer type even if the underlying representation is the same function types functional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions some multi paradigm languages such as javascript also have mechanisms for treating functions as data most contemporary type systems go beyond javascript s simple type function object and have a family of function types differentiated by argument and return types such as the type int bool denoting functions taking an integer and returning a boolean in c a function is not a first class data type but function pointers can be manipulated by the program java and c originally did not have function values but have added them in c 11 and java 8 type constructors a type constructor builds new types from old ones and can be thought of as an operator taking zero or more types as arguments and producing a type product types function types power types and list types can be made into type constructors quantified types universally quantified and existentially quantified types are based on predicate logic universal quantification is written as x f x displaystyle forall x f x or forall x f x and is the intersection over all types x of the body f x i e the value is of type f x for every x existential quantification written as x f x displaystyle exists x f x or exists x f x and is the union over all types x of the body f x i e the value is of type f x for some x in haskell universal quantification is commonly used but existential types must be encoded by transforming exists a f a to forall r forall a f a r r or a similar type refinement types a refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type for instance the type of natural numbers greater than 5 may be written as n n n 5 displaystyle n in mathbb n n 5 dependent types a dependent type is a type whose definition depends on a value two common examples of dependent types are dependent functions and dependent pairs the return type of a dependent function may depend on the value not just type of one of its arguments a dependent pair may have a second value of which the type depends on the first value intersection types an intersection type is a type containing those values that are members of two specified types for example in java the class boolean implements both the serializable and the comparable interfaces therefore an object of type boolean is a member of the type serializable comparable considering types as sets of values the intersection type σ τ displaystyle sigma cap tau is the set theoretic intersection of σ displaystyle sigma and τ displaystyle tau it is also possible to define a dependent intersection type denoted x σ τ displaystyle x sigma cap tau where the type τ displaystyle tau may depend on the term variable x displaystyle x meta types some programming languages represent the type information as data enabling type introspection and reflective programming reflection in contrast higher order type systems while allowing types to be constructed from other types and passed to functions as values typically avoid basing computational decisions on them convenience types for convenience high level languages and databases may supply ready made real world data types for instance times dates and monetary values currency these may be built in to the language or implemented as composite types in a library see also references further reading parnas david l shore john e weiss david 1976 abstract types defined as classes of variables proceedings of the 1976 conference on data abstraction definition and structure pp 149 154 doi 10 1145 800237 807133 s2cid 14448258 cardelli luca wegner peter december 1985 on understanding types data abstraction and polymorphism pdf acm computing surveys 17 4 471 523 citeseerx 10 1 1 117 695 doi 10 1145 6041 6042 issn 0360 0300 s2cid 2921816 archived pdf from the original on 2008 12 03 cleaveland j craig 1986 an introduction to data types addison wesley isbn 978 0201119404 external links media related to data types at wikimedia commons",
            "total_words": 3125,
            "unique_words_percentage": 27.328000000000003,
            "stopwords_percentage": 38.304
        },
        {
            "title": "Social data science",
            "link": "https://en.wikipedia.org/wiki/Social_data_science",
            "content": "social data science is an interdisciplinary field that addresses social science problems by applying or designing computational and digital methods as the name implies social data science is located primarily within the social science but it relies on technical advances in fields like data science network science and computer science the data in social data science is always about human beings and derives from social phenomena and it could be structured data e g surveys or unstructured data e g digital footprints the goal of social data science is to yield new knowledge about social networks human behavior cultural ideas and political ideologies a social data scientist combines cdomain knowledge and specialized theories from the social sciences with programming statistical and other data analysis skills methods social data science employs a wide range of quantitative both established methods in social science as well as new methods developed in computer science and interdisciplinary data science fields such as natural language processing nlp and network science social data science is closely related to computational social science but also sometimes includes qualitative data and mixed digital methods common social data science methods include quantitative methods machine learning deep learning social network analysis randomized controlled trials natural language processing nlp especially through text as data surveys qualitative methods interviewing observation ethnography content analysis discourse analysis mixed digital methods controversy mapping spatial analysis quali quantitative methods computational ethnography one of the pillars of social data science is in the combination of qualitative and quantitative data to analyze social phenomena and develop computationally grounded theories for example by using mixed methods to digitize qualitative data and analyzing it via computational methods or by qualitatively analyzing and interpreting quantitative data data social data scientists use both digitized data e g old books that have been digitized and natively digital data e g social media posts since such data often take the form of found data that were originally produced for other purposes commercial governance etc than research data scraping cleaning and other forms of preprocessing and data mining occupy a substantial part of a social data scientist s job sources of sds data include text data sensor data register data survey data geo location data observational data relations to other fields social sciences social data science is part of the social sciences along with established disciplines anthropology economics political science psychology and sociology and newer interdisciplinary fields like behavioral science criminology international relations and cognitive science as such its fundamental unit of study is social relations human behavior and cultural ideas which it investigates by using quantitative and or qualitative data and methods to develop test and improve fundamental theories concerning the nature of the human condition sds also differs from traditional social science in two ways first its primary object is digitized phenomena and data in the widest sense of this word ranging from digitized text corpora to the footprints gathered by digital platforms and sensors secondly more than simply applying existing quantitative and qualitative social science methods social data science seeks to develop and disrupt these via the import and integration of state of the art of data science techniques data science social data science is a form of data science in that it applies advanced computational methods and statistics to gain information and insights from data social data science researchers often make use of methods developed by data scientists such as data mining and machine learning which includes but is not limited to the extraction and processing of information from big data sources unlike the broader field of data science which involves any application and study involving the combination of computational and statistical methods social data science mainly concerns the scientific study of digital social data and or digital footprints from human behavior computational social science like computational social science social data science uses data science methods to solve social science problems this includes the reappropriation and refinement of methods developed by data scientists to better fit the questions and data of the social sciences as well as their specialized domain knowledge and theories unlike computational social science social data science also includes critical studies of how digital platforms and computational processes affect wider society and of how computational and non computational approaches integrate and combine digital methods while most social data science researchers are closely affiliated with or part of computational social science some qualitative oriented social data scientists are influenced by the fields of digital humanities and digital methods that emerged from science and technology studies sts like digital methods the aim is here to repurpose the methods of the medium to study digitally mediated society and to engage in an ongoing discussions about bias in science and society by bringing computational social science and digital methods into dialogue sds is also related to digital sociology and digital anthropology but to a higher degree aspires to augment qualitative data and digital methods with state of the art data science techniques history of the field the origin of term social data science coincided with the emergence of a number of research centers and degree programs in 2016 the copenhagen center for social data science sodas the first academic institution using the sds name was launched at the university of copenhagen the plan for an interdisciplinary center working at the intersection of the social and computational sciences was rooted in the copenhagen networks study from 2011 to 2016 by researchers from the technical university of denmark dtu and the university of copenhagen the university of oxford and the university of copenhagen were among the first research institutions to offer degree programmes in sds in 2018 the university of oxford launched the one year msc in social data science which was followed by the two year master s programme at the university of copenhagen in 2020 since then an increasing number of universities have begun to offer graduate programs or specializations in social data science social data science has emerged after the increasing availability of digitized social data sometimes referred to as big data and the ability to apply computational methods to this data at a low cost which has offered novel opportunities to address questions about social phenomena and human behavior while the origin of social data can be traced back to 1890s when some 15 million individual records were processed by the us census in the form of punch cards the social data boom in the 21th century is a direct consequence of the increasing availability of consumer data resulting from the advent of e commerce subsequent waves of availability of unstructured social data include amazon com review system and wikipedia and more recently social media which has played a key role in the emergence of the digital attention economy and big tech criticism and debates data scientists have played a vital role in the data revolution both during the original tech optimist phase where big data and the internet was seen as the solution to many societal and scientific problems and as participants in the tech lash that followed in its wake as result of among other things the facebook cambridge analytica data scandal social data science researchers and research projects have been especially impactful in debates and criticism revolving around surveillance capitalism digital disinformation algorithmic bias the replication and validity crisis on the social sciences ethics and privacy data governance impact and examples social data science research is typically published in multidisciplinary journals including top general journals science nature and pnas as well as notable specialized journals such as nature human behaviour nature computational science the journal of computational social science big data and society science advances nature communications scientific reports plos one in addition social data science research is published in the top social science field journals including american sociological review psychological science american economic review current anthropology education and research institutions there are multiple specific definitions of social data science but several institutions around the world currently offer degree and research programs under the rubric of social data science education m sc in social data science university of copenhagen msc in social data science university of oxford msc in social and economic data science seds university of konstanz bsc in social data science university of hong kong p grad dip in social data science university of dublin msc applied social data science the london school of economics master of science in social data science central european university msc social data science university of essex msc in techno anthropology university of aalborg msc social data science university college dublin bsc in social data science witten herdecke university quantitative analysis and social data science qass ku leuven human and social data science msc university of sussex research copenhagen center for social data science sodas university of copenhagen center for social data science university of helsinki social data science lab cardiff university soda laboratories monash university mannheim center for data science university of mannheim social behavioral data science centre sobe dsc university of amsterdam social data science alan turing institute social data science center university of maryland centre for social data analytics auckland university of technology masshine aalborg university professions and industry social data scientists are in high demand across society specifically for employers valuing interdisciplinary skills and can be found working as 1 industry researchers typical workplaces governments companies and corporations independent research institutes foundations ngos typical titles researcher data manager data steward data scientist data engineer consultant manager director partner politicians data analyst software developer bi ux ui researcher 2 academic researchers ph d students researchers postdocs professors 3 entrepreneurs start your own business using social data science methods to solve real world social problems typical titles cto ceo chief data scientist sub branches social data science is still a new field with developing branches broadly speaking the field can be divided into a range of method based sub fields method based sub fields network science network analysis is often utilized to visualize or study network dynamics in social data science studies this includes for instance social media networks mixed digital methods in computer assisted qualitative analysis the researcher often utilizes computational methods such as natural language processing techniques or topic modelling to explore a corpus of text such as parliamentary speeches or twitter data machine learning for causal inference the social sciences are often interested in finding causal relationships between variables this is of special interest to social data science where multiple fields of research try to establish appropriate policy responses to contemporary societal issues often drawing from research from judea pearls directed acyclical graph approach and the neymann rubin causal model to inform whether there exists a causal relationship between two or more variables furthermore incorporating machine learning into causal inference is of great interest natural language processing applied natural language processing is the adaptation and repurposing of methods from natural language processing and the application of these methods to questions of social behavior themes algorithmic bias and fairness considering how algorithms play a still larger role in humans everyday life the study of fairness in this context has grown as a field especially whether miniorieties are negatively or positively impacted by these algorithms polarization and misinformation many scholars use enormous amounts of granular data generated by social media and political agents to study social contagion the spread of misinformation and disinformation these studies often use text or social media interactions to explore how politicians and or the public behave and interact in the digital and physical arena climate social data science the intersection between climate science and digital behavioral data this includes climate activism on social media and using digital trace data to investigate how people and societies are impacted by rising temperatures cite rising temperature erode human sleep globally references ",
            "total_words": 1964,
            "unique_words_percentage": 32.9938900203666,
            "stopwords_percentage": 30.14256619144603
        },
        {
            "title": "Data analysis",
            "link": "https://en.wikipedia.org/wiki/Data_analysis",
            "content": "data analysis is the process of inspecting cleansing transforming and modeling data with the goal of discovering useful information informing conclusions and supporting decision making data analysis has multiple facets and approaches encompassing diverse techniques under a variety of names and is used in different business science and social science domains in today s business world data analysis plays a role in making decisions more scientific and helping businesses operate more effectively data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes while business intelligence covers data analysis that relies heavily on aggregation focusing mainly on business information in statistical applications data analysis can be divided into descriptive statistics exploratory data analysis eda and confirmatory data analysis cda eda focuses on discovering new features in the data while cda focuses on confirming or falsifying existing hypotheses predictive analytics focuses on the application of statistical models for predictive forecasting or classification while text analytics applies statistical linguistic and structural techniques to extract and classify information from textual sources a species of unstructured data all of the above are varieties of data analysis data integration is a precursor to data analysis and data analysis is closely linked to data visualization and data dissemination data analysis process analysis refers to dividing a whole into its separate components for individual examination data analysis is a process for obtaining raw data and subsequently converting it into information useful for decision making by users data is collected and analyzed to answer questions test hypotheses or disprove theories statistician john tukey defined data analysis in 1961 as procedures for analyzing data techniques for interpreting the results of such procedures ways of planning the gathering of data to make its analysis easier more precise or more accurate and all the machinery and results of mathematical statistics which apply to analyzing data there are several phases that can be distinguished described below the phases are iterative in that feedback from later phases may result in additional work in earlier phases the crisp framework used in data mining has similar steps data requirements the data is necessary as inputs to the analysis which is specified based upon the requirements of those directing the analytics or customers who will use the finished product of the analysis the general type of entity upon which the data will be collected is referred to as an experimental unit e g a person or population of people specific variables regarding a population e g age and income may be specified and obtained data may be numerical or categorical i e a text label for numbers data collection data is collected from a variety of sources a list of data sources are available for study research the requirements may be communicated by analysts to custodians of the data such as information technology personnel within an organization data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system which then enables one to answer relevant questions and evaluate outcomes the data may also be collected from sensors in the environment including traffic cameras satellites recording devices etc it may also be obtained through interviews downloads from online sources or reading documentation data processing data when initially obtained must be processed or organized for analysis for instance these may involve placing data into rows and columns in a table format known as structured data for further analysis often through the use of spreadsheet excel or statistical software data cleaning once processed and organized the data may be incomplete contain duplicates or contain errors the need for data cleaning will arise from problems in the way that the datum are entered and stored data cleaning is the process of preventing and correcting these errors common tasks include record matching identifying inaccuracy of data overall quality of existing data deduplication and column segmentation such data problems can also be identified through a variety of analytical techniques for example with financial information the totals for particular variables may be compared against separately published numbers that are believed to be reliable unusual amounts above or below predetermined thresholds may also be reviewed there are several types of data cleaning that are dependent upon the type of data in the set this could be phone numbers email addresses employers or other values quantitative data methods for outlier detection can be used to get rid of data that appears to have a higher likelihood of being input incorrectly textual data spell checkers can be used to lessen the amount of mistyped words however it is harder to tell if the words themselves are correct exploratory data analysis once the datasets are cleaned they can then be analyzed analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained within the obtained data the process of data exploration may result in additional data cleaning or additional requests for data thus the initialization of the iterative phases mentioned in the lead paragraph of this section descriptive statistics such as the average or median can be generated to aid in understanding the data data visualization is also a technique used in which the analyst is able to examine the data in a graphical format in order to obtain additional insights regarding the messages within the data modeling and algorithms mathematical formulas or models also known as algorithms may be applied to the data in order to identify relationships among the variables for example using correlation or causation in general terms models may be developed to evaluate a specific variable based on other variable s contained within the dataset with some residual error depending on the implemented model s accuracy e g data model error inferential statistics includes utilizing techniques that measure the relationships between particular variables for example regression analysis may be used to model whether a change in advertising independent variable x provides an explanation for the variation in sales dependent variable y in mathematical terms y sales is a function of x advertising it may be described as y ax b error where the model is designed such that a and b minimize the error when the model predicts y for a given range of values of x analysts may also attempt to build models that are descriptive of the data in an aim to simplify analysis and communicate results data product a data product is a computer application that takes data inputs and generates outputs feeding them back into the environment it may be based on a model or algorithm for instance an application that analyzes data about customer purchase history and uses the results to recommend other purchases the customer might enjoy communication once data is analyzed it may be reported in many formats to the users of the analysis to support their requirements the users may have feedback which results in additional analysis as such much of the analytical cycle is iterative when determining how to communicate the results the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience data visualization uses information displays graphics such as tables and charts to help communicate key messages contained in the data tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers while charts e g bar charts or line charts may help explain the quantitative messages contained in the data quantitative messages stephen few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process time series a single variable is captured over a period of time such as the unemployment rate over a 10 year period a line chart may be used to demonstrate the trend ranking categorical subdivisions are ranked in ascending or descending order such as a ranking of sales performance the measure by salespersons the category with each salesperson a categorical subdivision during a single period a bar chart may be used to show the comparison across the salespersons part to whole categorical subdivisions are measured as a ratio to the whole i e a percentage out of 100 a pie chart or bar chart can show the comparison of ratios such as the market share represented by competitors in a market deviation categorical subdivisions are compared against a reference such as a comparison of actual vs budget expenses for several departments of a business for a given time period a bar chart can show the comparison of the actual versus the reference amount frequency distribution shows the number of observations of a particular variable for a given interval such as the number of years in which the stock market return is between intervals such as 0 10 11 20 etc a histogram a type of bar chart may be used for this analysis correlation comparison between observations represented by two variables x y to determine if they tend to move in the same or opposite directions for example plotting unemployment x and inflation y for a sample of months a scatter plot is typically used for this message nominal comparison comparing categorical subdivisions in no particular order such as the sales volume by product code a bar chart may be used for this comparison geographic or geospatial comparison of a variable across a map or layout such as the unemployment rate by state or the number of persons on the various floors of a building a cartogram is a typical graphic used analyzing quantitative data author jonathan koomey has recommended a series of best practices for understanding quantitative data these include check raw data for anomalies prior to performing an analysis re perform important calculations such as verifying columns of data that are formula driven confirm main totals are the sum of subtotals check relationships between numbers that should be related in a predictable way such as ratios over time normalize numbers to make comparisons easier such as analyzing amounts per person or relative to gdp or as an index value relative to a base year break problems into component parts by analyzing factors that led to the results such as dupont analysis of return on equity for the variables under examination analysts typically obtain descriptive statistics for them such as the mean average median and standard deviation they may also analyze the distribution of the key variables to see how the individual values cluster around the mean the consultants at mckinsey and company named a technique for breaking a quantitative problem down into its component parts called the mece principle each layer can be broken down into its components each of the sub components must be mutually exclusive of each other and collectively add up to the layer above them the relationship is referred to as mutually exclusive and collectively exhaustive or mece for example profit by definition can be broken down into total revenue and total cost in turn total revenue can be analyzed by its components such as the revenue of divisions a b and c which are mutually exclusive of each other and should add to the total revenue collectively exhaustive analysts may use robust statistical measurements to solve certain analytical problems hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false for example the hypothesis might be that unemployment has no effect on inflation which relates to an economics concept called the phillips curve hypothesis testing involves considering the likelihood of type i and type ii errors which relate to whether the data supports accepting or rejecting the hypothesis regression analysis may be used when the analyst is trying to determine the extent to which independent variable x affects dependent variable y e g to what extent do changes in the unemployment rate x affect the inflation rate y this is an attempt to model or fit an equation line or curve to the data such that y is a function of x necessary condition analysis nca may be used when the analyst is trying to determine the extent to which independent variable x allows variable y e g to what extent is a certain unemployment rate x necessary for a certain inflation rate y whereas multiple regression analysis uses additive logic where each x variable can produce the outcome and the x s can compensate for each other they are sufficient but not necessary necessary condition analysis nca uses necessity logic where one or more x variables allow the outcome to exist but may not produce it they are necessary but not sufficient each single necessary condition must be present and compensation is not possible analytical activities of data users users may have particular data points of interest within a data set as opposed to the general messaging outlined above such low level user analytic activities are presented in the following table the taxonomy can also be organized by three poles of activities retrieving values finding data points and arranging data points barriers to effective analysis barriers to effective analysis may exist among the analysts performing the data analysis or among the audience distinguishing fact from opinion cognitive biases and innumeracy are all challenges to sound data analysis confusing fact and opinion effective analysis requires obtaining relevant facts to answer questions support a conclusion or formal opinion or test hypotheses facts by definition are irrefutable meaning that any person involved in the analysis should be able to agree upon them for example in august 2010 the congressional budget office cbo estimated that extending the bush tax cuts of 2001 and 2003 for the 2011 2020 time period would add approximately 3 3 trillion to the national debt everyone should be able to agree that indeed this is what cbo reported they can all examine the report this makes it a fact whether persons agree or disagree with the cbo is their own opinion as another example the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are fairly stated in all material respects this requires extensive analysis of factual data and evidence to support their opinion when making the leap from facts to opinions there is always the possibility that the opinion is erroneous cognitive biases there are a variety of cognitive biases that can adversely affect analysis for example confirmation bias is the tendency to search for or interpret information in a way that confirms one s preconceptions in addition individuals may discredit information that does not support their views analysts may be trained specifically to be aware of these biases and how to overcome them in his book psychology of intelligence analysis retired cia analyst richards heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions he emphasized procedures to help surface and debate alternative points of view innumeracy effective analysts are generally adept with a variety of numerical techniques however audiences may not have such literacy with numbers or numeracy they are said to be innumerate persons communicating the data may also be attempting to mislead or misinform deliberately using bad numerical techniques for example whether a number is rising or falling may not be the key factor more important may be the number relative to another number such as the size of government revenue or spending relative to the size of the economy gdp or the amount of cost relative to revenue in corporate financial statements this numerical technique is referred to as normalization or common sizing there are many such techniques employed by analysts whether adjusting for inflation i e comparing real vs nominal data or considering population increases demographics etc analysts apply a variety of techniques to address the various quantitative messages described in the section above analysts may also analyze data under different assumptions or scenario for example when analysts perform financial statement analysis they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow which they then discount to present value based on some interest rate to determine the valuation of the company or its stock similarly the cbo analyzes the effects of various policy options on the government s revenue outlays and deficits creating alternative future scenarios for key measures other topics smart buildings a data analytics approach can be used in order to predict energy consumption in buildings the different steps of the data analysis process are carried out in order to realise smart buildings where the building management and control operations including heating ventilation air conditioning lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time analytics and business intelligence analytics is the extensive use of data statistical and quantitative analysis explanatory and predictive models and fact based management to drive decisions and actions it is a subset of business intelligence which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision making education in education most educators have access to a data system for the purpose of analyzing student data these data systems present data to educators in an over the counter data format embedding labels supplemental documentation and a help system and making key package display and content decisions to improve the accuracy of educators data analyses practitioner notes this section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a wikipedia article initial data analysis the most important distinction between the initial data analysis phase and the main analysis phase is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question the initial data analysis phase is guided by the following four questions quality of data the quality of the data should be checked as early as possible data quality can be assessed in several ways using different types of analysis frequency counts descriptive statistics mean standard deviation median normality skewness kurtosis frequency histograms normal imputation is needed analysis of extreme observations outlying observations in the data are analyzed to see if they seem to disturb the distribution comparison and correction of differences in coding schemes variables are compared with coding schemes of variables external to the data set and possibly corrected if coding schemes are not comparable test for common method variance the choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase quality of measurements the quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study one should check whether structure of measurement instruments corresponds to structure reported in the literature there are two ways to assess measurement quality confirmatory factor analysis analysis of homogeneity internal consistency which gives an indication of the reliability of a measurement instrument during this analysis one inspects the variances of the items and the scales the cronbach s α of the scales and the change in the cronbach s alpha when an item would be deleted from a scale initial transformations after assessing the quality of the data and of the measurements one might decide to impute missing data or to perform initial transformations of one or more variables although this can also be done during the main analysis phase possible transformations of variables are square root transformation if the distribution differs moderately from normal log transformation if the distribution differs substantially from normal inverse transformation if the distribution differs severely from normal make categorical ordinal dichotomous if the distribution differs severely from normal and no transformations help did the implementation of the study fulfill the intentions of the research design one should check the success of the randomization procedure for instance by checking whether background and substantive variables are equally distributed within and across groups if the study did not need or use a randomization procedure one should check the success of the non random sampling for instance by checking whether all subgroups of the population of interest are represented in sample other possible data distortions that should be checked are dropout this should be identified during the initial data analysis phase item non response whether this is random or not should be assessed during the initial data analysis phase treatment quality using manipulation checks characteristics of data sample in any report or article the structure of the sample must be accurately described it is especially important to exactly determine the structure of the sample and specifically the size of the subgroups when subgroup analyses will be performed during the main analysis phase the characteristics of the data sample can be assessed by looking at basic statistics of important variables scatter plots correlations and associations cross tabulations final stage of the initial data analysis during the final stage the findings of the initial data analysis are documented and necessary preferable and possible corrective actions are taken also the original plan for the main data analyses can and should be specified in more detail or rewritten in order to do this several decisions about the main data analyses can and should be made in the case of non normals should one transform variables make variables categorical ordinal dichotomous adapt the analysis method in the case of missing data should one neglect or impute the missing data which imputation technique should be used in the case of outliers should one use robust analysis techniques in case items do not fit the scale should one adapt the measurement instrument by omitting items or rather ensure comparability with other uses of the measurement instrument s in the case of too small subgroups should one drop the hypothesis about inter group differences or use small sample techniques like exact tests or bootstrapping in case the randomization procedure seems to be defective can and should one calculate propensity scores and include them as covariates in the main analyses analysis several analyses can be used during the initial data analysis phase univariate statistics single variable bivariate associations correlations graphical techniques scatter plots it is important to take the measurement levels of the variables into account for the analyses as special statistical techniques are available for each level nominal and ordinal variables frequency counts numbers and percentages associations circumambulations crosstabulations hierarchical loglinear analysis restricted to a maximum of 8 variables loglinear analysis to identify relevant important variables and possible confounders exact tests or bootstrapping in case subgroups are small computation of new variables continuous variables distribution statistics m sd variance skewness kurtosis stem and leaf displays box plots nonlinear analysis nonlinear analysis is often necessary when the data is recorded from a nonlinear system nonlinear systems can exhibit complex dynamic effects including bifurcations chaos harmonics and subharmonics that cannot be analyzed using simple linear methods nonlinear data analysis is closely related to nonlinear system identification main data analysis in the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report exploratory and confirmatory approaches in the main analysis phase either an exploratory or confirmatory approach can be adopted usually the approach is decided before data is collected in an exploratory analysis no clear hypothesis is stated before analysing the data and the data is searched for models that describe the data well in a confirmatory analysis clear hypotheses about the data are tested exploratory data analysis should be interpreted carefully when testing multiple models at once there is a high chance on finding at least one of them to be significant but this can be due to a type 1 error it is important to always adjust the significance level when testing multiple models with for example a bonferroni correction also one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset an exploratory analysis is used to find ideas for a theory but not to test that theory as well when a model is found exploratory in a dataset then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place the confirmatory analysis therefore will not be more informative than the original exploratory analysis stability of results it is important to obtain some indication about how generalizable the results are while this is often difficult to check one can look at the stability of the results are the results reliable and reproducible there are two main ways of doing that cross validation by splitting the data into multiple parts we can check if an analysis like a fitted model based on one part of the data generalizes to another part of the data as well cross validation is generally inappropriate though if there are correlations within the data e g with panel data hence other methods of validation sometimes need to be used for more on this topic see statistical model validation sensitivity analysis a procedure to study the behavior of a system or model when global parameters are systematically varied one way to do that is via bootstrapping free software for data analysis notable free software for data analysis include devinfo a database system endorsed by the united nations development group for monitoring and analyzing human development elki data mining framework in java with data mining oriented visualization functions knime the konstanz information miner a user friendly and comprehensive data analytics framework orange a visual programming tool featuring interactive data visualization and methods for statistical data analysis data mining and machine learning pandas python library for data analysis paw fortran c data analysis framework developed at cern r a programming language and software environment for statistical computing and graphics root c data analysis framework developed at cern scipy python library for scientific computing julia a programming language well suited for numerical analysis and computational science reproducible analysis the typical data analysis workflow involves collecting data running analyses through various scripts creating visualizations and writing reports however this workflow presents challenges including a separation between analysis scripts and data as well as a gap between analysis and documentation often the correct order of running scripts is only described informally or resides in the data scientist s memory the potential for losing this information creates issues for reproducibility to address these challenges it is essential to have analysis scripts written for automated reproducible workflows additionally dynamic documentation is crucial providing reports that are understandable by both machines and humans ensuring accurate representation of the analysis workflow even as scripts evolve international data analysis contests different companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis a few examples of well known international data analysis contests are as follows kaggle competition which is held by kaggle ltpp data analysis contest held by fhwa and asce see also references citations bibliography adèr herman j 2008a chapter 14 phases and initial steps in data analysis in adèr herman j mellenbergh gideon j hand david j eds advising on research methods a consultant s companion huizen netherlands johannes van kessel pub pp 333 356 isbn 9789079418015 oclc 905799857 adèr herman j 2008b chapter 15 the main analysis phase in adèr herman j mellenbergh gideon j hand david j eds advising on research methods a consultant s companion huizen netherlands johannes van kessel pub pp 357 386 isbn 9789079418015 oclc 905799857 tabachnick b g fidell l s 2007 chapter 4 cleaning up your act screening data prior to analysis in b g tabachnick l s fidell eds using multivariate statistics fifth edition pp 60 116 boston pearson education inc allyn and bacon further reading adèr h j mellenbergh g j with contributions by d j hand 2008 advising on research methods a consultant s companion huizen the netherlands johannes van kessel publishing isbn 978 90 79418 01 5 chambers john m cleveland william s kleiner beat tukey paul a 1983 graphical methods for data analysis wadsworth duxbury press isbn 0 534 98052 x fandango armando 2017 python data analysis 2nd edition packt publishers isbn 978 1787127487 juran joseph m godfrey a blanton 1999 juran s quality handbook 5th edition new york mcgraw hill isbn 0 07 034003 x lewis beck michael s 1995 data analysis an introduction sage publications inc isbn 0 8039 5772 6 nist sematech 2008 handbook of statistical methods pyzdek t 2003 quality engineering handbook isbn 0 8247 4614 7 richard veryard 1984 pragmatic data analysis oxford blackwell scientific publications isbn 0 632 01311 7 tabachnick b g fidell l s 2007 using multivariate statistics 5th edition boston pearson education inc allyn and bacon isbn 978 0 205 45938 4",
            "total_words": 4914,
            "unique_words_percentage": 28.306878306878307,
            "stopwords_percentage": 38.48188848188848
        },
        {
            "title": "Computer science",
            "link": "https://en.wikipedia.org/wiki/Computer_science",
            "content": "computer science is the study of computation information and automation computer science spans theoretical disciplines such as algorithms theory of computation and information theory to applied disciplines including the design and implementation of hardware and software algorithms and data structures are central to computer science the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them the fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities computer graphics and computational geometry address the generation of images programming language theory considers different ways to describe computational processes and database theory concerns the management of repositories of data human computer interaction investigates the interfaces through which humans and computers interact and software engineering focuses on the design and principles behind developing software areas such as operating systems networks and embedded systems investigate the principles and design behind complex systems computer architecture describes the construction of computer components and computer operated equipment artificial intelligence and machine learning aim to synthesize goal orientated processes such as problem solving decision making environmental adaptation planning and learning found in humans and animals within artificial intelligence computer vision aims to understand and process image and video data while natural language processing aims to understand and process textual and linguistic data the fundamental concern of computer science is determining what can and cannot be automated the turing award is generally recognized as the highest distinction in computer science history the earliest foundations of what would become computer science predate the invention of the modern digital computer machines for calculating fixed numerical tasks such as the abacus have existed since antiquity aiding in computations such as multiplication and division algorithms for performing computations have existed since antiquity even before the development of sophisticated computing equipment wilhelm schickard designed and constructed the first working mechanical calculator in 1623 in 1673 gottfried leibniz demonstrated a digital mechanical calculator called the stepped reckoner leibniz may be considered the first computer scientist and information theorist because of various reasons including the fact that he documented the binary number system in 1820 thomas de colmar launched the mechanical calculator industry when he invented his simplified arithmometer the first calculating machine strong enough and reliable enough to be used daily in an office environment charles babbage started the design of the first automatic mechanical calculator his difference engine in 1822 which eventually gave him the idea of the first programmable mechanical calculator his analytical engine he started developing this machine in 1834 and in less than two years he had sketched out many of the salient features of the modern computer a crucial step was the adoption of a punched card system derived from the jacquard loom making it infinitely programmable in 1843 during the translation of a french article on the analytical engine ada lovelace wrote in one of the many notes she included an algorithm to compute the bernoulli numbers which is considered to be the first published algorithm ever specifically tailored for implementation on a computer around 1885 herman hollerith invented the tabulator which used punched cards to process statistical information eventually his company became part of ibm following babbage although unaware of his earlier work percy ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history in 1914 the spanish engineer leonardo torres quevedo published his essays on automatics and designed inspired by babbage a theoretical electromechanical calculating machine which was to be controlled by a read only program the paper also introduced the idea of floating point arithmetic in 1920 to celebrate the 100th anniversary of the invention of the arithmometer torres presented in paris the electromechanical arithmometer a prototype that demonstrated the feasibility of an electromechanical analytical engine on which commands could be typed and the results printed automatically in 1937 one hundred years after babbage s impossible dream howard aiken convinced ibm which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator the ascc harvard mark i based on babbage s analytical engine which itself used cards and a central computing unit when the machine was finished some hailed it as babbage s dream come true during the 1940s with the development of new and more powerful computing machines such as the atanasoff berry computer and eniac the term computer came to refer to the machines rather than their human predecessors as it became clear that computers could be used for more than just mathematical calculations the field of computer science broadened to study computation in general in 1945 ibm founded the watson scientific computing laboratory at columbia university in new york city the renovated fraternity house on manhattan s west side was ibm s first laboratory devoted to pure science the lab is the forerunner of ibm s research division which today operates research facilities around the world ultimately the close relationship between ibm and columbia university was instrumental in the emergence of a new scientific discipline with columbia offering one of the first academic credit courses in computer science in 1946 computer science began to be established as a distinct academic discipline in the 1950s and early 1960s the world s first computer science degree program the cambridge diploma in computer science began at the university of cambridge computer laboratory in 1953 the first computer science department in the united states was formed at purdue university in 1962 since practical computers became available many applications of computing have become distinct areas of study in their own rights etymology and scope although first proposed in 1956 the term computer science appears in a 1959 article in communications of the acm in which louis fein argues for the creation of a graduate school in computer sciences analogous to the creation of harvard business school in 1921 louis justifies the name by arguing that like management science the subject is applied and interdisciplinary in nature while having the characteristics typical of an academic discipline his efforts and those of others such as numerical analyst george forsythe were rewarded universities went on to create such departments starting with purdue in 1962 despite its name a significant amount of computer science does not involve the study of computers themselves because of this several alternative names have been proposed certain departments of major universities prefer the term computing science to emphasize precisely that difference danish scientist peter naur suggested the term datalogy to reflect the fact that the scientific discipline revolves around data and data treatment while not necessarily involving computers the first scientific institution to use the term was the department of datalogy at the university of copenhagen founded in 1969 with peter naur being the first professor in datalogy the term is used mainly in the scandinavian countries an alternative term also proposed by naur is data science this is now used for a multi disciplinary field of data analysis including statistics and databases in the early days of computing a number of terms for the practitioners of the field of computing were suggested in the communications of the acm turingineer turologist flow charts man applied meta mathematician and applied epistemologist three months later in the same journal comptologist was suggested followed next year by hypologist the term computics has also been suggested in europe terms derived from contracted translations of the expression automatic information e g informazione automatica in italian or information and mathematics are often used e g informatique french informatik german informatica italian dutch informática spanish portuguese informatika slavic languages and hungarian or pliroforiki πληροφορική which means informatics in greek similar words have also been adopted in the uk as in the school of informatics university of edinburgh in the u s however informatics is linked with applied computing or computing in the context of another domain a folkloric quotation often attributed to but almost certainly not first formulated by edsger dijkstra states that computer science is no more about computers than astronomy is about telescopes the design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science for example the study of computer hardware is usually considered part of computer engineering while the study of commercial computer systems and their deployment is often called information technology or information systems however there has been exchange of ideas between the various computer related disciplines computer science research also often intersects other disciplines such as cognitive science linguistics mathematics physics biology earth science statistics philosophy and logic computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines with some observers saying that computing is a mathematical science early computer science was strongly influenced by the work of mathematicians such as kurt gödel alan turing john von neumann rózsa péter and alonzo church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic category theory domain theory and algebra the relationship between computer science and software engineering is a contentious issue which is further muddied by disputes over what the term software engineering means and how computer science is defined david parnas taking a cue from the relationship between other engineering and science disciplines has claimed that the principal focus of computer science is studying the properties of computation in general while the principal focus of software engineering is the design of specific computations to achieve practical goals making the two separate but complementary disciplines the academic political and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science both types of departments tend to make efforts to bridge the field educationally if not across all research philosophy epistemology of computer science despite the word science in its name there is debate over whether or not computer science is a discipline of science mathematics or engineering allen newell and herbert a simon argued in 1975 computer science is an empirical discipline we would have called it an experimental science but like astronomy economics and geology some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method nonetheless they are experiments each new machine that is built is an experiment actually constructing the machine poses a question to nature and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available it has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs but a problem remains in defining the laws and theorems of computer science if any exist and defining the nature of experiments in computer science proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering they also argue that while empirical sciences observe what presently exists computer science observes what is possible to exist and while scientists discover laws from observation no proper laws have been found in computer science and it is instead concerned with creating phenomena proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods computer scientists edsger w dijkstra and tony hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems paradigms of computer science a number of computer scientists have argued for the distinction of three separate paradigms in computer science peter wegner argued that those paradigms are science technology and mathematics peter denning s working group argued that they are theory abstraction modeling and design amnon h eden described them as the rationalist paradigm which treats computer science as a branch of mathematics which is prevalent in theoretical computer science and mainly employs deductive reasoning the technocratic paradigm which might be found in engineering approaches most prominently in software engineering and the scientific paradigm which approaches computer related artifacts from the empirical perspective of natural sciences identifiable in some branches of artificial intelligence computer science focuses on methods involved in design specification programming verification implementation and testing of human made computing systems fields as a discipline computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software csab formerly called computing sciences accreditation board which is made up of representatives of the association for computing machinery acm and the ieee computer society ieee cs identifies four areas that it considers crucial to the discipline of computer science theory of computation algorithms and data structures programming methodology and languages and computer elements and architecture in addition to these four areas csab also identifies fields such as software engineering artificial intelligence computer networking and communication database systems parallel computation distributed computation human computer interaction computer graphics operating systems and numerical and symbolic computation as being important areas of computer science theoretical computer science theoretical computer science is mathematical and abstract in spirit but it derives its motivation from practical and everyday computation it aims to understand the nature of computation and as a consequence of this understanding provide more efficient methodologies theory of computation according to peter denning the fundamental question underlying computer science is what can be automated theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations in an effort to answer the first question computability theory examines which computational problems are solvable on various theoretical models of computation the second question is addressed by computational complexity theory which studies the time and space costs associated with different approaches to solving a multitude of computational problems the famous p np problem one of the millennium prize problems is an open problem in the theory of computation information and coding theory information theory closely related to probability and statistics is related to the quantification of information this was developed by claude shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data coding theory is the study of the properties of codes systems for converting information from one form to another and their fitness for a specific application codes are used for data compression cryptography error detection and correction and more recently also for network coding codes are studied for the purpose of designing efficient and reliable data transmission methods data structures and algorithms data structures and algorithms are the studies of commonly used computational methods and their computational efficiency programming language theory and formal methods programming language theory is a branch of computer science that deals with the design implementation analysis characterization and classification of programming languages and their individual features it falls within the discipline of computer science both depending on and affecting mathematics software engineering and linguistics it is an active research area with numerous dedicated academic journals formal methods are a particular kind of mathematically based technique for the specification development and verification of software and hardware systems the use of formal methods for software and hardware design is motivated by the expectation that as in other engineering disciplines performing appropriate mathematical analysis can contribute to the reliability and robustness of a design they form an important theoretical underpinning for software engineering especially where safety or security is involved formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing for industrial use tool support is required however the high cost of using formal methods means that they are usually only used in the development of high integrity and life critical systems where safety or security is of utmost importance formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals in particular logic calculi formal languages automata theory and program semantics but also type systems and algebraic data types to problems in software and hardware specification and verification applied computer science computer graphics and visualization computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data the study is connected to many other fields in computer science including computer vision image processing and computational geometry and is heavily applied in the fields of special effects and video games image and sound processing information can take the form of images sound video or other multimedia bits of information can be streamed via signals its processing is the central notion of informatics the european view on computing which studies information processing algorithms independently of the type of information carrier whether it is electrical mechanical or biological this field plays important role in information theory telecommunications information engineering and has applications in medical image computing and speech synthesis among others what is the lower bound on the complexity of fast fourier transform algorithms is one of the unsolved problems in theoretical computer science computational science finance and engineering scientific computing or computational science is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems a major usage of scientific computing is simulation of various processes including computational fluid dynamics physical electrical and electronic systems and circuits as well as societies and social situations notably war games along with their habitats among many others modern computers enable optimization of such designs as complete aircraft notable in electrical and electronic circuit design are spice as well as software for physical realization of new or modified designs the latter includes essential design software for integrated circuits human computer interaction human computer interaction hci is the field of study and research concerned with the design and use of computer systems mainly based on the analysis of the interaction between humans and computer interfaces hci has several subfields that focus on the relationship between emotions social behavior and brain activity with computers software engineering software engineering is the study of designing implementing and modifying the software in order to ensure it is of high quality affordable maintainable and fast to build it is a systematic approach to software design involving the application of engineering practices to software software engineering deals with the organizing and analyzing of software it does not just deal with the creation or manufacture of new software but its internal arrangement and maintenance for example software testing systems engineering technical debt and software development processes artificial intelligence artificial intelligence ai aims to or is required to synthesize goal orientated processes such as problem solving decision making environmental adaptation learning and communication found in humans and animals from its origins in cybernetics and in the dartmouth conference 1956 artificial intelligence research has been necessarily cross disciplinary drawing on areas of expertise such as applied mathematics symbolic logic semiotics electrical engineering philosophy of mind neurophysiology and social intelligence ai is associated in the popular mind with robotic development but the main field of practical application has been as an embedded component in areas of software development which require computational understanding the starting point in the late 1940s was alan turing s question can computers think and the question remains effectively unanswered although the turing test is still used to assess computer output on the scale of human intelligence but the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real world data computer systems computer architecture and microarchitecture computer architecture or digital computer organization is the conceptual design and fundamental operational structure of a computer system it focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory computer engineers study computational logic and design of computer hardware from individual processor components microcontrollers personal computers to supercomputers and embedded systems the term architecture in computer literature can be traced to the work of lyle r johnson and frederick p brooks jr members of the machine organization department in ibm s main research center in 1959 concurrent parallel and distributed computing concurrency is a property of systems in which several computations are executing simultaneously and potentially interacting with each other a number of mathematical models have been developed for general concurrent computation including petri nets process calculi and the parallel random access machine model when multiple computers are connected in a network while using concurrency this is known as a distributed system computers within that distributed system have their own private memory and information can be exchanged to achieve common goals computer networks this branch of computer science aims to manage networks between computers worldwide computer security and cryptography computer security is a branch of computer technology with the objective of protecting information from unauthorized access disruption or modification while maintaining the accessibility and usability of the system for its intended users historical cryptography is the art of writing and deciphering secret messages modern cryptography is the scientific study of problems relating to distributed computations that can be attacked technologies studied in modern cryptography include symmetric and asymmetric encryption digital signatures cryptographic hash functions key agreement protocols blockchain zero knowledge proofs and garbled circuits databases and data mining a database is intended to organize store and retrieve large amounts of data easily digital databases are managed using database management systems to store create maintain and search data through database models and query languages data mining is a process of discovering patterns in large data sets discoveries the philosopher of computing bill rapaport noted three great insights of computer science gottfried wilhelm leibniz s george boole s alan turing s claude shannon s and samuel morse s insight there are only two objects that a computer has to deal with in order to represent anything all the information about any computable problem can be represented using only 0 and 1 or any other bistable pair that can flip flop between two easily distinguishable states such as on off magnetized de magnetized high voltage low voltage etc alan turing s insight there are only five actions that a computer has to perform in order to do anything every algorithm can be expressed in a language for a computer consisting of only five basic instructions move left one location move right one location read symbol at current location print 0 at current location print 1 at current location corrado böhm and giuseppe jacopini s insight there are only three ways of combining these actions into more complex ones that are needed in order for a computer to do anything only three rules are needed to combine any set of basic instructions into more complex ones sequence first do this then do that selection if such and such is the case then do this else do that repetition while such and such is the case do this the three rules of boehm s and jacopini s insight can be further simplified with the use of goto which means it is more elementary than structured programming programming paradigms programming languages can be used to accomplish different tasks in different ways common programming paradigms include functional programming a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data it is a declarative programming paradigm which means programming is done with expressions or declarations instead of statements imperative programming a programming paradigm that uses statements that change a program s state in much the same way that the imperative mood in natural languages expresses commands an imperative program consists of commands for the computer to perform imperative programming focuses on describing how a program operates object oriented programming a programming paradigm based on the concept of objects which may contain data in the form of fields often known as attributes and code in the form of procedures often known as methods a feature of objects is that an object s procedures can access and often modify the data fields of the object with which they are associated thus object oriented computer programs are made out of objects that interact with one another service oriented programming a programming paradigm that uses services as the unit of computer work to design and implement integrated business applications and mission critical software programs many languages offer support for multiple paradigms making the distinction more a matter of style than of technical capabilities research conferences are important events for computer science research during these conferences researchers from the public and private sectors present their recent work and meet unlike in most other academic fields in computer science the prestige of conference papers is greater than that of journal publications one proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results a task better handled by conferences than by journals see also notes references further reading external links dblp computer science bibliography association for computing machinery institute of electrical and electronics engineers",
            "total_words": 4258,
            "unique_words_percentage": 30.155002348520433,
            "stopwords_percentage": 38.39830906528886
        },
        {
            "title": "Data engineering",
            "link": "https://en.wikipedia.org/wiki/Data_engineering",
            "content": "data engineering refers to the building of systems to enable the collection and usage of data this data is usually used to enable subsequent analysis and data science which often involves machine learning making the data usable usually involves substantial compute and storage as well as data processing history around the 1970s 1980s the term information engineering methodology iem was created to describe database design and the use of software for data analysis and processing these techniques were intended to be used by database administrators dbas and by systems analysts based upon an understanding of the operational processing needs of organizations for the 1980s in particular these techniques were meant to help bridge the gap between strategic business planning and information systems a key early contributor often called the father of information engineering methodology was the australian clive finkelstein who wrote several articles about it between 1976 and 1980 and also co authored an influential savant institute report on it with james martin over the next few years finkelstein continued work in a more business driven direction which was intended to address a rapidly changing business environment martin continued work in a more data processing driven direction from 1983 to 1987 charles m richter guided by clive finkelstein played a significant role in revamping iem as well as helping to design the iem software product user data which helped automate iem in the early 2000s the data and data tooling was generally held by the information technology it teams in most companies other teams then used data for their work e g reporting and there was usually little overlap in data skillset between these parts of the business in the early 2010s with the rise of the internet the massive increase in data volumes velocity and variety led to the term big data to describe the data itself and data driven tech companies like facebook and airbnb started using the phrase data engineer due to the new scale of the data major firms like google facebook amazon apple microsoft and netflix started to move away from traditional etl and storage techniques they started creating data engineering a type of software engineering focused on data and in particular infrastructure warehousing data protection cybersecurity mining modelling processing and metadata management this change in approach was particularly focused on cloud computing data started to be handled and used by many parts of the business such as sales and marketing and not just it tools compute high performance computing is critical for the processing and analysis of data one particularly widespread approach to computing for data engineering is dataflow programming in which the computation is represented as a directed graph dataflow graph nodes are the operations and edges represent the flow of data popular implementations include apache spark and the deep learning specific tensorflow more recent implementations such as differential timely dataflow have used incremental computing for much more efficient data processing storage data is stored in a variety of ways one of the key deciding factors is in how the data will be used data engineers optimize data storage and processing systems to reduce costs they use data compression partitioning and archiving databases if the data is structured and some form of online transaction processing is required then databases are generally used originally mostly relational databases were used with strong acid transaction correctness guarantees most relational databases use sql for their queries however with the growth of data in the 2010s nosql databases have also become popular since they horizontally scaled more easily than relational databases by giving up the acid transaction guarantees as well as reducing the object relational impedance mismatch more recently newsql databases which attempt to allow horizontal scaling while retaining acid guarantees have become popular data warehouses if the data is structured and online analytical processing is required but not online transaction processing then data warehouses are a main choice they enable data analysis mining and artificial intelligence on a much larger scale than databases can allow and indeed data often flow from databases into data warehouses business analysts data engineers and data scientists can access data warehouses using tools such as sql or business intelligence software data lakes a data lake is a centralized repository for storing processing and securing large volumes of data a data lake can contain structured data from relational databases semi structured data unstructured data and binary data a data lake can be created on premises or in a cloud based environment using the services from public cloud vendors such as amazon microsoft or google files if the data is less structured then often they are just stored as files there are several options file systems represent data hierarchically in nested folders block storage splits data into regularly sized chunks this often matches up with virtual hard drives or solid state drives object storage manages data using metadata often each file is assigned a key such as a uuid management the number and variety of different data processes and storage locations can become overwhelming for users this inspired the usage of a workflow management system e g airflow to allow the data tasks to be specified created and monitored the tasks are often specified as a directed acyclic graph dag lifecycle business planning business objectives that executives set for what s to come are characterized in key business plans with their more noteworthy definition in tactical business plans and implementation in operational business plans most businesses today recognize the fundamental need to grow a business plan that follows this strategy it is often difficult to implement these plans because of the lack of transparency at the tactical and operational degrees of organizations this kind of planning requires feedback to allow for early correction of problems that are due to miscommunication and misinterpretation of the business plan systems design the design of data systems involves several components such as architecting data platforms and designing data stores data modeling this is the process of producing a data model an abstract model to describe the data and relationships between different parts of the data roles data engineer a data engineer is a type of software engineer who creates big data etl pipelines to manage the flow of data through the organization this makes it possible to take huge amounts of data and translate it into insights they are focused on the production readiness of data and things like formats resilience scaling and security data engineers usually hail from a software engineering background and are proficient in programming languages like java python scala and rust they will be more familiar with databases architecture cloud computing and agile software development data scientist data scientists are more focused on the analysis of the data they will be more familiar with mathematics algorithms statistics and machine learning see also big data information technology software engineering computer science references further reading external links the complex method iem archived july 20 2019 at the wayback machine rapid application development enterprise engineering and rapid delivery of enterprise architecture",
            "total_words": 1174,
            "unique_words_percentage": 39.182282793867124,
            "stopwords_percentage": 36.79727427597956
        },
        {
            "title": "Big data",
            "link": "https://en.wikipedia.org/wiki/Big_data",
            "content": "big data primarily refers to data sets that are too large or complex to be dealt with by traditional data processing software data with many entries rows offer greater statistical power while data with higher complexity more attributes or columns may lead to a higher false discovery rate big data analysis challenges include capturing data data storage data analysis search sharing transfer visualization querying updating information privacy and data source big data was originally associated with three key concepts volume variety and velocity the analysis of big data presents challenges in sampling and thus previously allowing for only observations and sampling thus a fourth concept veracity refers to the quality or insightfulness of the data without sufficient investment in expertise for big data veracity the volume and variety of data can produce costs and risks that exceed an organization s capacity to create and capture value from big data current usage of the term big data tends to refer to the use of predictive analytics user behavior analytics or certain other advanced data analytics methods that extract value from big data and seldom to a particular size of data set there is little doubt that the quantities of data now available are indeed large but that s not the most relevant characteristic of this new data ecosystem analysis of data sets can find new correlations to spot business trends prevent diseases combat crime and so on scientists business executives medical practitioners advertising and governments alike regularly meet difficulties with large data sets in areas including internet searches fintech healthcare analytics geographic information systems urban informatics and business informatics scientists encounter limitations in e science work including meteorology genomics connectomics complex physics simulations biology and environmental research the size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices cheap and numerous information sensing internet of things devices aerial remote sensing equipment software logs cameras microphones radio frequency identification rfid readers and wireless sensor networks the world s technological per capita capacity to store information has roughly doubled every 40 months since the 1980s as of 2012 every day 2 5 exabytes 2 17 260 bytes of data are generated based on an idc report prediction the global data volume was predicted to grow exponentially from 4 4 zettabytes to 44 zettabytes between 2013 and 2020 by 2025 idc predicts there will be 163 zettabytes of data according to idc global spending on big data and business analytics bda solutions is estimated to reach 215 7 billion in 2021 while statista report the global big data market is forecasted to grow to 103 billion by 2027 in 2011 mckinsey company reported if us healthcare were to use big data creatively and effectively to drive efficiency and quality the sector could create more than 300 billion in value every year in the developed economies of europe government administrators could save more than 100 billion 149 billion in operational efficiency improvements alone by using big data and users of services enabled by personal location data could capture 600 billion in consumer surplus one question for large enterprises is determining who should own big data initiatives that affect the entire organization relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data the processing and analysis of big data may require massively parallel software running on tens hundreds or even thousands of servers what qualifies as big data varies depending on the capabilities of those analyzing it and their tools furthermore expanding capabilities make big data a moving target for some organizations facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options for others it may take tens or hundreds of terabytes before data size becomes a significant consideration definition the term big data has been in use since the 1990s with some giving credit to john mashey for popularizing the term big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture curate manage and process data within a tolerable elapsed time big data philosophy encompasses unstructured semi structured and structured data however the main focus is on unstructured data big data size is a constantly moving target as of 2012 ranging from a few dozen terabytes to many zettabytes of data big data requires a set of techniques and technologies with new forms of integration to reveal insights from data sets that are diverse complex and of a massive scale volume variety velocity and various other vs are added by some organizations to describe it a revision challenged by some industry authorities the vs of big data were often referred to as the three vs four vs and five vs they represented the qualities of big data in volume variety velocity veracity and value variability is often included as an additional quality of big data a 2018 definition states big data is where parallel computing tools are needed to handle data and notes this represents a distinct and clearly defined change in the computer science used via parallel programming theories and losses of some of the guarantees and capabilities made by codd s relational model in a comparative study of big datasets kitchin and mcardle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases for this reason other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait instead of focusing on the intrinsic characteristics of big data this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected stored made available and analyzed big data vs business intelligence the growing maturity of the concept more starkly delineates the difference between big data and business intelligence business intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things detect trends etc big data uses mathematical analysis optimization inductive statistics and concepts from nonlinear system identification to infer laws regressions nonlinear relationships and causal effects from large sets of data with low information density to reveal relationships and dependencies or to perform predictions of outcomes and behaviors characteristics big data can be described by the following characteristics volume the quantity of generated and stored data the size of the data determines the value and potential insight and whether it can be considered big data or not the size of big data is usually larger than terabytes and petabytes variety the type and nature of the data earlier technologies like rdbmss were capable to handle structured data efficiently and effectively however the change in type and nature from structured to semi structured or unstructured challenged the existing tools and technologies big data technologies evolved with the prime intention to capture store and process the semi structured and unstructured variety data generated with high speed velocity and huge in size volume later these tools and technologies were explored and used for handling structured data also but preferable for storage eventually the processing of structured data was still kept as optional either using big data or traditional rdbmss this helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media log files sensors etc big data draws from text images audio video plus it completes missing pieces through data fusion velocity the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development big data is often available in real time compared to small data big data is produced more continually two kinds of velocity related to big data are the frequency of generation and the frequency of handling recording and publishing veracity the truthfulness or reliability of the data which refers to the data quality and the data value big data must not only be large in size but also must be reliable in order to achieve value in the analysis of it the data quality of captured data can vary greatly affecting an accurate analysis value the worth in information that can be achieved by the processing and analysis of large datasets value also can be measured by an assessment of the other qualities of big data value may also represent the profitability of information that is retrieved from the analysis of big data variability the characteristic of the changing formats structure or sources of big data big data can include structured unstructured or combinations of structured and unstructured data big data analysis may integrate raw data from multiple sources the processing of raw data may also involve transformations of unstructured data to structured data other possible characteristics of big data are exhaustive whether the entire system i e n textstyle n all is captured or recorded or not big data may or may not include all the available data from sources fine grained and uniquely lexical respectively the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified relational if the data collected contains common fields that would enable a conjoining or meta analysis of different data sets extensional if new fields in each element of the data collected can be added or changed easily scalability if the size of the big data storage system can expand rapidly architecture big data repositories have existed in many forms often built by corporations with a special need commercial vendors historically offered parallel database management systems for big data beginning in the 1990s for many years wintercorp published the largest database report teradata corporation in 1984 marketed the parallel processing dbc 1012 system teradata systems were the first to store and analyze 1 terabyte of data in 1992 hard disk drives were 2 5 gb in 1991 so the definition of big data continuously evolves teradata installed the first petabyte class rdbms based system in 2007 as of 2017 there are a few dozen petabyte class teradata relational databases installed the largest of which exceeds 50 pb systems up until 2008 were 100 structured relational data since then teradata has added semi structured data types including xml json and avro in 2000 seisint inc now lexisnexis risk solutions developed a c based distributed platform for data processing and querying known as the hpcc systems platform this system automatically partitions distributes stores and delivers structured semi structured and unstructured data across multiple commodity servers users can write data processing pipelines and queries in a declarative dataflow programming language called ecl data analysts working in ecl are not required to define data schemas upfront and can rather focus on the particular problem at hand reshaping data in the best possible manner as they develop the solution in 2004 lexisnexis acquired seisint inc and their high speed parallel processing platform and successfully used this platform to integrate the data systems of choicepoint inc when they acquired that company in 2008 in 2011 the hpcc systems platform was open sourced under the apache v2 0 license cern and other physics experiments have collected big data sets for many decades usually analyzed via high throughput computing rather than the map reduce architectures usually meant by the current big data movement in 2004 google published a paper on a process called mapreduce that uses a similar architecture the mapreduce concept provides a parallel processing model and an associated implementation was released to process huge amounts of data with mapreduce queries are split and distributed across parallel nodes and processed in parallel the map step the results are then gathered and delivered the reduce step the framework was very successful so others wanted to replicate the algorithm therefore an implementation of the mapreduce framework was adopted by an apache open source project named hadoop apache spark was developed in 2012 in response to limitations in the mapreduce paradigm as it adds in memory processing and the ability to set up many operations not just map followed by reducing mike2 0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled big data solution offering the methodology addresses handling big data in terms of useful permutations of data sources complexity in interrelationships and difficulty in deleting or modifying individual records studies in 2012 showed that a multiple layer architecture was one option to address the issues that big data presents a distributed parallel architecture distributes data across multiple servers these parallel execution environments can dramatically improve data processing speeds this type of architecture inserts data into a parallel dbms which implements the use of mapreduce and hadoop frameworks this type of framework looks to make the processing power transparent to the end user by using a front end application server the data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management this enables quick segregation of data into the data lake thereby reducing the overhead time technologies a 2011 mckinsey global institute report characterizes the main components and ecosystem of big data as follows techniques for analyzing data such as a b testing machine learning and natural language processing big data technologies like business intelligence cloud computing and databases visualization such as charts graphs and other displays of the data multidimensional big data can also be represented as olap data cubes or mathematically tensors array database systems have set out to provide storage and high level query support on this data type additional technologies being applied to big data include efficient tensor based computation such as multilinear subspace learning massively parallel processing mpp databases search based applications data mining distributed file systems distributed cache e g burst buffer and memcached distributed databases cloud and hpc based infrastructure applications storage and computing resources and the internet although many approaches and technologies have been developed it still remains difficult to carry out machine learning with big data some mpp relational databases have the ability to store and manage petabytes of data implicit is the ability to load monitor back up and optimize the use of the large data tables in the rdbms darpa s topological data analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called ayasdi the practitioners of big data analytics processes are generally hostile to slower shared storage preferring direct attached storage das in its various forms from solid state drive ssd to high capacity sata disk buried inside parallel processing nodes the perception of shared storage architectures storage area network san and network attached storage nas is that they are relatively slow complex and expensive these qualities are not consistent with big data analytics systems that thrive on system performance commodity infrastructure and low cost real or near real time information delivery is one of the defining characteristics of big data analytics latency is therefore avoided whenever and wherever possible data in direct attached memory or disk is good data on memory or disk at the other end of an fc san connection is not the cost of an san at the scale needed for analytics applications is much higher than other storage techniques applications big data has increased the demand of information management specialists so much so that software ag oracle corporation ibm microsoft sap emc hp and dell have spent more than 15 billion on software firms specializing in data management and analytics in 2010 this industry was worth more than 100 billion and was growing at almost 10 percent a year about twice as fast as the software business as a whole developed economies increasingly use data intensive technologies there are 4 6 billion mobile phone subscriptions worldwide and between 1 billion and 2 billion people accessing the internet between 1990 and 2005 more than 1 billion people worldwide entered the middle class which means more people became more literate which in turn led to information growth the world s effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986 471 petabytes in 1993 2 2 exabytes in 2000 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014 according to one estimate one third of the globally stored information is in the form of alphanumeric text and still image data which is the format most useful for most big data applications this also shows the potential of yet unused data i e in the form of video and audio content while many vendors offer off the shelf products for big data experts promote the development of in house custom tailored systems if the company has sufficient technical capabilities government the use and adoption of big data within governmental processes allows efficiencies in terms of cost productivity and innovation but comes with flaws data analysis often requires multiple parts of government central and local to work in collaboration and create new and innovative processes to deliver the desired outcome a common government organization that makes use of big data is the national security administration nsa which monitors the activities of the internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up civil registration and vital statistics crvs collects all certificates status from birth to death crvs is a source of big data for governments international development research on the effective usage of information and communication technologies for development also known as ict4d suggests that big data technology can make important contributions but also present unique challenges to international development advancements in big data analysis offer cost effective opportunities to improve decision making in critical development areas such as health care employment economic productivity crime security and natural disaster and resource management additionally user generated data offers new opportunities to give the unheard a voice however longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy imperfect methodology and interoperability issues the challenge of big data for development is currently evolving toward the application of this data through machine learning known as artificial intelligence for development ai4d benefits a major practical application of big data for development has been fighting poverty with data in 2015 blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata and in 2016 jean and colleagues combined satellite imagery and machine learning to predict poverty using digital trace data to study the labor market and the digital economy in latin america hilbert and colleagues argue that digital trace data has several benefits such as thematic coverage including areas that were previously difficult or impossible to measure geographical coverage providing sizable and comparable data for almost all countries including many small countries that usually are not included in international inventories level of detail providing fine grained data with many interrelated variables and new aspects like network connections timeliness and timeseries graphs can be produced within days of being collected challenges at the same time working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis priorities change but the basic discussions remain the same among the main challenges are representativeness while traditional development statistics is mainly concerned with the representativeness of random survey samples digital trace data is never a random sample generalizability while observational data always represents this source very well it only represents what it represents and nothing more while it is tempting to generalize from specific observations of one platform to broader settings this is often very deceptive harmonization digital trace data still requires international harmonization of indicators it adds the challenge of so called data fusion the harmonization of different sources data overload analysts and institutions are not used to effectively deal with a large number of variables which is efficiently done with interactive dashboards practitioners still lack a standard workflow that would allow researchers users and policymakers to efficiently and effectively deal with data finance big data is being rapidly adopted in finance to 1 speed up processing and 2 deliver better more informed inferences both internally and to the clients of the financial institutions the financial applications of big data range from investing decisions and trading processing volumes of available price data limit order books economic data and more all at the same time portfolio management optimizing over an increasingly large array of financial instruments potentially selected from different asset classes risk management credit rating based on extended information and any other aspect where the data inputs are large big data has also been a typical concept within the field of alternative financial service some of the major areas involve crowd funding platforms and crypto currency exchanges healthcare big data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics clinical risk intervention and predictive analytics waste and care variability reduction automated external and internal reporting of patient data standardized medical terms and patient registries some areas of improvement are more aspirational than actually implemented the level of data generated within healthcare systems is not trivial with the added adoption of mhealth ehealth and wearable technologies the volume of data will continue to increase this includes electronic health record data imaging data patient generated data sensor data and other forms of difficult to process data there is now an even greater need for such environments to pay greater attention to data and information quality big data very often means dirty data and the fraction of data inaccuracies increases with data volume growth human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed while extensive information in healthcare is now electronic it fits under the big data umbrella as most is unstructured and difficult to use the use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights privacy and autonomy to transparency and trust big data in health research is particularly promising in terms of exploratory biomedical research as data driven analysis can move forward more quickly than hypothesis driven research then trends seen in data analysis can be tested in traditional hypothesis driven follow up biological research and eventually clinical research a related application sub area that heavily relies on big data within the healthcare field is that of computer aided diagnosis in medicine for instance for epilepsy monitoring it is customary to create 5 to 10 gb of data daily similarly a single uncompressed image of breast tomosynthesis averages 450 mb of data these are just a few of the many examples where computer aided diagnosis uses big data for this reason big data has been recognized as one of the seven key challenges that computer aided diagnosis systems need to overcome in order to reach the next level of performance education a mckinsey global institute study found a shortage of 1 5 million highly trained data professionals and managers and a number of universities including university of tennessee and uc berkeley have created masters programs to meet this demand private boot camps have also developed programs to meet that demand including paid programs like the data incubator or general assembly in the specific field of marketing one of the problems stressed by wedel and kannan is that marketing has several sub domains e g advertising promotions product development branding that all use different types of data media to understand how the media uses big data it is first necessary to provide some context into the mechanism used for media process it has been suggested by nick couldry and joseph turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals the industry appears to be moving away from the traditional approach of using specific media environments such as newspapers magazines or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations the ultimate aim is to serve or convey a message or content that is statistically speaking in line with the consumer s mindset for example publishing environments are increasingly tailoring messages advertisements and content articles to appeal to consumers that have been exclusively gleaned through various data mining activities targeting of consumers for advertising by marketers data capture data journalism publishers and journalists use big data tools to provide unique and innovative insights and infographics channel 4 the british public service television broadcaster is a leader in the field of big data and data analysis insurance health insurance providers are collecting data on social determinants of health such as food and tv consumption marital status clothing size and purchasing habits from which they make predictions on health costs in order to spot health issues in their clients it is controversial whether these predictions are currently being used for pricing internet of things iot big data and the iot work in conjunction data extracted from iot devices provides a mapping of device inter connectivity such mappings have been used by the media industry companies and governments to more accurately target their audience and increase media efficiency the iot is also increasingly adopted as a means of gathering sensory data and this sensory data has been used in medical manufacturing and transportation contexts kevin ashton the digital innovation expert who is credited with coining the term defines the internet of things in this quote if we had computers that knew everything there was to know about things using data they gathered without any help from us we would be able to track and count everything and greatly reduce waste loss and cost we would know when things needed replacing repairing or recalling and whether they were fresh or past their best information technology especially since 2015 big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology it the use of big data to resolve it and data collection issues within an enterprise is called it operations analytics itoa by applying big data principles into the concepts of machine intelligence and deep computing it departments can predict potential issues and prevent them itoa businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data survey science compared to survey based data collection big data has low cost per data point applies analysis techniques via machine learning and data mining and includes diverse and new data sources e g registers social media apps and other forms digital data since 2018 survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality there have been three big data meets survey science bigsurv conferences in 2018 2020 virtual 2023 and as of 2023 one conference forthcoming in 2025 a special issue in the social science computer review a special issue in journal of the royal statistical society and a special issue in ep j data science and a book called big data meets social sciences edited by craig hill and five other fellows of the american statistical association in 2021 the founding members of bigsurv received the warren j mitofsky innovators award from the american association for public opinion research marketing big data is notable in marketing due to the constant datafication of everyday consumers of the internet in which all forms of data are tracked the datafication of consumers can be defined as quantifying many of or all human behaviors for the purpose of marketing the increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially it is predicted to increase from 44 to 163 zettabytes within the span of five years the size of big data can often be difficult to navigate for marketers as a result adopters of big data may find themselves at a disadvantage algorithmic findings can be difficult to achieve with such large datasets big data in marketing is a highly lucrative tool that can be used for large corporations its value being as a result of the possibility of predicting significant trends interests or statistical outcomes in a consumer based manner there are three significant factors in the use of big data in marketing big data provides customer behavior pattern spotting for marketers since all human actions are being quantified into readable numbers for marketers to analyze and use for their research in addition big data can also be seen as a customized product recommendation tool specifically since big data is effective in analyzing customers purchase behaviors and browsing patterns this technology can assist companies in promoting specific personalized products to specific customers real time market responsiveness is important for marketers because of the ability to shift marketing efforts and correct to current trends which is helpful in maintaining relevance to consumers this can supply corporations with the information necessary to predict the wants and needs of consumers in advance data driven market ambidexterity are being highly fueled by big data new models and algorithms are being developed to make significant predictions about certain economic and social situations case studies government china the integrated joint operations platform ijop 一体化联合作战平台 is used by the government to monitor the population particularly uyghurs biometrics including dna samples are gathered through a program of free physicals by 2020 china plans to give all its citizens a personal social credit score based on how they behave the social credit system now being piloted in a number of chinese cities is considered a form of mass surveillance which uses big data analysis technology india big data analysis was tried out for the bjp to win the 2014 indian general election the indian government uses numerous techniques to ascertain how the indian electorate is responding to government action as well as ideas for policy augmentation israel personalized diabetic treatments can be created through glucome s big data solution united kingdom examples of uses of big data in public services data on prescription drugs by connecting origin location and the time of each prescription a research unit was able to exemplify and examine the considerable delay between the release of any given drug and a uk wide adaptation of the national institute for health and care excellence guidelines this suggests that new or most up to date drugs take some time to filter through to the general patient joining up data a local authority blended data about services such as road gritting rotas with services for people at risk such as meals on wheels the connection of data allowed the local authority to avoid any weather related delay united states in 2012 the obama administration announced the big data research and development initiative to explore how big data could be used to address important problems faced by the government the initiative is composed of 84 different big data programs spread across six departments big data analysis played a large role in barack obama s successful 2012 re election campaign the united states federal government owns four of the ten most powerful supercomputers in the world the utah data center has been constructed by the united states national security agency when finished the facility will be able to handle a large amount of information collected by the nsa over the internet the exact amount of storage space is unknown but more recent sources claim it will be on the order of a few exabytes this has posed security concerns regarding the anonymity of the data collected retail walmart handles more than 1 million customer transactions every hour which are imported into databases estimated to contain more than 2 5 petabytes 2560 terabytes of data the equivalent of 167 times the information contained in all the books in the us library of congress windermere real estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day fico card detection system protects accounts worldwide omnichannel retailing leverages online big data to improve offline experiences science the large hadron collider experiments represent about 150 million sensors delivering data 40 million times per second there are nearly 600 million collisions per second after filtering and refraining from recording more than 99 99995 of these streams there are 1 000 collisions of interest per second as a result only working with less than 0 001 of the sensor stream data the data flow from all four lhc experiments represents 25 petabytes annual rate before replication as of 2012 this becomes nearly 200 petabytes after replication if all sensor data were recorded in lhc the data flow would be extremely hard to work with the data flow would exceed 150 million petabytes annual rate or nearly 500 exabytes per day before replication to put the number in perspective this is equivalent to 500 quintillion 5 1020 bytes per day almost 200 times more than all the other sources combined in the world the square kilometre array is a radio telescope built of thousands of antennas it is expected to be operational by 2024 collectively these antennas are expected to gather 14 exabytes and store one petabyte per day it is considered one of the most ambitious scientific projects ever undertaken when the sloan digital sky survey sdss began to collect astronomical data in 2000 it amassed more in its first few weeks than all data collected in the history of astronomy previously continuing at a rate of about 200 gb per night sdss has amassed more than 140 terabytes of information when the large synoptic survey telescope successor to sdss comes online in 2020 its designers expect it to acquire that amount of data every five days decoding the human genome originally took 10 years to process now it can be achieved in less than a day the dna sequencers have divided the sequencing cost by 10 000 in the last ten years which is 100 times less expensive than the reduction in cost predicted by moore s law the nasa center for climate simulation nccs stores 32 petabytes of climate observations and simulations on the discover supercomputing cluster google s dnastack compiles and organizes dna samples of genetic data from around the world to identify diseases and other medical defects these fast and exact calculations eliminate any friction points or human errors that could be made by one of the numerous science and biology experts working with the dna dnastack a part of google genomics allows scientists to use the vast sample of resources from google s search server to scale social experiments that would usually take years instantly 23andme s dna database contains the genetic information of over 1 000 000 people worldwide the company explores selling the anonymous aggregated genetic data to other researchers and pharmaceutical companies for research purposes if patients give their consent ahmad hariri professor of psychology and neuroscience at duke university who has been using 23andme in his research since 2009 states that the most important aspect of the company s new service is that it makes genetic research accessible and relatively cheap for scientists a study that identified 15 genome sites linked to depression in 23andme s database lead to a surge in demands to access the repository with 23andme fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper computational fluid dynamics cfd and hydrodynamic turbulence research generate massive data sets the johns hopkins turbulence databases jhtdb contains over 350 terabytes of spatiotemporal fields from direct numerical simulations of various turbulent flows such data have been difficult to share using traditional methods such as downloading flat simulation output files the data within jhtdb can be accessed using virtual sensors with various access modes ranging from direct web browser queries access through matlab python fortran and c programs executing on clients platforms to cut out services to download raw data the data have been used in over 150 scientific publications sports big data can be used to improve training and understanding competitors using sport sensors it is also possible to predict winners in a match using big data analytics future performance of players could be predicted as well thus players value and salary is determined by data collected throughout the season in formula one races race cars with hundreds of sensors generate terabytes of data these sensors collect data points from tire pressure to fuel burn efficiency based on the data engineers and data analysts decide whether adjustments should be made in order to win a race besides using big data race teams try to predict the time they will finish the race beforehand based on simulations using data collected over the season technology as of 2013 ebay com uses two data warehouses at 7 5 petabytes and 40pb as well as a 40pb hadoop cluster for search consumer recommendations and merchandising amazon com handles millions of back end operations every day as well as queries from more than half a million third party sellers the core technology that keeps amazon running is linux based and as of 2005 they had the world s three largest linux databases with capacities of 7 8 tb 18 5 tb and 24 7 tb facebook handles 50 billion photos from its user base as of june 2017 facebook reached 2 billion monthly active users google was handling roughly 100 billion searches per month as of august 2012 covid 19 during the covid 19 pandemic big data was raised as a way to minimise the impact of the disease significant applications of big data included minimising the spread of the virus case identification and development of medical treatment governments used big data to track infected people to minimise spread early adopters included china taiwan south korea and israel research activities encrypted search and cluster formation in big data were demonstrated in march 2014 at the american society of engineering education gautam siwach engaged at tackling the challenges of big data by mit computer science and artificial intelligence laboratory and amir esmailpour at the unh research group investigated the key features of big data as the formation of clusters and their interconnections they focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology moreover they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data in march 2012 the white house announced a national big data initiative that consisted of six federal departments and agencies committing more than 200 million to big data research projects the initiative included a national science foundation expeditions in computing grant of 10 million over five years to the amplab at the university of california berkeley the amplab also received funds from darpa and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer the white house big data initiative also included a commitment by the department of energy to provide 25 million in funding over five years to establish the scalable data management analysis and visualization sdav institute led by the energy department s lawrence berkeley national laboratory the sdav institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department s supercomputers the u s state of massachusetts announced the massachusetts big data initiative in may 2012 which provides funding from the state government and private companies to a variety of research institutions the massachusetts institute of technology hosts the intel science and technology center for big data in the mit computer science and artificial intelligence laboratory combining government corporate and institutional funding and research efforts the european commission is funding the two year long big data public private forum through their seventh framework program to engage companies academics and other stakeholders in discussing big data issues the project aims to define a strategy in terms of research and innovation to guide supporting actions from the european commission in the successful implementation of the big data economy outcomes of this project will be used as input for horizon 2020 their next framework program the british government announced in march 2014 the founding of the alan turing institute named after the computer pioneer and code breaker which will focus on new ways to collect and analyze large data sets at the university of waterloo stratford campus canadian open data experience code inspiration day participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world computational social sciences anyone can use application programming interfaces apis provided by big data holders such as google and twitter to do research in the social and behavioral sciences often these apis are provided for free tobias preis et al used google trends data to demonstrate that internet users from countries with a higher per capita gross domestic products gdps are more likely to search for information about the future than information about the past the findings suggest there may be a link between online behaviors and real world economic indicators the authors of the study examined google queries logs made by ratio of the volume of searches for the coming year 2011 to the volume of searches for the previous year 2009 which they call the future orientation index they compared the future orientation index to the per capita gdp of each country and found a strong tendency for countries where google users inquire more about the future to have a higher gdp tobias preis and his colleagues helen susannah moat and h eugene stanley introduced a method to identify online precursors for stock market moves using trading strategies based on search volume data provided by google trends their analysis of google search volume for 98 terms of varying financial relevance published in scientific reports suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets big data sets come with algorithmic challenges that previously did not exist hence there is seen by some to be a need to fundamentally change the processing ways sampling big data a research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough the name big data itself contains a term related to size and this is an important characteristic of big data but sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population in manufacturing different types of sensory data such as acoustics vibration pressure current voltage and controller data are available at short time intervals to predict downtime it may not be necessary to look at all the data but a sample may be sufficient big data can be broken down by various data point categories such as demographic psychographic behavioral and transactional data with large sets of data points marketers are able to create and use more customized segments of consumers for more strategic targeting critique critiques of the big data paradigm come in two flavors those that question the implications of the approach itself and those that question the way it is currently done one approach to this criticism is the field of critical data studies critiques of the big data paradigm a crucial problem is that we do not know much about the underlying empirical micro processes that lead to the emergence of the typical network characteristics of big data in their critique snijders matzat and reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro processes mark graham has leveled broad critiques at chris anderson s assertion that big data will spell the end of theory focusing in particular on the notion that big data must always be contextualized in their social economic and political contexts even as companies invest eight and nine figure sums to derive insight from information streaming in from suppliers and customers less than 40 of employees have sufficiently mature processes and skills to do so to overcome this insight deficit big data no matter how comprehensive or well analyzed must be complemented by big judgment according to an article in the harvard business review much in the same line it has been pointed out that the decisions based on the analysis of big data are inevitably informed by the world as it was in the past or at best as it currently is fed by a large number of data on past experiences algorithms can predict future development if the future is similar to the past if the system s dynamics of the future change if it is not a stationary process the past can say little about the future in order to make predictions in changing environments it would be necessary to have a thorough understanding of the systems dynamic which requires theory as a response to this critique alemany oliver and vayre suggest to use abductive reasoning as a first step in the research process in order to bring context to consumers digital traces and make new theories emerge additionally it has been suggested to combine big data approaches with computer simulations such as agent based models and complex systems agent based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms finally the use of multivariate methods that probe for the latent structure of the data such as factor analysis and cluster analysis have proven useful as analytic approaches that go well beyond the bi variate approaches e g contingency tables typically employed with smaller data sets in health and biology conventional scientific approaches are based on experimentation for these approaches the limiting factor is the relevant data that can confirm or refute the initial hypothesis a new postulate is accepted now in biosciences the information provided by the data in huge volumes omics without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation in the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor the search logic is reversed and the limits of induction glory of science and philosophy scandal c d broad 1926 are to be considered privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information expert panels have released various policy recommendations to conform practice to expectations of privacy the misuse of big data in several cases by media companies and even the government has allowed for abolition of trust in almost every fundamental institution holding up society barocas and nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected with whom it is shared under what constraints and for what purposes critiques of the v model the v model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information this led to the framework of cognitive big data which characterizes big data applications according to data completeness understanding of the non obvious from data data correlation causation and predictability causality as not essential requirement to achieve predictability explainability and interpretability humans desire to understand and accept what they understand where algorithms do not cope with this level of automated decision making algorithms that support automated decision making and algorithmic self learning critiques of novelty large data sets have been analyzed by computing machines for well over a century including the us census analytics performed by ibm s punch card machines which computed statistics including means and variances of populations across the whole continent in more recent decades science experiments such as cern have produced data on similar scales to current commercial big data however science experiments have tended to analyze their data using specialized custom built high performance computing super computing clusters and grids rather than clouds of cheap commodity computers as in the current commercial wave implying a difference in both culture and technology stack critiques of big data execution ulf dietrich reips and uwe matzat wrote in 2014 that big data had become a fad in scientific research researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data this approach may lead to results that have a bias in one way or another integration across heterogeneous data resources some that might be considered big data and others not presents formidable logistical as well as analytical challenges but many researchers argue that such integrations are likely to represent the most promising new frontiers in science in the provocative article critical questions for big data the authors title big data a part of mythology large data sets offer a higher form of intelligence and knowledge with the aura of truth objectivity and accuracy users of big data are often lost in the sheer volume of numbers and working with big data is still subjective and what it quantifies does not necessarily have a closer claim on objective truth recent developments in bi domain such as pro active reporting especially target improvements in the usability of big data through automated filtering of non useful data and correlations big structures are full of spurious correlations either because of non causal coincidences law of truly large numbers solely nature of big randomness ramsey theory or existence of non included factors so the hope of early experimenters to make large databases of numbers speak for themselves and revolutionize scientific method is questioned catherine tucker has pointed to hype around big data writing by itself big data is unlikely to be valuable the article explains the many contexts where data is cheap relative to the cost of retaining talent to process it suggests that processing skills are more important than data itself in creating value for a firm big data analysis is often shallow compared to analysis of smaller data sets in many big data projects there is no large data analysis happening but the challenge is the extract transform load part of data pre processing big data is a buzzword and a vague term but at the same time an obsession with entrepreneurs consultants scientists and the media big data showcases such as google flu trends failed to deliver good predictions in recent years overstating the flu outbreaks by a factor of two similarly academy awards and election predictions solely based on twitter were more often off than on target big data often poses the same challenges as small data adding more data does not solve problems of bias but may emphasize other problems in particular data sources such as twitter are not representative of the overall population and results drawn from such sources may then lead to wrong conclusions google translate which is based on big data statistical analysis of text does a good job at translating web pages however results from specialized domains may be dramatically skewed on the other hand big data may also introduce new problems such as the multiple comparisons problem simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant ioannidis argued that most published research findings are false due to essentially the same effect when many scientific teams and researchers each perform many experiments i e process a big amount of scientific data although not with big data technology the likelihood of a significant result being false grows fast even more so when only positive results are published furthermore big data analytics results are only as good as the model on which they are predicated in an example big data took part in attempting to predict the results of the 2016 u s presidential election with varying degrees of success critiques of big data policing and surveillance big data has been used in policing and surveillance by institutions like law enforcement and corporations due to the less visible nature of data based surveillance as compared to traditional methods of policing objections to big data policing are less likely to arise according to sarah brayne s big data surveillance the case of policing big data policing can reproduce existing societal inequalities in three ways placing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm increasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system encouraging members of society to abandon interactions with institutions that would create a digital trace thus creating obstacles to social inclusion if these potential problems are not corrected or regulated the effects of big data policing may continue to shape societal hierarchies conscientious usage of big data policing could prevent individual level biases from becoming institutional biases brayne also notes see also references bibliography hilbert m 2016 big data for development a review of promises and challenges development policy review 34 1 135 74 doi 10 1111 dpr 12142 free access archived 21 april 2021 at the wayback machine snijders c matzat u reips u d 2012 big data big gaps of knowledge in the field of internet international journal of internet science 7 1 5 archived from the original on 23 november 2019 retrieved 13 april 2013 yanase j triantaphyllou e 2019 a systematic survey of computer aided diagnosis in medicine past and present developments expert systems with applications 138 112821 doi 10 1016 j eswa 2019 112821 s2cid 199019309 further reading peter kinnaird inbal talgam cohen eds 2012 big data xrds crossroads the acm magazine for students vol 19 no 1 association for computing machinery issn 1528 4980 oclc 779657714 jure leskovec anand rajaraman jeffrey d ullman 2014 mining of massive datasets cambridge university press isbn 978 1 10707723 2 oclc 888463433 viktor mayer schönberger kenneth cukier 2013 big data a revolution that will transform how we live work and think houghton mifflin harcourt isbn 978 1 29990302 9 oclc 828620988 press gil 9 may 2013 a very short history of big data forbes com jersey city nj retrieved 17 september 2016 stephens davidowitz seth 2017 everybody lies big data new data and what the internet can tell us about who we really are dey street books isbn 978 0 06239085 1 big data the management revolution harvard business review october 2012 o neil cathy 2017 weapons of math destruction how big data increases inequality and threatens democracy broadway books isbn 978 0 55341883 5 external links media related to big data at wikimedia commons the dictionary definition of big data at wiktionary",
            "total_words": 9328,
            "unique_words_percentage": 26.436535162950257,
            "stopwords_percentage": 35.51672384219554
        }
    ],
    "Big Data": [
        {
            "title": "Big data",
            "link": "https://en.wikipedia.org/wiki/Big_data",
            "content": "big data primarily refers to data sets that are too large or complex to be dealt with by traditional data processing software data with many entries rows offer greater statistical power while data with higher complexity more attributes or columns may lead to a higher false discovery rate big data analysis challenges include capturing data data storage data analysis search sharing transfer visualization querying updating information privacy and data source big data was originally associated with three key concepts volume variety and velocity the analysis of big data presents challenges in sampling and thus previously allowing for only observations and sampling thus a fourth concept veracity refers to the quality or insightfulness of the data without sufficient investment in expertise for big data veracity the volume and variety of data can produce costs and risks that exceed an organization s capacity to create and capture value from big data current usage of the term big data tends to refer to the use of predictive analytics user behavior analytics or certain other advanced data analytics methods that extract value from big data and seldom to a particular size of data set there is little doubt that the quantities of data now available are indeed large but that s not the most relevant characteristic of this new data ecosystem analysis of data sets can find new correlations to spot business trends prevent diseases combat crime and so on scientists business executives medical practitioners advertising and governments alike regularly meet difficulties with large data sets in areas including internet searches fintech healthcare analytics geographic information systems urban informatics and business informatics scientists encounter limitations in e science work including meteorology genomics connectomics complex physics simulations biology and environmental research the size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices cheap and numerous information sensing internet of things devices aerial remote sensing equipment software logs cameras microphones radio frequency identification rfid readers and wireless sensor networks the world s technological per capita capacity to store information has roughly doubled every 40 months since the 1980s as of 2012 every day 2 5 exabytes 2 17 260 bytes of data are generated based on an idc report prediction the global data volume was predicted to grow exponentially from 4 4 zettabytes to 44 zettabytes between 2013 and 2020 by 2025 idc predicts there will be 163 zettabytes of data according to idc global spending on big data and business analytics bda solutions is estimated to reach 215 7 billion in 2021 while statista report the global big data market is forecasted to grow to 103 billion by 2027 in 2011 mckinsey company reported if us healthcare were to use big data creatively and effectively to drive efficiency and quality the sector could create more than 300 billion in value every year in the developed economies of europe government administrators could save more than 100 billion 149 billion in operational efficiency improvements alone by using big data and users of services enabled by personal location data could capture 600 billion in consumer surplus one question for large enterprises is determining who should own big data initiatives that affect the entire organization relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data the processing and analysis of big data may require massively parallel software running on tens hundreds or even thousands of servers what qualifies as big data varies depending on the capabilities of those analyzing it and their tools furthermore expanding capabilities make big data a moving target for some organizations facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options for others it may take tens or hundreds of terabytes before data size becomes a significant consideration definition the term big data has been in use since the 1990s with some giving credit to john mashey for popularizing the term big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture curate manage and process data within a tolerable elapsed time big data philosophy encompasses unstructured semi structured and structured data however the main focus is on unstructured data big data size is a constantly moving target as of 2012 ranging from a few dozen terabytes to many zettabytes of data big data requires a set of techniques and technologies with new forms of integration to reveal insights from data sets that are diverse complex and of a massive scale volume variety velocity and various other vs are added by some organizations to describe it a revision challenged by some industry authorities the vs of big data were often referred to as the three vs four vs and five vs they represented the qualities of big data in volume variety velocity veracity and value variability is often included as an additional quality of big data a 2018 definition states big data is where parallel computing tools are needed to handle data and notes this represents a distinct and clearly defined change in the computer science used via parallel programming theories and losses of some of the guarantees and capabilities made by codd s relational model in a comparative study of big datasets kitchin and mcardle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases for this reason other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait instead of focusing on the intrinsic characteristics of big data this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected stored made available and analyzed big data vs business intelligence the growing maturity of the concept more starkly delineates the difference between big data and business intelligence business intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things detect trends etc big data uses mathematical analysis optimization inductive statistics and concepts from nonlinear system identification to infer laws regressions nonlinear relationships and causal effects from large sets of data with low information density to reveal relationships and dependencies or to perform predictions of outcomes and behaviors characteristics big data can be described by the following characteristics volume the quantity of generated and stored data the size of the data determines the value and potential insight and whether it can be considered big data or not the size of big data is usually larger than terabytes and petabytes variety the type and nature of the data earlier technologies like rdbmss were capable to handle structured data efficiently and effectively however the change in type and nature from structured to semi structured or unstructured challenged the existing tools and technologies big data technologies evolved with the prime intention to capture store and process the semi structured and unstructured variety data generated with high speed velocity and huge in size volume later these tools and technologies were explored and used for handling structured data also but preferable for storage eventually the processing of structured data was still kept as optional either using big data or traditional rdbmss this helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media log files sensors etc big data draws from text images audio video plus it completes missing pieces through data fusion velocity the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development big data is often available in real time compared to small data big data is produced more continually two kinds of velocity related to big data are the frequency of generation and the frequency of handling recording and publishing veracity the truthfulness or reliability of the data which refers to the data quality and the data value big data must not only be large in size but also must be reliable in order to achieve value in the analysis of it the data quality of captured data can vary greatly affecting an accurate analysis value the worth in information that can be achieved by the processing and analysis of large datasets value also can be measured by an assessment of the other qualities of big data value may also represent the profitability of information that is retrieved from the analysis of big data variability the characteristic of the changing formats structure or sources of big data big data can include structured unstructured or combinations of structured and unstructured data big data analysis may integrate raw data from multiple sources the processing of raw data may also involve transformations of unstructured data to structured data other possible characteristics of big data are exhaustive whether the entire system i e n textstyle n all is captured or recorded or not big data may or may not include all the available data from sources fine grained and uniquely lexical respectively the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified relational if the data collected contains common fields that would enable a conjoining or meta analysis of different data sets extensional if new fields in each element of the data collected can be added or changed easily scalability if the size of the big data storage system can expand rapidly architecture big data repositories have existed in many forms often built by corporations with a special need commercial vendors historically offered parallel database management systems for big data beginning in the 1990s for many years wintercorp published the largest database report teradata corporation in 1984 marketed the parallel processing dbc 1012 system teradata systems were the first to store and analyze 1 terabyte of data in 1992 hard disk drives were 2 5 gb in 1991 so the definition of big data continuously evolves teradata installed the first petabyte class rdbms based system in 2007 as of 2017 there are a few dozen petabyte class teradata relational databases installed the largest of which exceeds 50 pb systems up until 2008 were 100 structured relational data since then teradata has added semi structured data types including xml json and avro in 2000 seisint inc now lexisnexis risk solutions developed a c based distributed platform for data processing and querying known as the hpcc systems platform this system automatically partitions distributes stores and delivers structured semi structured and unstructured data across multiple commodity servers users can write data processing pipelines and queries in a declarative dataflow programming language called ecl data analysts working in ecl are not required to define data schemas upfront and can rather focus on the particular problem at hand reshaping data in the best possible manner as they develop the solution in 2004 lexisnexis acquired seisint inc and their high speed parallel processing platform and successfully used this platform to integrate the data systems of choicepoint inc when they acquired that company in 2008 in 2011 the hpcc systems platform was open sourced under the apache v2 0 license cern and other physics experiments have collected big data sets for many decades usually analyzed via high throughput computing rather than the map reduce architectures usually meant by the current big data movement in 2004 google published a paper on a process called mapreduce that uses a similar architecture the mapreduce concept provides a parallel processing model and an associated implementation was released to process huge amounts of data with mapreduce queries are split and distributed across parallel nodes and processed in parallel the map step the results are then gathered and delivered the reduce step the framework was very successful so others wanted to replicate the algorithm therefore an implementation of the mapreduce framework was adopted by an apache open source project named hadoop apache spark was developed in 2012 in response to limitations in the mapreduce paradigm as it adds in memory processing and the ability to set up many operations not just map followed by reducing mike2 0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled big data solution offering the methodology addresses handling big data in terms of useful permutations of data sources complexity in interrelationships and difficulty in deleting or modifying individual records studies in 2012 showed that a multiple layer architecture was one option to address the issues that big data presents a distributed parallel architecture distributes data across multiple servers these parallel execution environments can dramatically improve data processing speeds this type of architecture inserts data into a parallel dbms which implements the use of mapreduce and hadoop frameworks this type of framework looks to make the processing power transparent to the end user by using a front end application server the data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management this enables quick segregation of data into the data lake thereby reducing the overhead time technologies a 2011 mckinsey global institute report characterizes the main components and ecosystem of big data as follows techniques for analyzing data such as a b testing machine learning and natural language processing big data technologies like business intelligence cloud computing and databases visualization such as charts graphs and other displays of the data multidimensional big data can also be represented as olap data cubes or mathematically tensors array database systems have set out to provide storage and high level query support on this data type additional technologies being applied to big data include efficient tensor based computation such as multilinear subspace learning massively parallel processing mpp databases search based applications data mining distributed file systems distributed cache e g burst buffer and memcached distributed databases cloud and hpc based infrastructure applications storage and computing resources and the internet although many approaches and technologies have been developed it still remains difficult to carry out machine learning with big data some mpp relational databases have the ability to store and manage petabytes of data implicit is the ability to load monitor back up and optimize the use of the large data tables in the rdbms darpa s topological data analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called ayasdi the practitioners of big data analytics processes are generally hostile to slower shared storage preferring direct attached storage das in its various forms from solid state drive ssd to high capacity sata disk buried inside parallel processing nodes the perception of shared storage architectures storage area network san and network attached storage nas is that they are relatively slow complex and expensive these qualities are not consistent with big data analytics systems that thrive on system performance commodity infrastructure and low cost real or near real time information delivery is one of the defining characteristics of big data analytics latency is therefore avoided whenever and wherever possible data in direct attached memory or disk is good data on memory or disk at the other end of an fc san connection is not the cost of an san at the scale needed for analytics applications is much higher than other storage techniques applications big data has increased the demand of information management specialists so much so that software ag oracle corporation ibm microsoft sap emc hp and dell have spent more than 15 billion on software firms specializing in data management and analytics in 2010 this industry was worth more than 100 billion and was growing at almost 10 percent a year about twice as fast as the software business as a whole developed economies increasingly use data intensive technologies there are 4 6 billion mobile phone subscriptions worldwide and between 1 billion and 2 billion people accessing the internet between 1990 and 2005 more than 1 billion people worldwide entered the middle class which means more people became more literate which in turn led to information growth the world s effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986 471 petabytes in 1993 2 2 exabytes in 2000 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014 according to one estimate one third of the globally stored information is in the form of alphanumeric text and still image data which is the format most useful for most big data applications this also shows the potential of yet unused data i e in the form of video and audio content while many vendors offer off the shelf products for big data experts promote the development of in house custom tailored systems if the company has sufficient technical capabilities government the use and adoption of big data within governmental processes allows efficiencies in terms of cost productivity and innovation but comes with flaws data analysis often requires multiple parts of government central and local to work in collaboration and create new and innovative processes to deliver the desired outcome a common government organization that makes use of big data is the national security administration nsa which monitors the activities of the internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up civil registration and vital statistics crvs collects all certificates status from birth to death crvs is a source of big data for governments international development research on the effective usage of information and communication technologies for development also known as ict4d suggests that big data technology can make important contributions but also present unique challenges to international development advancements in big data analysis offer cost effective opportunities to improve decision making in critical development areas such as health care employment economic productivity crime security and natural disaster and resource management additionally user generated data offers new opportunities to give the unheard a voice however longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy imperfect methodology and interoperability issues the challenge of big data for development is currently evolving toward the application of this data through machine learning known as artificial intelligence for development ai4d benefits a major practical application of big data for development has been fighting poverty with data in 2015 blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata and in 2016 jean and colleagues combined satellite imagery and machine learning to predict poverty using digital trace data to study the labor market and the digital economy in latin america hilbert and colleagues argue that digital trace data has several benefits such as thematic coverage including areas that were previously difficult or impossible to measure geographical coverage providing sizable and comparable data for almost all countries including many small countries that usually are not included in international inventories level of detail providing fine grained data with many interrelated variables and new aspects like network connections timeliness and timeseries graphs can be produced within days of being collected challenges at the same time working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis priorities change but the basic discussions remain the same among the main challenges are representativeness while traditional development statistics is mainly concerned with the representativeness of random survey samples digital trace data is never a random sample generalizability while observational data always represents this source very well it only represents what it represents and nothing more while it is tempting to generalize from specific observations of one platform to broader settings this is often very deceptive harmonization digital trace data still requires international harmonization of indicators it adds the challenge of so called data fusion the harmonization of different sources data overload analysts and institutions are not used to effectively deal with a large number of variables which is efficiently done with interactive dashboards practitioners still lack a standard workflow that would allow researchers users and policymakers to efficiently and effectively deal with data finance big data is being rapidly adopted in finance to 1 speed up processing and 2 deliver better more informed inferences both internally and to the clients of the financial institutions the financial applications of big data range from investing decisions and trading processing volumes of available price data limit order books economic data and more all at the same time portfolio management optimizing over an increasingly large array of financial instruments potentially selected from different asset classes risk management credit rating based on extended information and any other aspect where the data inputs are large big data has also been a typical concept within the field of alternative financial service some of the major areas involve crowd funding platforms and crypto currency exchanges healthcare big data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics clinical risk intervention and predictive analytics waste and care variability reduction automated external and internal reporting of patient data standardized medical terms and patient registries some areas of improvement are more aspirational than actually implemented the level of data generated within healthcare systems is not trivial with the added adoption of mhealth ehealth and wearable technologies the volume of data will continue to increase this includes electronic health record data imaging data patient generated data sensor data and other forms of difficult to process data there is now an even greater need for such environments to pay greater attention to data and information quality big data very often means dirty data and the fraction of data inaccuracies increases with data volume growth human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed while extensive information in healthcare is now electronic it fits under the big data umbrella as most is unstructured and difficult to use the use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights privacy and autonomy to transparency and trust big data in health research is particularly promising in terms of exploratory biomedical research as data driven analysis can move forward more quickly than hypothesis driven research then trends seen in data analysis can be tested in traditional hypothesis driven follow up biological research and eventually clinical research a related application sub area that heavily relies on big data within the healthcare field is that of computer aided diagnosis in medicine for instance for epilepsy monitoring it is customary to create 5 to 10 gb of data daily similarly a single uncompressed image of breast tomosynthesis averages 450 mb of data these are just a few of the many examples where computer aided diagnosis uses big data for this reason big data has been recognized as one of the seven key challenges that computer aided diagnosis systems need to overcome in order to reach the next level of performance education a mckinsey global institute study found a shortage of 1 5 million highly trained data professionals and managers and a number of universities including university of tennessee and uc berkeley have created masters programs to meet this demand private boot camps have also developed programs to meet that demand including paid programs like the data incubator or general assembly in the specific field of marketing one of the problems stressed by wedel and kannan is that marketing has several sub domains e g advertising promotions product development branding that all use different types of data media to understand how the media uses big data it is first necessary to provide some context into the mechanism used for media process it has been suggested by nick couldry and joseph turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals the industry appears to be moving away from the traditional approach of using specific media environments such as newspapers magazines or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations the ultimate aim is to serve or convey a message or content that is statistically speaking in line with the consumer s mindset for example publishing environments are increasingly tailoring messages advertisements and content articles to appeal to consumers that have been exclusively gleaned through various data mining activities targeting of consumers for advertising by marketers data capture data journalism publishers and journalists use big data tools to provide unique and innovative insights and infographics channel 4 the british public service television broadcaster is a leader in the field of big data and data analysis insurance health insurance providers are collecting data on social determinants of health such as food and tv consumption marital status clothing size and purchasing habits from which they make predictions on health costs in order to spot health issues in their clients it is controversial whether these predictions are currently being used for pricing internet of things iot big data and the iot work in conjunction data extracted from iot devices provides a mapping of device inter connectivity such mappings have been used by the media industry companies and governments to more accurately target their audience and increase media efficiency the iot is also increasingly adopted as a means of gathering sensory data and this sensory data has been used in medical manufacturing and transportation contexts kevin ashton the digital innovation expert who is credited with coining the term defines the internet of things in this quote if we had computers that knew everything there was to know about things using data they gathered without any help from us we would be able to track and count everything and greatly reduce waste loss and cost we would know when things needed replacing repairing or recalling and whether they were fresh or past their best information technology especially since 2015 big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology it the use of big data to resolve it and data collection issues within an enterprise is called it operations analytics itoa by applying big data principles into the concepts of machine intelligence and deep computing it departments can predict potential issues and prevent them itoa businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data survey science compared to survey based data collection big data has low cost per data point applies analysis techniques via machine learning and data mining and includes diverse and new data sources e g registers social media apps and other forms digital data since 2018 survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality there have been three big data meets survey science bigsurv conferences in 2018 2020 virtual 2023 and as of 2023 one conference forthcoming in 2025 a special issue in the social science computer review a special issue in journal of the royal statistical society and a special issue in ep j data science and a book called big data meets social sciences edited by craig hill and five other fellows of the american statistical association in 2021 the founding members of bigsurv received the warren j mitofsky innovators award from the american association for public opinion research marketing big data is notable in marketing due to the constant datafication of everyday consumers of the internet in which all forms of data are tracked the datafication of consumers can be defined as quantifying many of or all human behaviors for the purpose of marketing the increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially it is predicted to increase from 44 to 163 zettabytes within the span of five years the size of big data can often be difficult to navigate for marketers as a result adopters of big data may find themselves at a disadvantage algorithmic findings can be difficult to achieve with such large datasets big data in marketing is a highly lucrative tool that can be used for large corporations its value being as a result of the possibility of predicting significant trends interests or statistical outcomes in a consumer based manner there are three significant factors in the use of big data in marketing big data provides customer behavior pattern spotting for marketers since all human actions are being quantified into readable numbers for marketers to analyze and use for their research in addition big data can also be seen as a customized product recommendation tool specifically since big data is effective in analyzing customers purchase behaviors and browsing patterns this technology can assist companies in promoting specific personalized products to specific customers real time market responsiveness is important for marketers because of the ability to shift marketing efforts and correct to current trends which is helpful in maintaining relevance to consumers this can supply corporations with the information necessary to predict the wants and needs of consumers in advance data driven market ambidexterity are being highly fueled by big data new models and algorithms are being developed to make significant predictions about certain economic and social situations case studies government china the integrated joint operations platform ijop 一体化联合作战平台 is used by the government to monitor the population particularly uyghurs biometrics including dna samples are gathered through a program of free physicals by 2020 china plans to give all its citizens a personal social credit score based on how they behave the social credit system now being piloted in a number of chinese cities is considered a form of mass surveillance which uses big data analysis technology india big data analysis was tried out for the bjp to win the 2014 indian general election the indian government uses numerous techniques to ascertain how the indian electorate is responding to government action as well as ideas for policy augmentation israel personalized diabetic treatments can be created through glucome s big data solution united kingdom examples of uses of big data in public services data on prescription drugs by connecting origin location and the time of each prescription a research unit was able to exemplify and examine the considerable delay between the release of any given drug and a uk wide adaptation of the national institute for health and care excellence guidelines this suggests that new or most up to date drugs take some time to filter through to the general patient joining up data a local authority blended data about services such as road gritting rotas with services for people at risk such as meals on wheels the connection of data allowed the local authority to avoid any weather related delay united states in 2012 the obama administration announced the big data research and development initiative to explore how big data could be used to address important problems faced by the government the initiative is composed of 84 different big data programs spread across six departments big data analysis played a large role in barack obama s successful 2012 re election campaign the united states federal government owns four of the ten most powerful supercomputers in the world the utah data center has been constructed by the united states national security agency when finished the facility will be able to handle a large amount of information collected by the nsa over the internet the exact amount of storage space is unknown but more recent sources claim it will be on the order of a few exabytes this has posed security concerns regarding the anonymity of the data collected retail walmart handles more than 1 million customer transactions every hour which are imported into databases estimated to contain more than 2 5 petabytes 2560 terabytes of data the equivalent of 167 times the information contained in all the books in the us library of congress windermere real estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day fico card detection system protects accounts worldwide omnichannel retailing leverages online big data to improve offline experiences science the large hadron collider experiments represent about 150 million sensors delivering data 40 million times per second there are nearly 600 million collisions per second after filtering and refraining from recording more than 99 99995 of these streams there are 1 000 collisions of interest per second as a result only working with less than 0 001 of the sensor stream data the data flow from all four lhc experiments represents 25 petabytes annual rate before replication as of 2012 this becomes nearly 200 petabytes after replication if all sensor data were recorded in lhc the data flow would be extremely hard to work with the data flow would exceed 150 million petabytes annual rate or nearly 500 exabytes per day before replication to put the number in perspective this is equivalent to 500 quintillion 5 1020 bytes per day almost 200 times more than all the other sources combined in the world the square kilometre array is a radio telescope built of thousands of antennas it is expected to be operational by 2024 collectively these antennas are expected to gather 14 exabytes and store one petabyte per day it is considered one of the most ambitious scientific projects ever undertaken when the sloan digital sky survey sdss began to collect astronomical data in 2000 it amassed more in its first few weeks than all data collected in the history of astronomy previously continuing at a rate of about 200 gb per night sdss has amassed more than 140 terabytes of information when the large synoptic survey telescope successor to sdss comes online in 2020 its designers expect it to acquire that amount of data every five days decoding the human genome originally took 10 years to process now it can be achieved in less than a day the dna sequencers have divided the sequencing cost by 10 000 in the last ten years which is 100 times less expensive than the reduction in cost predicted by moore s law the nasa center for climate simulation nccs stores 32 petabytes of climate observations and simulations on the discover supercomputing cluster google s dnastack compiles and organizes dna samples of genetic data from around the world to identify diseases and other medical defects these fast and exact calculations eliminate any friction points or human errors that could be made by one of the numerous science and biology experts working with the dna dnastack a part of google genomics allows scientists to use the vast sample of resources from google s search server to scale social experiments that would usually take years instantly 23andme s dna database contains the genetic information of over 1 000 000 people worldwide the company explores selling the anonymous aggregated genetic data to other researchers and pharmaceutical companies for research purposes if patients give their consent ahmad hariri professor of psychology and neuroscience at duke university who has been using 23andme in his research since 2009 states that the most important aspect of the company s new service is that it makes genetic research accessible and relatively cheap for scientists a study that identified 15 genome sites linked to depression in 23andme s database lead to a surge in demands to access the repository with 23andme fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper computational fluid dynamics cfd and hydrodynamic turbulence research generate massive data sets the johns hopkins turbulence databases jhtdb contains over 350 terabytes of spatiotemporal fields from direct numerical simulations of various turbulent flows such data have been difficult to share using traditional methods such as downloading flat simulation output files the data within jhtdb can be accessed using virtual sensors with various access modes ranging from direct web browser queries access through matlab python fortran and c programs executing on clients platforms to cut out services to download raw data the data have been used in over 150 scientific publications sports big data can be used to improve training and understanding competitors using sport sensors it is also possible to predict winners in a match using big data analytics future performance of players could be predicted as well thus players value and salary is determined by data collected throughout the season in formula one races race cars with hundreds of sensors generate terabytes of data these sensors collect data points from tire pressure to fuel burn efficiency based on the data engineers and data analysts decide whether adjustments should be made in order to win a race besides using big data race teams try to predict the time they will finish the race beforehand based on simulations using data collected over the season technology as of 2013 ebay com uses two data warehouses at 7 5 petabytes and 40pb as well as a 40pb hadoop cluster for search consumer recommendations and merchandising amazon com handles millions of back end operations every day as well as queries from more than half a million third party sellers the core technology that keeps amazon running is linux based and as of 2005 they had the world s three largest linux databases with capacities of 7 8 tb 18 5 tb and 24 7 tb facebook handles 50 billion photos from its user base as of june 2017 facebook reached 2 billion monthly active users google was handling roughly 100 billion searches per month as of august 2012 covid 19 during the covid 19 pandemic big data was raised as a way to minimise the impact of the disease significant applications of big data included minimising the spread of the virus case identification and development of medical treatment governments used big data to track infected people to minimise spread early adopters included china taiwan south korea and israel research activities encrypted search and cluster formation in big data were demonstrated in march 2014 at the american society of engineering education gautam siwach engaged at tackling the challenges of big data by mit computer science and artificial intelligence laboratory and amir esmailpour at the unh research group investigated the key features of big data as the formation of clusters and their interconnections they focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology moreover they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data in march 2012 the white house announced a national big data initiative that consisted of six federal departments and agencies committing more than 200 million to big data research projects the initiative included a national science foundation expeditions in computing grant of 10 million over five years to the amplab at the university of california berkeley the amplab also received funds from darpa and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer the white house big data initiative also included a commitment by the department of energy to provide 25 million in funding over five years to establish the scalable data management analysis and visualization sdav institute led by the energy department s lawrence berkeley national laboratory the sdav institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department s supercomputers the u s state of massachusetts announced the massachusetts big data initiative in may 2012 which provides funding from the state government and private companies to a variety of research institutions the massachusetts institute of technology hosts the intel science and technology center for big data in the mit computer science and artificial intelligence laboratory combining government corporate and institutional funding and research efforts the european commission is funding the two year long big data public private forum through their seventh framework program to engage companies academics and other stakeholders in discussing big data issues the project aims to define a strategy in terms of research and innovation to guide supporting actions from the european commission in the successful implementation of the big data economy outcomes of this project will be used as input for horizon 2020 their next framework program the british government announced in march 2014 the founding of the alan turing institute named after the computer pioneer and code breaker which will focus on new ways to collect and analyze large data sets at the university of waterloo stratford campus canadian open data experience code inspiration day participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world computational social sciences anyone can use application programming interfaces apis provided by big data holders such as google and twitter to do research in the social and behavioral sciences often these apis are provided for free tobias preis et al used google trends data to demonstrate that internet users from countries with a higher per capita gross domestic products gdps are more likely to search for information about the future than information about the past the findings suggest there may be a link between online behaviors and real world economic indicators the authors of the study examined google queries logs made by ratio of the volume of searches for the coming year 2011 to the volume of searches for the previous year 2009 which they call the future orientation index they compared the future orientation index to the per capita gdp of each country and found a strong tendency for countries where google users inquire more about the future to have a higher gdp tobias preis and his colleagues helen susannah moat and h eugene stanley introduced a method to identify online precursors for stock market moves using trading strategies based on search volume data provided by google trends their analysis of google search volume for 98 terms of varying financial relevance published in scientific reports suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets big data sets come with algorithmic challenges that previously did not exist hence there is seen by some to be a need to fundamentally change the processing ways sampling big data a research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough the name big data itself contains a term related to size and this is an important characteristic of big data but sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population in manufacturing different types of sensory data such as acoustics vibration pressure current voltage and controller data are available at short time intervals to predict downtime it may not be necessary to look at all the data but a sample may be sufficient big data can be broken down by various data point categories such as demographic psychographic behavioral and transactional data with large sets of data points marketers are able to create and use more customized segments of consumers for more strategic targeting critique critiques of the big data paradigm come in two flavors those that question the implications of the approach itself and those that question the way it is currently done one approach to this criticism is the field of critical data studies critiques of the big data paradigm a crucial problem is that we do not know much about the underlying empirical micro processes that lead to the emergence of the typical network characteristics of big data in their critique snijders matzat and reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro processes mark graham has leveled broad critiques at chris anderson s assertion that big data will spell the end of theory focusing in particular on the notion that big data must always be contextualized in their social economic and political contexts even as companies invest eight and nine figure sums to derive insight from information streaming in from suppliers and customers less than 40 of employees have sufficiently mature processes and skills to do so to overcome this insight deficit big data no matter how comprehensive or well analyzed must be complemented by big judgment according to an article in the harvard business review much in the same line it has been pointed out that the decisions based on the analysis of big data are inevitably informed by the world as it was in the past or at best as it currently is fed by a large number of data on past experiences algorithms can predict future development if the future is similar to the past if the system s dynamics of the future change if it is not a stationary process the past can say little about the future in order to make predictions in changing environments it would be necessary to have a thorough understanding of the systems dynamic which requires theory as a response to this critique alemany oliver and vayre suggest to use abductive reasoning as a first step in the research process in order to bring context to consumers digital traces and make new theories emerge additionally it has been suggested to combine big data approaches with computer simulations such as agent based models and complex systems agent based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms finally the use of multivariate methods that probe for the latent structure of the data such as factor analysis and cluster analysis have proven useful as analytic approaches that go well beyond the bi variate approaches e g contingency tables typically employed with smaller data sets in health and biology conventional scientific approaches are based on experimentation for these approaches the limiting factor is the relevant data that can confirm or refute the initial hypothesis a new postulate is accepted now in biosciences the information provided by the data in huge volumes omics without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation in the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor the search logic is reversed and the limits of induction glory of science and philosophy scandal c d broad 1926 are to be considered privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information expert panels have released various policy recommendations to conform practice to expectations of privacy the misuse of big data in several cases by media companies and even the government has allowed for abolition of trust in almost every fundamental institution holding up society barocas and nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected with whom it is shared under what constraints and for what purposes critiques of the v model the v model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information this led to the framework of cognitive big data which characterizes big data applications according to data completeness understanding of the non obvious from data data correlation causation and predictability causality as not essential requirement to achieve predictability explainability and interpretability humans desire to understand and accept what they understand where algorithms do not cope with this level of automated decision making algorithms that support automated decision making and algorithmic self learning critiques of novelty large data sets have been analyzed by computing machines for well over a century including the us census analytics performed by ibm s punch card machines which computed statistics including means and variances of populations across the whole continent in more recent decades science experiments such as cern have produced data on similar scales to current commercial big data however science experiments have tended to analyze their data using specialized custom built high performance computing super computing clusters and grids rather than clouds of cheap commodity computers as in the current commercial wave implying a difference in both culture and technology stack critiques of big data execution ulf dietrich reips and uwe matzat wrote in 2014 that big data had become a fad in scientific research researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data this approach may lead to results that have a bias in one way or another integration across heterogeneous data resources some that might be considered big data and others not presents formidable logistical as well as analytical challenges but many researchers argue that such integrations are likely to represent the most promising new frontiers in science in the provocative article critical questions for big data the authors title big data a part of mythology large data sets offer a higher form of intelligence and knowledge with the aura of truth objectivity and accuracy users of big data are often lost in the sheer volume of numbers and working with big data is still subjective and what it quantifies does not necessarily have a closer claim on objective truth recent developments in bi domain such as pro active reporting especially target improvements in the usability of big data through automated filtering of non useful data and correlations big structures are full of spurious correlations either because of non causal coincidences law of truly large numbers solely nature of big randomness ramsey theory or existence of non included factors so the hope of early experimenters to make large databases of numbers speak for themselves and revolutionize scientific method is questioned catherine tucker has pointed to hype around big data writing by itself big data is unlikely to be valuable the article explains the many contexts where data is cheap relative to the cost of retaining talent to process it suggests that processing skills are more important than data itself in creating value for a firm big data analysis is often shallow compared to analysis of smaller data sets in many big data projects there is no large data analysis happening but the challenge is the extract transform load part of data pre processing big data is a buzzword and a vague term but at the same time an obsession with entrepreneurs consultants scientists and the media big data showcases such as google flu trends failed to deliver good predictions in recent years overstating the flu outbreaks by a factor of two similarly academy awards and election predictions solely based on twitter were more often off than on target big data often poses the same challenges as small data adding more data does not solve problems of bias but may emphasize other problems in particular data sources such as twitter are not representative of the overall population and results drawn from such sources may then lead to wrong conclusions google translate which is based on big data statistical analysis of text does a good job at translating web pages however results from specialized domains may be dramatically skewed on the other hand big data may also introduce new problems such as the multiple comparisons problem simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant ioannidis argued that most published research findings are false due to essentially the same effect when many scientific teams and researchers each perform many experiments i e process a big amount of scientific data although not with big data technology the likelihood of a significant result being false grows fast even more so when only positive results are published furthermore big data analytics results are only as good as the model on which they are predicated in an example big data took part in attempting to predict the results of the 2016 u s presidential election with varying degrees of success critiques of big data policing and surveillance big data has been used in policing and surveillance by institutions like law enforcement and corporations due to the less visible nature of data based surveillance as compared to traditional methods of policing objections to big data policing are less likely to arise according to sarah brayne s big data surveillance the case of policing big data policing can reproduce existing societal inequalities in three ways placing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm increasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system encouraging members of society to abandon interactions with institutions that would create a digital trace thus creating obstacles to social inclusion if these potential problems are not corrected or regulated the effects of big data policing may continue to shape societal hierarchies conscientious usage of big data policing could prevent individual level biases from becoming institutional biases brayne also notes see also references bibliography hilbert m 2016 big data for development a review of promises and challenges development policy review 34 1 135 74 doi 10 1111 dpr 12142 free access archived 21 april 2021 at the wayback machine snijders c matzat u reips u d 2012 big data big gaps of knowledge in the field of internet international journal of internet science 7 1 5 archived from the original on 23 november 2019 retrieved 13 april 2013 yanase j triantaphyllou e 2019 a systematic survey of computer aided diagnosis in medicine past and present developments expert systems with applications 138 112821 doi 10 1016 j eswa 2019 112821 s2cid 199019309 further reading peter kinnaird inbal talgam cohen eds 2012 big data xrds crossroads the acm magazine for students vol 19 no 1 association for computing machinery issn 1528 4980 oclc 779657714 jure leskovec anand rajaraman jeffrey d ullman 2014 mining of massive datasets cambridge university press isbn 978 1 10707723 2 oclc 888463433 viktor mayer schönberger kenneth cukier 2013 big data a revolution that will transform how we live work and think houghton mifflin harcourt isbn 978 1 29990302 9 oclc 828620988 press gil 9 may 2013 a very short history of big data forbes com jersey city nj retrieved 17 september 2016 stephens davidowitz seth 2017 everybody lies big data new data and what the internet can tell us about who we really are dey street books isbn 978 0 06239085 1 big data the management revolution harvard business review october 2012 o neil cathy 2017 weapons of math destruction how big data increases inequality and threatens democracy broadway books isbn 978 0 55341883 5 external links media related to big data at wikimedia commons the dictionary definition of big data at wiktionary",
            "total_words": 9328,
            "unique_words_percentage": 26.436535162950257,
            "stopwords_percentage": 35.51672384219554
        },
        {
            "title": "Big Data (band)",
            "link": "https://en.wikipedia.org/wiki/Big_Data_(band)",
            "content": "big data is an american electronic music project created by producer alan wilkis big data is best known for the single dangerous featuring joywave which reached number one on the billboard alternative songs chart in august 2014 and was certified gold by the riaa in may 2015 big data s first ep 1 0 was released on october 1 2013 on wilkis s own wilcassettes label and features the songs the stroke of return dangerous big dater and bombs over brooklyn in early december 2013 they also released a remix ep 1 5 which included eight remixes of the song dangerous including one by joywave another remix ep 1 6 was released in late september 2014 and included seven remixes of dangerous big data s first studio album 2 0 was released on march 20 2015 their second album 3 0 was released on july 26 2019 discography studio albums singles as lead artist promotional singles extended plays references external links official website",
            "total_words": 163,
            "unique_words_percentage": 61.963190184049076,
            "stopwords_percentage": 27.607361963190186
        },
        {
            "title": "Data",
            "link": "https://en.wikipedia.org/wiki/Data",
            "content": "data day tə us also dat ə are a collection of discrete or continuous values that convey information describing the quantity quality fact statistics other basic units of meaning or simply sequences of symbols that may be further interpreted formally a datum is an individual value in a collection of data data are usually organized into structures such as tables that provide additional context and meaning and may themselves be used as data in larger structures data may be used as variables in a computational process data may represent abstract ideas or concrete measurements data are commonly used in scientific research economics and virtually every other form of human organizational activity examples of data sets include price indices such as the consumer price index unemployment rates literacy rates and census data in this context data represent the raw facts and figures from which useful information can be extracted data are collected using techniques such as measurement observation query or analysis and are typically represented as numbers or characters that may be further processed field data are data that are collected in an uncontrolled in situ environment experimental data are data that are generated in the course of a controlled scientific experiment data are analyzed using techniques such as calculation reasoning discussion presentation visualization or other forms of post analysis prior to analysis raw data or unprocessed data is typically cleaned outliers are removed and obvious instrument or data entry errors are corrected data can be seen as the smallest units of factual information that can be used as a basis for calculation reasoning or discussion data can range from abstract ideas to concrete measurements including but not limited to statistics thematically connected data presented in some relevant context can be viewed as information contextually connected pieces of information can then be described as data insights or intelligence the stock of insights and intelligence that accumulate over time resulting from the synthesis of data into information can then be described as knowledge data has been described as the new oil of the digital economy data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing advances in computing technologies have led to the advent of big data which usually refers to very large quantities of data usually at the petabyte scale using traditional data analysis methods and computing working with such large and growing datasets is difficult even impossible theoretically speaking infinite data would yield infinite information which would render extracting insights or intelligence impossible in response the relatively new field of data science uses machine learning and other artificial intelligence ai methods that allow for efficient applications of analytic methods to big data etymology and terminology the latin word data is the plural of datum thing given and the neuter past participle of dare to give the first english use of the word data is from the 1640s the word data was first used to mean transmissible and storable computer information in 1946 the expression data processing was first used in 1954 when data is used more generally as a synonym for information it is treated as a mass noun in singular form this usage is common in everyday language and in technical and scientific fields such as software development and computer science one example of this usage is the term big data when used more specifically to refer to the processing and analysis of sets of data the term retains its plural form this usage is common in the natural sciences life sciences social sciences software development and computer science and grew in popularity in the 20th and 21st centuries some style guides do not recognize the different meanings of the term and simply recommend the form that best suits the target audience of the guide for example apa style as of the 7th edition requires data to be treated as a plural form meaning data information knowledge and wisdom are closely related concepts but each has its role concerning the other and each term has its meaning according to a common view data is collected and analyzed data only becomes information suitable for making decisions once it has been analyzed in some fashion one can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person the amount of information contained in a data stream may be characterized by its shannon entropy knowledge is the awareness of its environment that some entity possesses whereas data merely communicates that knowledge for example the entry in a database specifying the height of mount everest is a datum that communicates a precisely measured value this measurement may be included in a book along with other data on mount everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it awareness of the characteristics represented by this data is knowledge data are often assumed to be the least abstract concept information the next least and knowledge the most abstract in this view data becomes information by interpretation e g the height of mount everest is generally considered data a book on mount everest geological characteristics may be considered information and a climber s guidebook containing practical information on the best way to reach mount everest s peak may be considered knowledge information bears a diversity of meanings that range from everyday usage to technical use this view however has also been argued to reverse how data emerges from information and information from knowledge generally speaking the concept of information is closely related to notions of constraint communication control data form instruction knowledge meaning mental stimulus pattern perception and representation beynon davies uses the concept of a sign to differentiate between data and information data is a series of symbols while information occurs when the symbols are used to refer to something before the development of computing devices and machines people had to manually collect data and impose patterns on it with the development of computing devices and machines these devices can also collect data in the 2010s computers were widely used in many fields to collect data and sort or process it in disciplines ranging from marketing analysis of social service usage by citizens to scientific research these patterns in the data are seen as information that can be used to enhance knowledge these patterns may be interpreted as truth though truth can be a subjective concept and may be authorized as aesthetic and ethical criteria in some disciplines or cultures events that leave behind perceivable physical or virtual remains can be traced back through data marks are no longer considered data once the link between the mark and observation is broken mechanical computing devices are classified according to how they represent data an analog computer represents a datum as a voltage distance position or other physical quantity a digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet the most common digital computers use a binary alphabet that is an alphabet of two characters typically denoted 0 and 1 more familiar representations such as numbers or letters are then constructed from the binary alphabet some special forms of data are distinguished a computer program is a collection of data that can be interpreted as instructions most computer languages make a distinction between programs and the other data on which programs operate but in some languages notably lisp and similar languages programs are essentially indistinguishable from other data it is also useful to distinguish metadata that is a description of other data a similar yet earlier term for metadata is ancillary data the prototypical example of metadata is the library catalog which is a description of the contents of books data documents whenever data needs to be registered data exists in the form of a data document kinds of data documents include data repository data study data set software data paper database data handbook data journal some of these data documents data repositories data studies data sets and software are indexed in data citation indexes while data papers are indexed in traditional bibliographic databases e g science citation index data collection gathering data can be accomplished through a primary source the researcher is the first person to obtain the data or a secondary source the researcher obtains the data that has already been collected by other sources such as data disseminated in a scientific journal data analysis methodologies vary and include data triangulation and data percolation the latter offers an articulate method of collecting classifying and analyzing data using five possible angles of analysis at least three to maximize the research s objectivity and permit an understanding of the phenomena under investigation as complete as possible qualitative and quantitative methods literature reviews including scholarly articles interviews with experts and computer simulation the data is thereafter percolated using a series of pre determined steps so as to extract the most relevant information data longevity and accessibility an important field in computer science technology and library science is the longevity of data scientific research generates huge amounts of data especially in genomics and astronomy but also in the medical sciences e g in medical imaging in the past scientific data has been published in papers and books stored in libraries but more recently practically all data is stored on hard drives or optical discs however in contrast to paper these storage devices may become unreadable after a few decades scientific publishers and libraries have been struggling with this problem for a few decades and there is still no satisfactory solution for the long term storage of data over centuries or even for eternity data accessibility another problem is that much scientific data is never published or deposited in data repositories such as databases in a recent survey data was requested from 516 studies that were published between 2 and 22 years earlier but less than one out of five of these studies were able or willing to provide the requested data overall the likelihood of retrieving data dropped by 17 each year after publication similarly a survey of 100 datasets in dryad found that more than half lacked the details to reproduce the research results from these studies this shows the dire situation of access to scientific data that is not published or does not have enough details to be reproduced a solution to the problem of reproducibility is the attempt to require fair data that is data that is findable accessible interoperable and reusable data that fulfills these requirements can be used in subsequent research and thus advances science and technology in other fields although data is also increasingly used in other fields it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as given peter checkland introduced the term capta from the latin capere to take to distinguish between an immense number of possible data and a sub set of them to which attention is oriented johanna drucker has argued that since the humanities affirm knowledge production as situated partial and constitutive using data may introduce assumptions that are counterproductive for example that phenomena are discrete or are observer independent the term capta which emphasizes the act of observation as constitutive is offered as an alternative to data for visual representations in the humanities the term data driven is a neologism applied to an activity which is primarily compelled by data over all other factors data driven applications include data driven programming and data driven journalism see also references external links data is a singular noun a detailed assessment ",
            "total_words": 1967,
            "unique_words_percentage": 35.43467208947636,
            "stopwords_percentage": 41.73868835790544
        },
        {
            "title": "Data science",
            "link": "https://en.wikipedia.org/wiki/Data_science",
            "content": "data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processing scientific visualization algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy structured or unstructured data data science also integrates domain knowledge from the underlying application domain e g natural sciences information technology and medicine data science is multifaceted and can be described as a science a research paradigm a research method a discipline a workflow and a profession data science is a concept to unify statistics data analysis informatics and their related methods to understand and analyze actual phenomena with data it uses techniques and theories drawn from many fields within the context of mathematics statistics computer science information science and domain knowledge however data science is different from computer science and information science turing award winner jim gray imagined data science as a fourth paradigm of science empirical theoretical computational and now data driven and asserted that everything about science is changing because of the impact of information technology and the data deluge a data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data foundations data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains the field encompasses preparing data for analysis formulating data science problems analyzing data developing data driven solutions and presenting findings to inform high level decisions in a broad range of application domains as such it incorporates skills from computer science statistics information science mathematics data visualization information visualization data sonification data integration graphic design complex systems communication and business statistician nathan yau drawing on ben fry also links data science to human computer interaction users should be able to intuitively control and explore data in 2015 the american statistical association identified database management statistics and machine learning and distributed and parallel systems as the three emerging foundational professional communities relationship to statistics many statisticians including nate silver have argued that data science is not a new field but rather another name for statistics others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data vasant dhar writes that statistics emphasizes quantitative data and description in contrast data science deals with quantitative and qualitative data e g from images text sensors transactions customer information etc and emphasizes prediction and action andrew gelman of columbia university has described statistics as a non essential part of data science stanford professor david donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program he describes data science as an applied field growing out of traditional statistics etymology early usage in 1962 john tukey described a field he called data analysis which resembles modern data science in 1985 in a lecture given to the chinese academy of sciences in beijing c f jeff wu used the term data science for the first time as an alternative name for statistics later attendees at a 1992 statistics symposium at the university of montpellier ii acknowledged the emergence of a new discipline focused on data of various origins and forms combining established concepts and principles of statistics and data analysis with computing the term data science has been traced back to 1974 when peter naur proposed it as an alternative name to computer science in 1996 the international federation of classification societies became the first conference to specifically feature data science as a topic however the definition was still in flux after the 1985 lecture at the chinese academy of sciences in beijing in 1997 c f jeff wu again suggested that statistics should be renamed data science he reasoned that a new name would help statistics shed inaccurate stereotypes such as being synonymous with accounting or limited to describing data in 1998 hayashi chikio argued for data science as a new interdisciplinary concept with three aspects data design collection and analysis during the 1990s popular terms for the process of finding patterns in datasets which were increasingly large included knowledge discovery and data mining modern usage in 2012 technologists thomas h davenport and dj patil declared data scientist the sexiest job of the 21st century a catchphrase that was picked up even by major city newspapers like the new york times and the boston globe a decade later they reaffirmed it stating that the job is more in demand than ever with employers the modern conception of data science as an independent discipline is sometimes attributed to william s cleveland in a 2001 paper he advocated an expansion of statistics beyond theory into technical areas because this would significantly change the field it warranted a new name data science became more widely used in the next few years in 2002 the committee on data for science and technology launched the data science journal in 2003 columbia university launched the journal of data science in 2014 the american statistical association s section on statistical learning and data mining changed its name to the section on statistical learning and data science reflecting the ascendant popularity of data science the professional title of data scientist has been attributed to dj patil and jeff hammerbacher in 2008 though it was used by the national science board in their 2005 report long lived digital data collections enabling research and education in the 21st century it referred broadly to any key role in managing a digital data collection there is still no consensus on the definition of data science and it is considered by some to be a buzzword big data is a related marketing term data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations data science and data analysis data science and data analysis are both important disciplines in the field of data management and analysis but they differ in several key ways while both fields involve working with data data science is more of an interdisciplinary field that involves the application of statistical computational and machine learning methods to extract insights from data and make predictions while data analysis is more focused on the examination and interpretation of data to identify patterns and trends data analysis typically involves working with smaller structured datasets to answer specific questions or solve specific problems this can involve tasks such as data cleaning data visualization and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data for example a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies data science on the other hand is a more complex and iterative process that involves working with larger more complex datasets that often require advanced computational and statistical methods to analyze data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data driven decisions in addition to statistical analysis data science often involves tasks such as data preprocessing feature engineering and model selection for instance a data scientist might develop a recommendation system for an e commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences while data analysis focuses on extracting insights from existing data data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions data scientists are often responsible for collecting and cleaning data selecting appropriate analytical techniques and deploying models in real world scenarios they work at the intersection of mathematics computer science and domain expertise to solve complex problems and uncover hidden patterns in large datasets despite these differences data science and data analysis are closely related fields and often require similar skill sets both fields require a solid foundation in statistics programming and data visualization as well as the ability to communicate findings effectively to both technical and non technical audiences both fields benefit from critical thinking and domain knowledge as understanding the context and nuances of the data is essential for accurate analysis and modeling in summary data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis data analysis focuses on extracting insights and drawing conclusions from structured data while data science involves a more comprehensive approach that combines statistical analysis computational methods and machine learning to extract insights build predictive models and drive data driven decision making both fields use data to understand patterns make informed decisions and solve complex problems across various domains data science as an academic discipline as illustrated in the previous sections there is substantially some considerable differences between data science data analysis and statistics consequently just like statistics grew into an independent field from applied mathematics similarly data science has emerged as a independent field and has gained traction over the recent years the unique demand for professional skills on computerized data analysis skills has exploded due to the increasing amounts of data emanating from a variety of independent sources whereas some of these highly sought skills can be provided by statisticians the lack of high algorithmic writing skills makes them less preferred than trained data scientists who provide unique expertise on skills such as nosql apache hadoop cloud computing platforms and use of complex networks this paradigm shift has seen various institution craft academic programmes to prepare skilled labor for the market some of the institutions offering degree programmes in data science include stanford university harvard university university of oxford eth zurich meru university among many others cloud computing for data science cloud computing can offer access to large amounts of computational power and storage in big data where volumes of information are continually generated and processed these platforms can be used to handle complex and resource intensive analytical tasks some distributed computing frameworks are designed to handle big data workloads these frameworks can enable data scientists to process and analyze large datasets in parallel which can reducing processing times ethical consideration in data science data science involve collecting processing and analyzing data which often including personal and sensitive information ethical concerns include potential privacy violations bias perpetuation and negative societal impacts machine learning models can amplify existing biases present in training data leading to discriminatory or unfair outcomes see also python programming language r programming language data engineering big data machine learning references ",
            "total_words": 1799,
            "unique_words_percentage": 36.68704836020011,
            "stopwords_percentage": 33.407448582545854
        },
        {
            "title": "Big data ethics",
            "link": "https://en.wikipedia.org/wiki/Big_data_ethics",
            "content": "big data ethics also known simply as data ethics refers to systemizing defending and recommending concepts of right and wrong conduct in relation to data in particular personal data since the dawn of the internet the sheer quantity and quality of data has dramatically increased and is continuing to do so exponentially big data describes this large amount of data that is so voluminous and complex that traditional data processing application software is inadequate to deal with them recent innovations in medical research and healthcare such as high throughput genome sequencing high resolution imaging electronic medical patient records and a plethora of internet connected health devices have triggered a data deluge that will reach the exabyte range in the near future data ethics is of increasing relevance as the quantity of data increases because of the scale of the impact big data ethics are different from information ethics because the focus of information ethics is more concerned with issues of intellectual property and concerns relating to librarians archivists and information professionals while big data ethics is more concerned with collectors and disseminators of structured or unstructured data such as data brokers governments and large corporations however since artificial intelligence or machine learning systems are regularly built using big data sets the discussions surrounding data ethics are often intertwined with those in the ethics of artificial intelligence more recently issues of big data ethics have also been researched in relation with other areas of technology and science ethics including ethics in mathematics and engineering ethics as many areas of applied mathematics and engineering use increasingly large data sets principles data ethics is concerned with the following principles ownership individuals own their personal data transaction transparency if an individual s personal data is used they should have transparent access to the algorithm design used to generate aggregate data sets consent if an individual or legal entity would like to use personal data one needs informed and explicitly expressed consent of what personal data moves to whom when and for what purpose from the owner of the data privacy if data transactions occur all reasonable effort needs to be made to preserve privacy currency individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions openness aggregate data sets should be freely available ownership ownership of data involves determining rights and duties over property such as the ability to exercise individual control over including limit the sharing of personal data comprising one s digital identity the question of data ownership arises when someone records observations on an individual person the observer and the observed both state a claim to the data questions also arise as to the responsibilities that the observer and the observed have in relation to each other these questions have become increasingly relevant with the internet magnifying the scale and systematization of observing people and their thoughts the question of personal data ownership relates to questions of corporate ownership and intellectual property in the european union some people argue that the general data protection regulation indicates that individuals own their personal data although this is contested transaction transparency concerns have been raised around how biases can be integrated into algorithm design resulting in systematic oppressionwhether consciously or unconsciously these manipulations often stem from biases in the data the design of the algorithm or the underlying goals of the organization deploying them one major cause of algorithmic bias is that algorithms learn from historical data which may perpetuate existing inequities in many cases algorithms exhibit reduced accuracy when applied to individuals from marginalized or underrepresented communities a notable example of this is pulse oximetry which has shown reduced reliability for certain demographic groups due to a lack of sufficient testing or information on these populations additionally many algorithms are designed to maximize specific metrics such as engagement or profit without adequately considering ethical implications for instance companies like facebook and twitter have been criticized for providing anonymity to harassers and for allowing racist content disguised as humor to proliferate as such content often increases engagement these challenges are compounded by the fact that many algorithms operate as black boxes for proprietary reasons meaning that the reasoning behind their outputs is not fully understood by users this opacity makes it more difficult to identify and address algorithmic bias in terms of governance big data ethics is concerned with which types of inferences and predictions should be made using big data technologies such as algorithms anticipatory governance is the practice of using predictive analytics to assess possible future behaviors this has ethical implications because it affords the ability to target particular groups and places which can encourage prejudice and discrimination for example predictive policing highlights certain groups or neighborhoods which should be watched more closely than others which leads to more sanctions in these areas and closer surveillance for those who fit the same profiles as those who are sanctioned the term control creep refers to data that has been generated with a particular purpose in mind but which is repurposed this practice is seen with airline industry data which has been repurposed for profiling and managing security risks at airports privacy privacy has been presented as a limitation to data usage which could also be considered unethical for example the sharing of healthcare data can shed light on the causes of diseases the effects of treatments an can allow for tailored analyses based on individuals needs this is of ethical significance in the big data ethics field because while many value privacy the affordances of data sharing are also quite valuable although they may contradict one s conception of privacy attitudes against data sharing may be based in a perceived loss of control over data and a fear of the exploitation of personal data however it is possible to extract the value of data without compromising privacy government surveillance of big data has the potential to undermine individual privacy by collecting and storing data on phone calls internet activity and geolocation among other things for example the nsa s collection of metadata exposed in global surveillance disclosures raised concerns about whether privacy was adequately protected even when the content of communications was not analyzed the right to privacy is often complicated by legal frameworks that grant governments broad authority over data collection for national security purposes in the united states the supreme court has not recognized a general right to informational privacy or control over personal information though legislators have addressed the issue selectively through specific statutes from an equity perspective government surveillance and privacy violations tend to disproportionately harm marginalized communities historically activists involved in the civil rights movement were frequently targets of government surveillance as they were perceived as subversive elements programs such as cointelpro exemplified this pattern involving espionage against civil rights leaders this pattern persists today with evidence of ongoing surveillance of activists and organizations additionally the use of algorithms by governments to act on data obtained without consent introduces significant concerns about algorithmic bias predictive policing tools for example utilize historical crime data to predict risky areas or individuals but these tools have been shown to disproportionately target minority communities one such tool the compas system is a notable example black defendants are twice as likely to be misclassified as high risk compared to white defendants and hispanic defendants are similarly more likely to be classified as high risk than their white counterparts marginalized communities often lack the resources or education needed to challenge these privacy violations or protect their data from nonconsensual use furthermore there is a psychological toll known as the chilling effect where the constant awareness of being surveilled disproportionately impacts communities already facing societal discrimination this effect can deter individuals from engaging in legal but potentially risky activities such as protesting or seeking legal assistance further limiting their freedoms and exacerbating existing inequities some scholars such as jonathan h king and neil m richards are redefining the traditional meaning of privacy and others to question whether or not privacy still exists in a 2014 article for the wake forest law review king and richard argue that privacy in the digital age can be understood not in terms of secrecy but in term of regulations which govern and control the use of personal information in the european union the right to be forgotten entitles eu countries to force the removal or de linking of personal data from databases at an individual s request if the information is deemed irrelevant or out of date according to andrew hoskins this law demonstrates the moral panic of eu members over the perceived loss of privacy and the ability to govern personal data in the digital age in the united states citizens have the right to delete voluntarily submitted data this is very different from the right to be forgotten because much of the data produced using big data technologies and platforms are not voluntarily submitted while traditional notions of privacy are under scrutiny different legal frameworks related to privacy in the eu and us demonstrate how countries are grappling with these concerns in the context of big data for example the right to be forgotten in the eu and the right to delete voluntarily submitted data in the us illustrate the varying approaches to privacy regulation in the digital age how much data is worth the difference in value between the services facilitated by tech companies and the equity value of these tech companies is the difference in the exchange rate offered to the citizen and the market rate of the value of their data scientifically there are many holes in this rudimentary calculation the financial figures of tax evading companies are unreliable either revenue or profit could be more appropriate how a user is defined a large number of individuals are needed for the data to be valuable possible tiered prices for different people in different countries etc although these calculations are crude they serve to make the monetary value of data more tangible another approach is to find the data trading rates in the black market rsa publishes a yearly cybersecurity shopping list that takes this approach this raises the economic question of whether free tech services in exchange for personal data is a worthwhile implicit exchange for the consumer in the personal data trading model rather than companies selling data an owner can sell their personal data and keep the profit openness the idea of open data is centered around the argument that data should be freely available and should not have restrictions that would prohibit its use such as copyright laws as of 2014 many governments had begun to move towards publishing open datasets for the purpose of transparency and accountability this movement has gained traction via open data activists who have called for governments to make datasets available to allow citizens to themselves extract meaning from the data and perform checks and balances themselves king and richards have argued that this call for transparency includes a tension between openness and secrecy activists and scholars have also argued that because this open sourced model of data evaluation is based on voluntary participation the availability of open datasets has a democratizing effect on a society allowing any citizen to participate to some the availability of certain types of data is seen as a right and an essential part of a citizen s agency open knowledge foundation okf lists several dataset types it argues should be provided by governments for them to be truly open okf has a tool called the global open data index godi a crowd sourced survey for measuring the openness of governments based on its open definition godi aims to be a tool for providing feedback to governments about the quality of their open datasets willingness to share data varies from person to person preliminary studies have been conducted into the determinants of the willingness to share data for example some have suggested that baby boomers are less willing to share data than millennials historical cases snowden disclosures the fallout from edward snowden s disclosures in 2013 significantly reshaped public discourse around data collection and the privacy principle of big data ethics the case revealed that governments controlled and possessed far more information about civilians than previously understood violating the principle of ownership particularly in ways that disproportionately affected disadvantaged communities for instance activists were frequently targeted including members of movements such as occupy wall street and black lives matter this revelation prompted governments and organizations to revisit data collection and storage practices to better protect individual privacy while also addressing national security concerns the case also exposed widespread online surveillance of other countries and their citizens raising important questions about data sovereignty and ownership in response some countries such as brazil and germany took action to push back against these practices however many developing nations lacked the technological independence necessary or were too generally dependent on the nations surveilling them to resist such surveillance leaving them at a disadvantage in addressing these concerns cambridge analytica scandal the cambridge analytica scandal highlighted significant ethical concerns in the use of big data data was harvested from approximately 87 million facebook users without their explicit consent and used to display targeted political advertisements this violated the currency principle of big data ethics as individuals were initially unaware of how their data was being exploited the scandal revealed how data collected for one purpose could be repurposed for entirely different uses bypassing users consent and emphasizing the need for explicit and informed consent in data usage additionally the algorithms used for ad delivery were opaque challenging the principles of transaction transparency and openness in some cases the political ads spread misinformation often disproportionately targeting disadvantaged groups and contributing to knowledge gaps marginalized communities and individuals with lower digital literacy were disproportionately affected as they were less likely to recognize or act against exploitation in contrast users with more resources or digital literacy could better safeguard their data exacerbating existing power imbalances footnotes references ",
            "total_words": 2349,
            "unique_words_percentage": 35.759897828863345,
            "stopwords_percentage": 40.01702852277565
        },
        {
            "title": "Data management",
            "link": "https://en.wikipedia.org/wiki/Data_management",
            "content": "data management comprises all disciplines related to handling data as a valuable resource it is the practice of managing an organization s data so it can be analyzed for decision making concept the concept of data management arose in the 1980s as technology moved from sequential processing first punched cards then magnetic tape to random access storage since it was now possible to store a discrete fact and quickly access it using random access disk technology those suggesting that data management was more important than business process management used arguments such as a customer s home address is stored in 75 or some other large number places in our computer systems however during this period random access processing was not competitively fast so those suggesting process management was more important than data management used batch processing time as their primary argument as application software evolved into real time interactive usage it became obvious that both management processes were important if the data was not well defined the data would be mis used in applications if the process wasn t well defined it was impossible to meet user needs patterns followings are common data management patterns cache aside command query responsibility segregation cqrs event sourcing index table materialized view sharding valet key static content hosting topics topics in data management include usage in modern management usage the term data is increasingly replaced by information or even knowledge in a non technical context thus data management has become information management or knowledge management this trend obscures the raw data processing and renders interpretation implicit the distinction between data and derived value is illustrated by the information ladder however data has staged a comeback with the popularisation of the term big data which refers to the collection and analyses of massive sets of data while big data is a recent phenomenon the requirement for data to aid decision making traces back to the early 1970s with the emergence of decision support systems dss these systems can be considered as the initial iteration of data management for decision support several organisations have established data management centers dmc for their operations data sources marketers and marketing organizations have been using data collection and analysis to refine their operations for the last few decades marketing departments in organizations and marketing companies conduct data collection and analysis by collecting data from different data sources and analyzing them to come up with insightful data they can use for strategic decision making baier et al 2012 in the modern business environment data has evolved into a crucial asset for businesses since businesses use data as a strategic asset that is used regularly to create a competitive advantage and improve customer experiences among the most significant forms of data is customer information which is a critical asset used to assess customer behavior and trends and use it for developing new strategies for improving customer experience ahmed 2004 however data has to be of high quality to be used as a business asset for creating a competitive advantage therefore data governance is a critical element of data collection and analysis since it determines the quality of data while integrity constraints guarantee the reliability of information collected from data sources various technologies including big data are used by businesses and organizations to allow users to search for specific information from raw data by grouping it based on the preferred criteria marketing departments in organizations could apply for developing targeted marketing strategies ahmed 2004 as technology evolves new forms of data are being introduced for analysis and classification purposes in marketing organizations and businesses the introduction of new gadgets such as smartphones and new generation pcs has also introduced new data sources from which organizations can collect analyze and classify data when developing marketing strategies retail businesses are the business category that uses customer data from smart devices and websites to understand how their current and targeted customers perceive their services before using the information to make improvements and increase customer satisfaction cerchiello and guidici 2012 analyzing customer data is crucial for businesses since it allows marketing teams to understand customer behavior and trends which makes a considerable difference during the development of new marketing campaigns and strategies retailers who use customer data from various sources gain an advantage in the market since they can develop data informed strategies for attracting and retaining customers in the overly competitive business environment based on the information on the benefits of data collection and analysis the following hypotheses are proposed the sources of data used as the foundation of data collection and analysis have a considerable impact on the data analysis tools used for analyzing and categorizing data data analysis tools organizations use various data analysis tools for discovering unknown information and insights from huge databases this allows organizations to discover new patterns that were not known to them or extract buried information before using it to come up with new patterns and relationships ahmed 2004 there are 2 main categories of data analysis tools data mining tools and data profiling tools also most commercial data analysis tools are used by organizations for extracting transforming and loading etl for data warehouses in a manner that ensures no element is left out during the process turban et al 2008 thus the data analysis tools are used for supporting the 3 vs in big data volume variety and velocity factor velocity emerged in the 1980s as one of the most important procedures in data analysis tools which was widely used by organizations for market research the tools used to select core variables from the data that was collected from various sources and analyzed it if the amount of data used to be too huge for humans to understand via manual observation factor analysis would be introduced to distinguish between qualitative and quantitative data stewart 1981 organizations collect data from numerous sources including websites emails and customer devices before conducting data analysis collecting data from numerous sources and analyzing it using different data analysis tools has its advantages including overcoming the risk of method bias using data from different sources and analyzing it using multiple analysis methods guarantees businesses and organizations robust and reliable findings they can use in decision making on the other hand researchers use modern technologies to analyze and group data collected from respondents in the form of images audio and video files by applying algorithms and other analysis software berry et al 1997 researchers and marketers can then use the information obtained from the new generation analysis tools and methods for forecasting decision support and making estimations for decision making for instance information from different data sources on demand forecasts can help a retail business determine the amount of stock required in an upcoming season depending on data from previous seasons the analysis can allow organizations to make data informed decisions to gain competitive advantage in an era where all businesses and organizations are capitalizing on emerging technologies and business intelligence tools to gain competitive edges while there are numerous analysis tools in the market big data analytics is the most common and advanced technology that has led to the following hypothesis data analytic tools used to analyze data collected from numerous data sources determine the quality and reliability of data analysis data security and data privacy while organizations need to use quality data collection and analysis tools to guarantee the quality and reliability of the customer data they collect they must implement security and privacy strategies to protect the data and customer information from privacy leaks van till 2013 a study conducted by pwc indicated that more than two thirds of retail customers prefer purchasing products and services from businesses that have data protection and privacy plans for protecting customer information also the study indicated that customers trust businesses that can prove they cannot use customer data for any other purposes other than marketing as technology and the internet continue improving the success of businesses using it as a platform for marketing their products will depend on how effectively they can gain and maintain the trust of customers and users therefore businesses will have to introduce and implement effective data protection and privacy strategies to protect business data and customer privacy although developing trust between customers and businesses affects the customers purchasing intentions it also has a considerable impact on long term purchasing behaviors including how frequently customers purchase which could impact the profitability of a business in the long run thus the above information leads to the following hypothesis implementing data security and privacy plans has a positive impact on economic and financial outcomes financial and economic outcomes studies indicate that customer transactions account for a 40 increase in the data collected annually which means that financial data has a considerable impact on business decisions therefore modern organizations are using big data analytics to identify 5 to 10 new data sources that can help them collect and analyze data for improved decision making jonsen 2013 explains that organizations using average analytics technologies are 20 more likely to gain higher returns compared to their competitors who have not introduced any analytics capabilities in their operations also iri reported that the retail industry could experience an increase of more than 10 billion each year resulting from the implementation of modern analytics technologies therefore the following hypothesis can be proposed economic and financial outcomes can impact how organizations use data analytics tools see also references external links media related to data management at wikimedia commons",
            "total_words": 1589,
            "unique_words_percentage": 33.92070484581498,
            "stopwords_percentage": 36.500943989930775
        },
        {
            "title": "List of big data companies",
            "link": "https://en.wikipedia.org/wiki/List_of_big_data_companies",
            "content": "this is an alphabetical list of notable it companies using the marketing term big data a alpine data labs an analytics interface working with apache hadoop and big data avocadata a two sided marketplace allowing consumers to buy sell data with ease azure data lake is a highly scalable data storage and analytics service the service is hosted in azure microsoft s public cloud b big data partnership a professional services company based in london big data scoring a cloud based service that lets consumer lenders improve loan quality and acceptance rates through the use of big data bigpanda a technology company headquartered in mountain view california bright computing developer of software for deploying and managing high performance hpc clusters big data clusters and openstack in data centers and in the cloud c clarivate analytics a global company that owns and operates a collection of subscription based services focused largely on analytics cloudera an american based software company that provides apache hadoop based software support and services and training to business customers compuverde an it company with a focus on big data storage cvidya a provider of big data analytics products for communications and digital service providers d databricks a company founded by the creators of apache spark dataiku a french computer software company datastax domo f fluentd g greenplum groundhog technologies h hack reduce hazelcast hortonworks hpcc systems i ibm imply corporation m mapr marklogic medio medopad n netapp o oracle cloud platform p palantir technologies pentaho a data integration and business analytics company with an enterprise class open source based platform for big data deployments pitney bowes platfora q qumulo r rocket fuel inc s sap se offers the sap data hub to connect data bases and other products through acquisition of altiscale salesforceiq scylladb developer of a database for big data sense networks shanghai data exchange sk telecom developer of big data analytics platform metatron discovery sojern splunk sumo logic t teradata thetaray tubemogul v volometrix z zaloni deployment and vendor agnostic data lake management platform zoomdata references ",
            "total_words": 341,
            "unique_words_percentage": 59.82404692082112,
            "stopwords_percentage": 24.926686217008797
        },
        {
            "title": "Data lake",
            "link": "https://en.wikipedia.org/wiki/Data_lake",
            "content": "a data lake is a system or repository of data stored in its natural raw format usually object blobs or files a data lake is usually a single store of data including raw copies of source system data sensor data social data etc and transformed data used for tasks such as reporting visualization advanced analytics and machine learning a data lake can include structured data from relational databases rows and columns semi structured data csv logs xml json unstructured data emails documents pdfs and binary data images audio video a data lake can be established on premises within an organization s data centers or in the cloud using cloud services background james dixon then chief technology officer at pentaho coined the term by 2011 to contrast it with data mart which is a smaller repository of interesting attributes derived from raw data in promoting data lakes he argued that data marts have several inherent problems such as information siloing pricewaterhousecoopers pwc said that data lakes could put an end to data silos in their study on data lakes they noted that enterprises were starting to extract and place data for analytics into a single hadoop based repository examples many companies use cloud storage services such as google cloud storage and amazon s3 or a distributed file system such as apache hadoop distributed file system hdfs there is a gradual academic interest in the concept of data lakes for example personal datalake at cardiff university is a new type of data lake which aims at managing big data of individual users by providing a single point of collecting organizing and sharing personal data early data lakes such as hadoop 1 0 had limited capabilities because it only supported batch oriented processing map reduce interacting with it required expertise in java map reduce and higher level tools like apache pig apache spark and apache hive which were also originally batch oriented criticism poorly managed data lakes have been facetiously called data swamps in june 2015 david needle characterized so called data lakes as one of the more controversial ways to manage big data pwc was also careful to note in their research that not all data lake initiatives are successful they quote sean martin cto of cambridge semantics we see customers creating big data graveyards dumping everything into hadoop distributed file system hdfs and hoping to do something with it down the road but then they just lose track of what s there the main challenge is not creating a data lake but taking advantage of the opportunities it presents they describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization another criticism is that the term data lake is not useful because it is used in so many different ways it may be used to refer to for example any tools or data management practices that are not data warehouses a particular technology for implementation a raw data reservoir a hub for etl offload or a central hub for self service analytics while critiques of data lakes are warranted in many cases they apply to other data projects as well for example the definition of data warehouse is also changeable and not all data warehouse efforts have been successful in response to various critiques mckinsey noted that the data lake should be viewed as a service model for delivering business value within the enterprise not a technology outcome data lakehouses data lakehouses are a hybrid approach that can ingest a variety of raw data formats like a data lake yet provide acid transactions and enforce data quality like a data warehouse a data lakehouse architecture attempts to address several criticisms of data lakes by adding data warehouse capabilities such as transaction support schema enforcement governance and support for diverse workloads according to oracle data lakehouses combine the flexible storage of unstructured data from a data lake and the management features and tools from data warehouses see also azure data lake references ",
            "total_words": 675,
            "unique_words_percentage": 48.148148148148145,
            "stopwords_percentage": 35.7037037037037
        },
        {
            "title": "Data mining",
            "link": "https://en.wikipedia.org/wiki/Data_mining",
            "content": "data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning statistics and database systems data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information with intelligent methods from a data set and transforming the information into a comprehensible structure for further use data mining is the analysis step of the knowledge discovery in databases process or kdd aside from the raw analysis step it also involves database and data management aspects data pre processing model and inference considerations interestingness metrics complexity considerations post processing of discovered structures visualization and online updating the term data mining is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data not the extraction mining of data itself it also is a buzzword and is frequently applied to any form of large scale data or information processing collection extraction warehousing analysis and statistics as well as any application of computer decision support system including artificial intelligence e g machine learning and business intelligence often the more general terms large scale data analysis and analytics or when referring to actual methods artificial intelligence and machine learning are more appropriate the actual data mining task is the semi automatic or automatic analysis of large quantities of data to extract previously unknown interesting patterns such as groups of data records cluster analysis unusual records anomaly detection and dependencies association rule mining sequential pattern mining this usually involves using database techniques such as spatial indices these patterns can then be seen as a kind of summary of the input data and may be used in further analysis or for example in machine learning and predictive analytics for example the data mining step might identify multiple groups in the data which can then be used to obtain more accurate prediction results by a decision support system neither the data collection data preparation nor result interpretation and reporting is part of the data mining step although they do belong to the overall kdd process as additional steps the difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset e g analyzing the effectiveness of a marketing campaign regardless of the amount of data in contrast data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data the related terms data dredging data fishing and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are or may be too small for reliable statistical inferences to be made about the validity of any patterns discovered these methods can however be used in creating new hypotheses to test against the larger data populations etymology in the 1960s statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a priori hypothesis the term data mining was used in a similarly critical way by economist michael lovell in an article published in the review of economic studies in 1983 lovell indicates that the practice masquerades under a variety of aliases ranging from experimentation positive to fishing or snooping negative the term data mining appeared around 1990 in the database community with generally positive connotations for a short time in 1980s the phrase database mining was used but since it was trademarked by hnc a san diego based company to pitch their database mining workstation researchers consequently turned to data mining other terms used include data archaeology information harvesting information discovery knowledge extraction etc gregory piatetsky shapiro coined the term knowledge discovery in databases for the first workshop on the same topic kdd 1989 and this term became more popular in the ai and machine learning communities however the term data mining became more popular in the business and press communities currently the terms data mining and knowledge discovery are used interchangeably background the manual extraction of patterns from data has occurred for centuries early methods of identifying patterns in data include bayes theorem 1700s and regression analysis 1800s the proliferation ubiquity and increasing power of computer technology have dramatically increased data collection storage and manipulation ability as data sets have grown in size and complexity direct hands on data analysis has increasingly been augmented with indirect automated data processing aided by other discoveries in computer science specially in the field of machine learning such as neural networks cluster analysis genetic algorithms 1950s decision trees and decision rules 1960s and support vector machines 1990s data mining is the process of applying these methods with the intention of uncovering hidden patterns in large data sets it bridges the gap from applied statistics and artificial intelligence which usually provide the mathematical background to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently allowing such methods to be applied to ever larger data sets process the knowledge discovery in databases kdd process is commonly defined with the stages selection pre processing transformation data mining interpretation evaluation it exists however in many variations on this theme such as the cross industry standard process for data mining crisp dm which defines six phases business understanding data understanding data preparation modeling evaluation deployment or a simplified process such as 1 pre processing 2 data mining and 3 results validation polls conducted in 2002 2004 2007 and 2014 show that the crisp dm methodology is the leading methodology used by data miners the only other data mining standard named in these polls was semma however 3 4 times as many people reported using crisp dm several teams of researchers have published reviews of data mining process models and azevedo and santos conducted a comparison of crisp dm and semma in 2008 pre processing before data mining algorithms can be used a target data set must be assembled as data mining can only uncover patterns actually present in the data the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit a common source for data is a data mart or data warehouse pre processing is essential to analyze the multivariate data sets before data mining the target set is then cleaned data cleaning removes the observations containing noise and those with missing data data mining data mining involves six common classes of tasks anomaly detection outlier change deviation detection the identification of unusual data records that might be interesting or data errors that require further investigation due to being out of standard range association rule learning dependency modeling searches for relationships between variables for example a supermarket might gather data on customer purchasing habits using association rule learning the supermarket can determine which products are frequently bought together and use this information for marketing purposes this is sometimes referred to as market basket analysis clustering is the task of discovering groups and structures in the data that are in some way or another similar without using known structures in the data classification is the task of generalizing known structure to apply to new data for example an e mail program might attempt to classify an e mail as legitimate or as spam regression attempts to find a function that models the data with the least error that is for estimating the relationships among data or datasets summarization providing a more compact representation of the data set including visualization and report generation results validation data mining can unintentionally be misused producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data therefore bearing little use this is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing a simple version of this problem in machine learning is known as overfitting but the same problem can arise at different phases of the process and thus a train test split when applicable at all may not be sufficient to prevent this from happening the final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set not all patterns found by the algorithms are necessarily valid it is common for data mining algorithms to find patterns in the training set which are not present in the general data set this is called overfitting to overcome this the evaluation uses a test set of data on which the data mining algorithm was not trained the learned patterns are applied to this test set and the resulting output is compared to the desired output for example a data mining algorithm trying to distinguish spam from legitimate e mails would be trained on a training set of sample e mails once trained the learned patterns would be applied to the test set of e mails on which it had not been trained the accuracy of the patterns can then be measured from how many e mails they correctly classify several statistical methods may be used to evaluate the algorithm such as roc curves if the learned patterns do not meet the desired standards it is necessary to re evaluate and change the pre processing and data mining steps if the learned patterns do meet the desired standards then the final step is to interpret the learned patterns and turn them into knowledge research the premier professional body in the field is the association for computing machinery s acm special interest group sig on knowledge discovery and data mining sigkdd since 1989 this acm sig has hosted an annual international conference and published its proceedings and since 1999 it has published a biannual academic journal titled sigkdd explorations computer science conferences on data mining include cikm conference acm conference on information and knowledge management european conference on machine learning and principles and practice of knowledge discovery in databases kdd conference acm sigkdd conference on knowledge discovery and data mining data mining topics are also present in many data management database conferences such as the icde conference sigmod conference and international conference on very large data bases standards there have been some efforts to define standards for the data mining process for example the 1999 european cross industry standard process for data mining crisp dm 1 0 and the 2004 java data mining standard jdm 1 0 development on successors to these processes crisp dm 2 0 and jdm 2 0 was active in 2006 but has stalled since jdm 2 0 was withdrawn without reaching a final draft for exchanging the extracted models in particular for use in predictive analytics the key standard is the predictive model markup language pmml which is an xml based language developed by the data mining group dmg and supported as exchange format by many data mining applications as the name suggests it only covers prediction models a particular data mining task of high importance to business applications however extensions to cover for example subspace clustering have been proposed independently of the dmg notable uses data mining is used wherever there is digital data available notable examples of data mining can be found throughout business medicine science finance construction and surveillance privacy concerns and ethics while the term data mining itself may have no ethical implications it is often associated with the mining of information in relation to user behavior ethical and otherwise the ways in which data mining can be used can in some cases and contexts raise questions regarding privacy legality and ethics in particular data mining government or commercial data sets for national security or law enforcement purposes such as in the total information awareness program or in advise has raised privacy concerns data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations a common way for this to occur is through data aggregation data aggregation involves combining data together possibly from various sources in a way that facilitates analysis but that also might make identification of private individual level data deducible or otherwise apparent this is not data mining per se but a result of the preparation of data before and for the purposes of the analysis the threat to an individual s privacy comes into play when the data once compiled cause the data miner or anyone who has access to the newly compiled data set to be able to identify specific individuals especially when the data were originally anonymous it is recommended to be aware of the following before data are collected the purpose of the data collection and any known data mining projects how the data will be used who will be able to mine the data and use the data and their derivatives the status of security surrounding access to the data how collected data can be updated data may also be modified so as to become anonymous so that individuals may not readily be identified however even anonymized data sets can potentially contain enough information to allow identification of individuals as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by aol the inadvertent revelation of personally identifiable information leading to the provider violates fair information practices this indiscretion can cause financial emotional or bodily harm to the indicated individual in one instance of privacy violation the patrons of walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies situation in europe europe has rather strong privacy laws and efforts are underway to further strengthen the rights of the consumers however the u s e u safe harbor principles developed between 1998 and 2000 currently effectively expose european users to privacy exploitation by u s companies as a consequence of edward snowden s global surveillance disclosure there has been increased discussion to revoke this agreement as in particular the data will be fully exposed to the national security agency and attempts to reach an agreement with the united states have failed in the united kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices these groups tend to be people of lower socio economic status who are not savvy to the ways they can be exploited in digital market places situation in the united states in the united states privacy concerns have been addressed by the us congress via the passage of regulatory controls such as the health insurance portability and accountability act hipaa the hipaa requires individuals to give their informed consent regarding information they provide and its intended present and future uses according to an article in biotech business week n practice hipaa may not offer any greater protection than the longstanding regulations in the research arena says the aahc more importantly the rule s goal of protection through informed consent is approach a level of incomprehensibility to average individuals this underscores the necessity for data anonymity in data aggregation and mining practices u s information privacy legislation such as hipaa and the family educational rights and privacy act ferpa applies only to the specific areas that each such law addresses the use of data mining by the majority of businesses in the u s is not controlled by any legislation copyright law situation in europe under european copyright database laws the mining of in copyright works such as by web mining without the permission of the copyright owner is not legal where a database is pure data in europe it may be that there is no copyright but database rights may exist so data mining becomes subject to intellectual property owners rights that are protected by the database directive on the recommendation of the hargreaves review this led to the uk government to amend its copyright law in 2014 to allow content mining as a limitation and exception the uk was the second country in the world to do so after japan which introduced an exception in 2009 for data mining however due to the restriction of the information society directive 2001 the uk exception only allows content mining for non commercial purposes uk copyright law also does not allow this provision to be overridden by contractual terms and conditions since 2020 also switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art 24d of the swiss copyright act this new article entered into force on 1 april 2020 the european commission facilitated stakeholder discussion on text and data mining in 2013 under the title of licences for europe the focus on the solution to this legal issue such as licensing rather than limitations and exceptions led to representatives of universities researchers libraries civil society groups and open access publishers to leave the stakeholder dialogue in may 2013 situation in the united states us copyright law and in particular its provision for fair use upholds the legality of content mining in america and other fair use countries such as israel taiwan and south korea as content mining is transformative that is it does not supplant the original work it is viewed as being lawful under fair use for example as part of the google book settlement the presiding judge on the case ruled that google s digitization project of in copyright books was lawful in part because of the transformative uses that the digitization project displayed one being text and data mining software free open source data mining software and applications the following applications are available under free open source licenses public access to application source code is also available carrot2 text and search results clustering framework chemicalize org a chemical structure miner and web search engine elki a university research project with advanced cluster analysis and outlier detection methods written in the java language gate a natural language processing and language engineering tool knime the konstanz information miner a user friendly and comprehensive data analytics framework massive online analysis moa a real time big data stream mining with concept drift tool in the java programming language mepx cross platform tool for regression and classification problems based on a genetic programming variant mlpack a collection of ready to use machine learning algorithms written in the c language nltk natural language toolkit a suite of libraries and programs for symbolic and statistical natural language processing nlp for the python language opennn open neural networks library orange a component based data mining and machine learning software suite written in the python language pspp data mining and statistics software under the gnu project similar to spss r a programming language and software environment for statistical computing data mining and graphics it is part of the gnu project scikit learn an open source machine learning library for the python programming language torch an open source deep learning library for the lua programming language and scientific computing framework with wide support for machine learning algorithms uima the uima unstructured information management architecture is a component framework for analyzing unstructured content such as text audio and video originally developed by ibm weka a suite of machine learning software applications written in the java programming language proprietary data mining software and applications the following applications are available under proprietary licenses angoss knowledgestudio data mining tool lionsolver an integrated software application for data mining business intelligence and modeling that implements the learning and intelligent optimization lion approach polyanalyst data and text mining software by megaputer intelligence microsoft analysis services data mining software provided by microsoft netowl suite of multilingual text and entity analytics products that enable data mining oracle data mining data mining software by oracle corporation pseven platform for automation of engineering simulation and analysis multidisciplinary optimization and data mining provided by datadvance qlucore omics explorer data mining software rapidminer an environment for machine learning and data mining experiments sas enterprise miner data mining software provided by the sas institute spss modeler data mining software provided by ibm statistica data miner data mining software provided by statsoft tanagra visualisation oriented data mining software also for teaching vertica data mining software provided by hewlett packard google cloud platform automated custom ml models managed by google amazon sagemaker managed service provided by amazon for creating productionising custom ml models see also methods application domains application examples related topics for more information about extracting information out of data as opposed to analyzing data see other resources international journal of data warehousing and mining references further reading external links ",
            "total_words": 3476,
            "unique_words_percentage": 31.0126582278481,
            "stopwords_percentage": 36.56501726121979
        }
    ],
    "Cloud Computing": [
        {
            "title": "Cloud computing",
            "link": "https://en.wikipedia.org/wiki/Cloud_computing",
            "content": " cloud computing is a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self service provisioning and administration on demand according to iso essential characteristics in 2011 the national institute of standards and technology nist identified five essential characteristics for cloud systems below are the exact definitions according to nist on demand self service a consumer can unilaterally provision computing capabilities such as server time and network storage as needed automatically without requiring human interaction with each service provider broad network access capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms e g mobile phones tablets laptops and workstations resource pooling the provider s computing resources are pooled to serve multiple consumers using a multi tenant model with different physical and virtual resources dynamically assigned and reassigned according to consumer demand rapid elasticity capabilities can be elastically provisioned and released in some cases automatically to scale rapidly outward and inward commensurate with demand to the consumer the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time measured service cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service e g storage processing bandwidth and active user accounts resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the utilized service by 2023 the international organization for standardization iso had expanded and refined the list history the history of cloud computing extends back to the 1960s with the initial concepts of time sharing becoming popularized via remote job entry rje the data center model where users submitted jobs to operators to run on mainframes was predominantly used during this era this was a time of exploration and experimentation with ways to make large scale computing power available to more users through time sharing optimizing the infrastructure platform and applications and increasing efficiency for end users the cloud metaphor for virtualized services dates to 1994 when it was used by general magic for the universe of places that mobile agents in the telescript environment could go the metaphor is credited to david hoffman a general magic communications specialist based on its long standing use in networking and telecom the expression cloud computing became more widely known in 1996 when compaq computer corporation drew up a business plan for future computing and the internet the company s ambition was to supercharge sales with cloud computing enabled applications the business plan foresaw that online consumer file storage would likely be commercially successful as a result compaq decided to sell server hardware to internet service providers in the 2000s the application of cloud computing began to take shape with the establishment of amazon web services aws in 2002 which allowed developers to build applications independently in 2006 amazon simple storage service known as amazon s3 and the amazon elastic compute cloud ec2 were released in 2008 nasa s development of the first open source software for deploying private and hybrid clouds the following decade saw the launch of various cloud services in 2010 microsoft launched microsoft azure and rackspace hosting and nasa initiated an open source cloud software project openstack ibm introduced the ibm smartcloud framework in 2011 and oracle announced the oracle cloud in 2012 in december 2019 amazon launched aws outposts a service that extends aws infrastructure services apis and tools to customer data centers co location spaces or on premises facilities value proposition cloud computing can enable shorter time to market by providing pre configured tools scalable resources and managed services allowing users to focus on their core business value instead of maintaining infrastructure cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model where costs scale with usage cloud platforms also offer managed services and tools such as artificial intelligence data analytics and machine learning which might otherwise require significant in house expertise and infrastructure investment while cloud computing can offer cost advantages through effective resource optimization organizations often face challenges such as unused resources inefficient configurations and hidden costs without proper oversight and governance many cloud platforms provide cost management tools such as aws cost explorer and azure cost management and frameworks like finops have emerged to standardize financial operations in the cloud cloud computing also facilitates collaboration remote work and global service delivery by enabling secure access to data and applications from any location with an internet connection cloud providers offer various redundancy options for core services such as managed storage and managed databases though redundancy configurations often vary by service tier advanced redundancy strategies such as cross region replication or failover systems typically require explicit configuration and may incur additional costs or licensing fees cloud environments operate under a shared responsibility model where providers are typically responsible for infrastructure security physical hardware and software updates while customers are accountable for data encryption identity and access management iam and application level security these responsibilities vary depending on the cloud service model infrastructure as a service iaas platform as a service paas or software as a service saas with customers typically having more control and responsibility in iaas environments and progressively less in paas and saas models often trading control for convenience and managed services factors influencing adoption and suitability of cloud computing the decision to adopt cloud computing or maintain on premises infrastructure depends on factors such as scalability cost structure latency requirements regulatory constraints and infrastructure customization organizations with variable or unpredictable workloads limited capital for upfront investments or a focus on rapid scalability benefit from cloud adoption startups saas companies and e commerce platforms often prefer the pay as you go operational expenditure opex model of cloud infrastructure additionally companies prioritizing global accessibility remote workforce enablement disaster recovery and leveraging advanced services such as ai ml and analytics are well suited for the cloud in recent years some cloud providers have started offering specialized services for high performance computing and low latency applications addressing some use cases previously exclusive to on premises setups on the other hand organizations with strict regulatory requirements highly predictable workloads or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable businesses in industries like defense government or those handling highly sensitive data often favor on premises setups for greater control and data sovereignty additionally companies with ultra low latency requirements such as high frequency trading hft firms rely on custom hardware e g fpgas and physical proximity to exchanges which most cloud providers cannot fully replicate despite recent advancements similarly tech giants like google meta and amazon build their own data centers due to economies of scale predictable workloads and the ability to customize hardware and network infrastructure for optimal efficiency however these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs in practice many organizations are increasingly adopting hybrid cloud architectures combining on premises infrastructure with cloud services this approach allows businesses to balance scalability cost effectiveness and control offering the benefits of both deployment models while mitigating their respective limitations challenges and limitations one of the main challenges of cloud computing in comparison to more traditional on premises computing is data security and privacy cloud users entrust their sensitive data to third party providers who may not have adequate measures to protect it from unauthorized access breaches or leaks cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection such as gdpr or hipaa another challenge of cloud computing is reduced visibility and control cloud users may not have full insight into how their cloud resources are managed configured or optimized by their providers they may also have limited ability to customize or modify their cloud services according to their specific needs or preferences complete understanding of all technology may be impossible especially given the scale complexity and deliberate opacity of contemporary systems however there is a need for understanding complex technologies and their interconnections to have power and agency within them the metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous it is something experienced without precisely understanding what it is or how it works additionally cloud migration is a significant challenge this process involves transferring data applications or workloads from one cloud environment to another or from on premises infrastructure to the cloud cloud migration can be complicated time consuming and expensive particularly when there are compatibility issues between different cloud platforms or architectures if not carefully planned and executed cloud migration can lead to downtime reduced performance or even data loss cloud migration challenges according to the 2024 state of the cloud report by flexera approximately 50 of respondents identified the following top challenges when migrating workloads to public clouds understanding application dependencies comparing on premise and cloud costs assessing technical feasibility implementation challenges applications hosted in the cloud are susceptible to the fallacies of distributed computing a series of misconceptions that can lead to significant issues in software development and deployment cloud cost overruns in a report by gartner a survey of 200 it leaders revealed that 69 experienced budget overruns in their organizations cloud expenditures during 2023 conversely 31 of it leaders whose organizations stayed within budget attributed their success to accurate forecasting and budgeting proactive monitoring of spending and effective optimization the 2024 flexera state of cloud report identifies the top cloud challenges as managing cloud spend followed by security concerns and lack of expertise public cloud expenditures exceeded budgeted amounts by an average of 15 the report also reveals that cost savings is the top cloud initiative for 60 of respondents furthermore 65 measure cloud progress through cost savings while 42 prioritize shorter time to market indicating that cloud s promise of accelerated deployment is often overshadowed by cost concerns service level agreements typically cloud providers service level agreements slas do not encompass all forms of service interruptions exclusions typically include planned maintenance downtime resulting from external factors such as network issues human errors like misconfigurations natural disasters force majeure events or security breaches typically customers bear the responsibility of monitoring sla compliance and must file claims for any unmet slas within a designated timeframe customers should be aware of how deviations from slas are calculated as these parameters may vary by service these requirements can place a considerable burden on customers additionally sla percentages and conditions can differ across various services within the same provider with some services lacking any sla altogether in cases of service interruptions due to hardware failures in the cloud provider the company typically does not offer monetary compensation instead eligible users may receive credits as outlined in the corresponding sla leaky abstractions cloud computing abstractions aim to simplify resource management but leaky abstractions can expose underlying complexities these variations in abstraction quality depend on the cloud vendor service and architecture mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize service lock in within the same vendor service lock in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor making it challenging to switch to alternative services within the same vendor when their needs change security and privacy cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time it could accidentally or deliberately alter or delete information many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant that is permitted in their privacy policies which users must agree to before they start using cloud services solutions to privacy include policy and legislation as well as end users choices for how data is stored users can encrypt data that is processed or stored within the cloud to prevent unauthorized access identity management systems can also provide practical solutions to privacy concerns in cloud computing these systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity the systems work by creating and describing identities recording activities and getting rid of unused identities according to the cloud security alliance the top three threats in the cloud are insecure interfaces and apis data loss leakage and hardware failure which accounted for 29 25 and 10 of all cloud security outages respectively together these form shared technology vulnerabilities in a cloud provider platform being shared by different users there may be a possibility that information belonging to different customers resides on the same data server additionally eugene schultz chief technology officer at emagined security said that hackers are spending substantial time and effort looking for ways to penetrate the cloud there are some real achilles heels in the cloud infrastructure that are making big holes for the bad guys to get into because data from hundreds or thousands of companies can be stored on large cloud servers hackers can theoretically gain control of huge stores of information through a single attack a process he called hyperjacking some examples of this include the dropbox security breach and icloud 2014 leak dropbox had been breached in october 2014 having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by bitcoins btc by having these passwords they are able to read private data as well as have this data be indexed by search engines making the information public there is the problem of legal ownership of the data if a user stores some data in the cloud can the cloud provider profit from it many terms of service agreements are silent on the question of ownership physical control of the computer equipment private cloud is more secure than having the equipment off site and under someone else s control public cloud this delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services some small businesses that do not have expertise in it security could find that it is more secure for them to use a public cloud there is the risk that end users do not understand the issues involved when signing on to a cloud service persons sometimes do not read the many pages of the terms of service agreement and just click accept without reading this is important now that cloud computing is common and required for some services to work for example for an intelligent personal assistant apple s siri or google assistant fundamentally private cloud is seen as more secure with higher levels of control for the owner however public cloud is seen to be more flexible and requires less time and money investment from the user the attacks that can be made on cloud computing systems include man in the middle attacks phishing attacks authentication attacks and malware attacks one of the largest threats is considered to be malware attacks such as trojan horses recent research conducted in 2022 has revealed that the trojan horse injection method is a serious problem with harmful impacts on cloud computing systems service models the national institute of standards and technology recognized three cloud service models in 2011 infrastructure as a service iaas platform as a service paas and software as a service saas the international organization for standardization iso later identified additional models in 2023 including network as a service communications as a service compute as a service and data storage as a service infrastructure as a service iaas infrastructure as a service iaas refers to online services that provide high level apis used to abstract various low level details of underlying network infrastructure like physical computing resources location data partitioning scaling security backup etc a hypervisor runs the virtual machines as guests pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers varying requirements linux containers run in isolated partitions of a single linux kernel running directly on the physical hardware linux cgroups and namespaces are the underlying linux kernel technologies used to isolate secure and manage the containers the use of containers offers higher performance than virtualization because there is no hypervisor overhead iaas clouds often offer additional resources such as a virtual machine disk image library raw block storage file or object storage firewalls load balancers ip addresses virtual local area networks vlans and software bundles the nist s definition of cloud computing describes iaas as where the consumer is able to deploy and run arbitrary software which can include operating systems and applications the consumer does not manage or control the underlying cloud infrastructure but has control over operating systems storage and deployed applications and possibly limited control of select networking components e g host firewalls iaas cloud providers supply these resources on demand from their large pools of equipment installed in data centers for wide area connectivity customers can use either the internet or carrier clouds dedicated virtual private networks to deploy their applications cloud users install operating system images and their application software on the cloud infrastructure in this model the cloud user patches and maintains the operating systems and the application software cloud providers typically bill iaas services on a utility computing basis cost reflects the number of resources allocated and consumed platform as a service paas the nist s definition of cloud computing defines platform as a service as the capability provided to the consumer is to deploy onto the cloud infrastructure consumer created or acquired applications created using programming languages libraries services and tools supported by the provider the consumer does not manage or control the underlying cloud infrastructure including network servers operating systems or storage but has control over the deployed applications and possibly configuration settings for the application hosting environment paas vendors offer a development environment to application developers the provider typically develops toolkit and standards for development and channels for distribution and payment in the paas models cloud providers deliver a computing platform typically including an operating system programming language execution environment database and the web server application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers with some paas the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually some integration and data management providers also use specialized applications of paas as delivery models for data examples include ipaas integration platform as a service and dpaas data platform as a service ipaas enables customers to develop execute and govern integration flows under the ipaas integration model customers drive the development and deployment of integrations without installing or managing any hardware or middleware dpaas delivers integration and data management products as a fully managed service under the dpaas model the paas provider not the customer manages the development and execution of programs by building data applications for the customer dpaas users access data through data visualization tools software as a service saas the nist s definition of cloud computing defines software as a service as the capability provided to the consumer is to use the provider s applications running on a cloud infrastructure the applications are accessible from various client devices through either a thin client interface such as a web browser e g web based email or a program interface the consumer does not manage or control the underlying cloud infrastructure including network servers operating systems storage or even individual application capabilities with the possible exception of limited user specific application configuration settings in the software as a service saas model users gain access to application software and databases cloud providers manage the infrastructure and platforms that run the applications saas is sometimes referred to as on demand software and is usually priced on a pay per use basis or using a subscription fee in the saas model cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients cloud users do not manage the cloud infrastructure and platform where the application runs this eliminates the need to install and run the application on the cloud user s own computers which simplifies maintenance and support cloud applications differ from other applications in their scalability which can be achieved by cloning tasks onto multiple virtual machines at run time to meet changing work demand load balancers distribute the work over the set of virtual machines this process is transparent to the cloud user who sees only a single access point to accommodate a large number of cloud users cloud applications can be multitenant meaning that any machine may serve more than one cloud user organization the pricing model for saas applications is typically a monthly or yearly flat fee per user so prices become scalable and adjustable if users are added or removed at any point it may also be free proponents claim that saas gives a business the potential to reduce it operational costs by outsourcing hardware and software maintenance and support to the cloud provider this enables the business to reallocate it operations costs away from hardware software spending and from personnel expenses towards meeting other goals in addition with applications hosted centrally updates can be released without the need for users to install new software one drawback of saas comes with storing the users data on the cloud provider s server as a result there could be unauthorized access to the data examples of applications offered as saas are games and productivity software like google docs and office online saas applications may be integrated with cloud storage or file hosting services which is the case with google docs being integrated with google drive and office online being integrated with onedrive serverless computing serverless computing allows customers to use various cloud capabilities without the need to provision deploy or manage hardware or software resources apart from providing their application code or data iso iec 22123 2 2023 classifies serverless alongside infrastructure as a service iaas platform as a service paas and software as a service saas under the broader category of cloud service categories notably while iso refers to these classifications as cloud service categories the national institute of standards and technology nist refers to them as service models deployment models a cloud deployment model represents the way in which cloud computing can be organized based on the control and sharing of physical or virtual resources cloud deployment models define the fundamental patterns of interaction between cloud customers and cloud providers they do not detail implementation specifics or the configuration of resources private private cloud is cloud infrastructure operated solely for a single organization whether managed internally or by a third party and hosted either internally or externally undertaking a private cloud project requires significant engagement to virtualize the business environment and requires the organization to reevaluate decisions about existing resources it can improve business but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities self run data centers are generally capital intensive they have a significant physical footprint requiring allocations of space hardware and environmental controls these assets have to be refreshed periodically resulting in additional capital expenditures they have attracted criticism because users still have to buy build and manage them and thus do not benefit from less hands on management essentially the economic model that makes cloud computing such an intriguing concept public cloud services are considered public when they are delivered over the public internet and they may be offered as a paid subscription or free of charge architecturally there are few differences between public and private cloud services but security concerns increase substantially when services applications storage and other resources are shared by multiple customers most public cloud providers offer direct connection services that allow customers to securely link their legacy data centers to their cloud resident applications several factors like the functionality of the solutions cost integrational and organizational aspects as well as safety security are influencing the decision of enterprises and organizations to choose a public cloud or on premises solution hybrid hybrid cloud is a composition of a public cloud and a private environment such as a private cloud or on premises resources that remain distinct entities but are bound together offering the benefits of multiple deployment models hybrid cloud can also mean the ability to connect collocation managed and or dedicated services with cloud resources gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private public and community cloud services from different service providers a hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private public or community cloud service it allows one to extend either the capacity or the capability of a cloud service by aggregation integration or customization with another cloud service varied use cases for hybrid cloud composition exist for example an organization may store sensitive client data in house on a private cloud application but interconnect that application to a business intelligence application provided on a public cloud as a software service this example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services hybrid cloud adoption depends on a number of factors such as data security and compliance requirements level of control needed over data and the applications an organization uses another example of hybrid cloud is one where it organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud this capability enables hybrid clouds to employ cloud bursting for scaling across clouds cloud bursting is an application deployment model in which an application runs in a private cloud or data center and bursts to a public cloud when the demand for computing capacity increases a primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed cloud bursting enables data centers to create an in house it infrastructure that supports average workloads and use cloud resources from public or private clouds during spikes in processing demands community community cloud shares infrastructure between several organizations from a specific community with common concerns security compliance jurisdiction etc whether it is managed internally or by a third party and hosted internally or externally the costs are distributed among fewer users compared to a public cloud but more than a private cloud as a result only a portion of the potential cost savings of cloud computing is achieved multi cloud according to iso iec 22123 1 multi cloud is a cloud deployment model in which a customer uses public cloud services provided by two or more cloud service providers poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers it differs from multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more than could be done with a single provider market according to international data corporation idc global spending on cloud computing services has reached 706 billion and is expected to reach 1 3 trillion by 2025 gartner estimated that global public cloud services end user spending would reach 600 billion by 2023 according to a mckinsey company report cloud cost optimization levers and value oriented business use cases foresee more than 1 trillion in run rate ebitda across fortune 500 companies as up for grabs in 2030 in 2022 more than 1 3 trillion in enterprise it spending was at stake from the shift to the cloud growing to almost 1 8 trillion in 2025 according to gartner the european commission s 2012 communication identified several issues which were impeding the development of the cloud computing market section 3 fragmentation of the digital single market across the eu concerns about contracts including reservations about data access and ownership data portability and change control variations in standards applicable to cloud computing the communication set out a series of digital agenda actions which the commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services pages 6 14 list of public clouds adobe creative cloud amazon web services google cloud ibm cloud microsoft azure openstack oracle cloud panorama9 similar concepts the goal of cloud computing is to allow users to take benefit from all of these technologies without the need for deep knowledge about or expertise with each one of them the cloud aims to cut costs and helps the users focus on their core business instead of being impeded by it obstacles the main enabling technology for cloud computing is virtualization virtualization software separates a physical computing device into one or more virtual devices each of which can be easily used and managed to perform computing tasks with operating system level virtualization essentially creating a scalable system of multiple independent computing devices idle computing resources can be allocated and used more efficiently virtualization provides the agility required to speed up it operations and reduces cost by increasing infrastructure utilization autonomic computing automates the process through which the user can provision resources on demand by minimizing user involvement automation speeds up the process reduces labor costs and reduces the possibility of human errors cloud computing uses concepts from utility computing to provide metrics for the services used cloud computing attempts to address qos quality of service and reliability problems of other grid computing models cloud computing shares characteristics with client server model client server computing refers broadly to any distributed application that distinguishes between service providers servers and service requestors clients computer bureau a service bureau providing computer services particularly from the 1960s to 1980s grid computing a form of distributed and parallel computing whereby a super and virtual computer is composed of a cluster of networked loosely coupled computers acting in concert to perform very large tasks fog computing distributed computing paradigm that provides data compute storage and application services closer to the client or near user edge devices such as network routers furthermore fog computing handles data at the network level on smart devices and on the end user client side e g mobile devices instead of sending data to a remote location for processing utility computing the packaging of computing resources such as computation and storage as a metered service similar to a traditional public utility such as electricity peer to peer a distributed architecture without the need for central coordination participants are both suppliers and consumers of resources in contrast to the traditional client server model cloud sandbox a live isolated computer environment in which a program code or file can run without affecting the application in which it runs see also notes references further reading millard christopher 2013 cloud computing law oxford university press isbn 978 0 19 967168 7 weisser alexander 2020 international taxation of cloud computing editions juridiques libres isbn 978 2 88954 030 3 singh jatinder powles julia pasquier thomas bacon jean july 2015 data flow management and compliance in cloud computing ieee cloud computing 2 4 24 32 doi 10 1109 mcc 2015 69 s2cid 9812531 armbrust michael stoica ion zaharia matei fox armando griffith rean joseph anthony d katz randy konwinski andy lee gunho patterson david rabkin ariel 1 april 2010 a view of cloud computing communications of the acm 53 4 50 doi 10 1145 1721654 1721672 s2cid 1673644 hu tung hui 2015 a prehistory of the cloud mit press isbn 978 0 262 02951 3 mell p 2011 september the nist definition of cloud computing retrieved november 1 2015 from national institute of standards and technology website media related to cloud computing at wikimedia commons",
            "total_words": 5388,
            "unique_words_percentage": 27.839643652561247,
            "stopwords_percentage": 34.66963622865627
        },
        {
            "title": "Cloud computing security",
            "link": "https://en.wikipedia.org/wiki/Cloud_computing_security",
            "content": "cloud computing security or more simply cloud security refers to a broad set of policies technologies applications and controls utilized to protect virtualized ip data applications services and the associated infrastructure of cloud computing it is a sub domain of computer security network security and more broadly information security security issues associated with the cloud cloud computing and storage provide users with the capabilities to store and process their data in third party data centers organizations use the cloud in a variety of different service models with acronyms such as saas paas and iaas and deployment models private public hybrid and community security concerns associated with cloud computing are typically categorized in two ways as security issues faced by cloud providers organizations providing software platform or infrastructure as a service via the cloud and security issues faced by their customers companies or organizations who host applications or store data on the cloud the responsibility is shared however and is often detailed in a cloud provider s shared security responsibility model or shared responsibility model the provider must ensure that their infrastructure is secure and that their clients data and applications are protected while the user must take measures to fortify their application and use strong passwords and authentication measures when an organization elects to store data or host applications on the public cloud it loses its ability to have physical access to the servers hosting its information as a result potentially sensitive data is at risk from insider attacks according to a 2010 cloud security alliance report insider attacks are one of the top seven biggest threats in cloud computing therefore cloud service providers must ensure that thorough background checks are conducted for employees who have physical access to the servers in the data center additionally data centers are recommended to be frequently monitored for suspicious activity in order to conserve resources cut costs and maintain efficiency cloud service providers often store more than one customer s data on the same server as a result there is a chance that one user s private data can be viewed by other users possibly even competitors to handle such sensitive situations cloud service providers should ensure proper data isolation and logical storage segregation the extensive use of virtualization in implementing cloud infrastructure brings unique security concerns for customers or tenants of a public cloud service virtualization alters the relationship between the os and underlying hardware be it computing storage or even networking this introduces an additional layer virtualization that itself must be properly configured managed and secured specific concerns include the potential to compromise the virtualization software or hypervisor while these concerns are largely theoretical they do exist for example a breach in the administrator workstation with the management software of the virtualization software can cause the whole data center to go down or be reconfigured to an attacker s liking cloud security controls cloud security architecture is effective only if the correct defensive implementations are in place an efficient cloud security architecture should recognize the issues that will arise with security management and follow all of the best practices procedures and guidelines to ensure a secure cloud environment security management addresses these issues with security controls these controls protect cloud environments and are put in place to safeguard any weaknesses in the system and reduce the effect of an attack while there are many types of controls behind a cloud security architecture they can usually be found in one of the following categories deterrent controls these controls are administrative mechanisms intended to reduce attacks on a cloud system and are utilized to ensure compliance with external controls much like a warning sign on a fence or a property deterrent controls typically reduce the threat level by informing potential attackers that there will be adverse consequences for them if they proceed some consider them a subset of preventive controls examples of such controls could be considered as policies procedures standards guidelines laws and regulations that guide an organization towards security although most malicious actors ignore such deterrent controls such controls are intended to ward off those who are inexperienced or curious about compromising the it infrastructure of an organization preventive controls the main objective of preventive controls is to strengthen the system against incidents generally by reducing if not actually eliminating vulnerabilities as well as preventing unauthorized intruders from accessing or entering the system this could be achieved by either adding software or feature implementations such as firewall protection endpoint protection and multi factor authentication or removing unneeded functionalities so that the attack surface is minimized as in unikernel applications additionally educating individuals through security awareness training and exercises is included in such controls due to human error being the weakest point of security strong authentication of cloud users for instance makes it less likely that unauthorized users can access cloud systems and more likely that cloud users are positively identified all in all preventative controls affect the likelihood of a loss event occurring and are intended to prevent or eliminate the systems exposure to malicious action detective controls detective controls are intended to detect and react appropriately to any incidents that occur in the event of an attack a detective control will signal the preventative or corrective controls to address the issue detective security controls function not only when such an activity is in progress and after it has occurred system and network security monitoring including intrusion detection and prevention arrangements are typically employed to detect attacks on cloud systems and the supporting communications infrastructure most organizations acquire or create a dedicated security operations center soc where dedicated members continuously monitor the organization s it infrastructure through logs and security information and event management siem software siems are security solutions that help organizations and security teams analyze log data in real time for swift detection of security incidents siems are not the only examples of detective controls there are also physical security controls intrusion detection systems and anti virus anti malware tools which all have different functions centered around the exact purpose of detecting security compromises within an it infrastructure corrective controls corrective controls reduce the consequences of an incident generally by limiting the damage such controls include technical physical and administrative measures that occur during or after an incident to restore the systems or resources to their previous state after a security incident there are plenty of examples of corrective controls both physical and technical for instance re issuing an access card or repairing physical damage can be considered corrective controls however technical controls such as terminating a process and administrative controls such as implementing an incident response plan could also be considered corrective controls corrective controls are focused on recovering and repairing any damage caused by a security incident or unauthorized activity the value is needed to change the function of security dimensions of cloud security cloud security engineering is characterized by the security layers plan design programming and best practices that exist inside a cloud security arrangement cloud security engineering requires the composed and visual model design and ui to be characterized by the tasks inside the cloud this cloud security engineering process includes such things as access to the executives techniques and controls to ensure applications and information it also includes ways to deal with and keep up with permeability consistency danger stance and by and large security processes for imparting security standards into cloud administrations and activities assume an approach that fulfills consistent guidelines and essential framework security parts for interest in cloud advancements to be viable companies should recognize the various parts of the cloud and how they remain to impact and help them these interests may include investments in cloud computing and security for example this of course leads to leads to driving push for the cloud advancements to succeed though the idea of cloud computing isn t new associations are increasingly enforcing it because of its flexible scalability relative trustability and cost frugality of services however despite its rapid fire relinquishment in some sectors and disciplines it s apparent from exploration and statistics that security related pitfalls are the most conspicuous hedge to its wide relinquishment it is generally recommended that information security controls be selected and implemented according to and in proportion to the risks typically by assessing the threats vulnerabilities and impacts cloud security concerns can be grouped in various ways gartner named seven while the cloud security alliance identified twelve areas of concern cloud access security brokers casbs are software that sits between cloud users and cloud applications to provide visibility into cloud application usage data protection and governance to monitor all activity and enforce security policies security and privacy any service without a hardened environment is considered a soft target virtual servers should be protected just like a physical server against data leakage malware and exploited vulnerabilities data loss or leakage represents 24 6 and cloud related malware 3 4 of threats causing cloud outages identity management every enterprise will have its own identity management system to control access to information and computing resources cloud providers either integrate the customer s identity management system into their own infrastructure using federation or sso technology or a biometric based identification system or provide an identity management system of their own cloudid for instance provides privacy preserving cloud based and cross enterprise biometric identification it links the confidential information of the users to their biometrics and stores it in an encrypted fashion making use of a searchable encryption technique biometric identification is performed in the encrypted domain to make sure that the cloud provider or potential attackers do not gain access to any sensitive data or even the contents of the individual queries physical security cloud service providers physically secure the it hardware servers routers cables etc against unauthorized access interference theft fires floods etc and ensure that essential supplies such as electricity are sufficiently robust to minimize the possibility of disruption this is normally achieved by serving cloud applications from professionally specified designed constructed managed monitored and maintained data centers personnel security various information security concerns relating to the it and other professionals associated with cloud services are typically handled through pre para and post employment activities such as security screening potential recruits security awareness and training programs and proactive privacy providers ensure that all critical data credit card numbers for example are masked or encrypted and that only authorized users have access to data in its entirety moreover digital identities and credentials must be protected as should any data that the provider collects or produces about customer activity in the cloud penetration testing penetration testing is the process of performing offensive security tests on a system service or computer network to find security weaknesses in it since the cloud is a shared environment with other customers or tenants following penetration testing rules of engagement step by step is a mandatory requirement scanning and penetration testing from inside or outside the cloud should be authorized by the cloud provider violation of acceptable use policies can lead to termination of the service cloud vulnerability and penetration testing scanning the cloud from outside and inside using free or commercial products is crucial because without a hardened environment your service is considered a soft target virtual servers should be hardened just like a physical server against data leakage malware and exploited vulnerabilities data loss or leakage represents 24 6 and cloud related malware 3 4 of threats causing cloud outages scanning and penetration testing from inside or outside the cloud must be authorized by the cloud provider since the cloud is a shared environment with other customers or tenants following penetration testing rules of engagement step by step is a mandatory requirement violation of acceptable use policies can lead to the termination of the service some key terminology to grasp when discussing penetration testing is the difference between application and network layer testing understanding what is asked of you as the tester is sometimes the most important step in the process the network layer testing refers to testing that includes internal external connections as well as the interconnected systems throughout the local network oftentimes social engineering attacks are carried out as the most vulnerable link in security is often the employee white box testing testing under the condition that the attacker has full knowledge of the internal network its design and implementation grey box testing testing under the condition that the attacker has partial knowledge of the internal network its design and implementation black box testing testing under the condition that the attacker has no prior knowledge of the internal network its design and implementation data security there are numerous security threats associated with cloud data services this includes traditional threats and non traditional threats traditional threats include network eavesdropping illegal invasion and denial of service attacks but also specific cloud computing threats such as side channel attacks virtualization vulnerabilities and abuse of cloud services in order to mitigate these threats security controls often rely on monitoring the three areas of the cia triad the cia triad refers to confidentiality including access controllability which can be further understood from the following integrity and availability many effective security measures cover several or all of the three categories encryption for example can be used to prevent unauthorized access and also ensure integrity of the data backups on the other hand generally cover integrity and availability and firewalls only cover confidentiality and access controllability confidentiality data confidentiality is the property in that data contents are not made available or disclosed to illegal users outsourced data is stored in a cloud and out of the owners direct control only authorized users can access the sensitive data while others including csps should not gain any information about the data meanwhile data owners expect to fully utilize cloud data services e g data search data computation and data sharing without the leakage of the data contents to csps or other adversaries confidentiality refers to how data must be kept strictly confidential to the owner of said data an example of security control that covers confidentiality is encryption so that only authorized users can access the data symmetric or asymmetric key paradigm can be used for encryption access controllability access controllability means that a data owner can perform the selective restriction of access to their data outsourced to the cloud legal users can be authorized by the owner to access the data while others can not access it without permission further it is desirable to enforce fine grained access control to the outsourced data i e different users should be granted different access privileges with regard to different data pieces the access authorization must be controlled only by the owner in untrusted cloud environments access control can also be referred to as availability while unauthorized access should be strictly prohibited access for administrative or even consumer uses should be allowed but monitored as well availability and access control ensure that the proper amount of permissions is granted to the correct persons integrity data integrity demands maintaining and assuring the accuracy and completeness of data a data owner always expects that her or his data in a cloud can be stored correctly and trustworthy it means that the data should not be illegally tampered with improperly modified deliberately deleted or maliciously fabricated if any undesirable operations corrupt or delete the data the owner should be able to detect the corruption or loss further when a portion of the outsourced data is corrupted or lost it can still be retrieved by the data users effective integrity security controls go beyond protection from malicious actors and protect data from unintentional alterations as well an example of security control that covers integrity is automated backups of information risks and vulnerabilities of cloud computing while cloud computing is on the cutting edge of information technology there are risks and vulnerabilities to consider before investing fully in it security controls and services do exist for the cloud but as with any security system they are not guaranteed to succeed furthermore some risks extend beyond asset security and may involve issues in productivity and even privacy as well privacy concerns cloud computing is still an emerging technology and thus is developing in relatively new technological structures as a result all cloud services must undertake privacy impact assessments or pias before releasing their platform consumers as well that intend to use clouds to store their customer s data must also be aware of the vulnerabilities of having non physical storage for private information unauthorized access to management interface due to the autonomous nature of the cloud consumers are often given management interfaces to monitor their databases by having controls in such a congregated location and by having the interface be easily accessible for convenience for users there is a possibility that a single actor could gain access to the cloud s management interface giving them a great deal of control and power over the database data recovery vulnerabilities the cloud s capabilities with allocating resources as needed often result in resources in memory and otherwise being recycled to another user at a later event for these memory or storage resources it could be possible for current users to access information left by previous ones internet vulnerabilities the cloud requires an internet connection and therefore internet protocols to access therefore it is open to many internet protocol vulnerabilities such as man in the middle attacks furthermore by having a heavy reliance on internet connectivity if the connection fails consumers will be completely cut off from any cloud resources encryption vulnerabilities cryptography is an ever growing field and technology what was secure 10 years ago may be considered a significant security risk by today s standards as technology continues to advance and older technologies grow old new methods of breaking encryptions will emerge as well as fatal flaws in older encryption methods cloud providers must keep up to date with their encryption as the data they typically contain is especially valuable legal issues privacy legislation often varies from country to country by having information stored via the cloud it is difficult to determine under which jurisdictions the data falls under transborder clouds are especially popular given that the largest companies transcend several countries other legal dilemmas from the ambiguity of the cloud refer to how there is a difference in privacy regulation between information shared between and information shared inside of organizations attacks there are several different types of attacks on cloud computing one that is still very much untapped is infrastructure compromise though not completely known it is listed as the attack with the highest amount of payoff what makes this so dangerous is that the person carrying out the attack is able to gain a level of privilege of having essentially root access to the machine it is very hard to defend against attacks like these because they are so unpredictable and unknown attacks of this type are also called zero day exploits because they are difficult to defend against since the vulnerabilities were previously unknown and unchecked until the attack has already occurred dos attacks aim to have systems be unavailable to their users since cloud computing software is used by large numbers of people resolving these attacks is increasingly difficult now with cloud computing on the rise this has left new opportunities for attacks because of the virtualization of data centers and cloud services being utilized more with the global pandemic that started early in 2020 taking effect there was a massive shift to remote work because of this companies became more reliant on the cloud this massive shift has not gone unnoticed especially by cybercriminals and bad actors many of which saw the opportunity to attack the cloud because of this new remote work environment companies have to constantly remind their employees to keep constant vigilance especially remotely constantly keeping up to date with the latest security measures and policies mishaps in communication are some of the things that these cybercriminals are looking for and will prey upon moving work to the household was critical for workers to be able to continue but as the move to remote work happened several security issues arose quickly the need for data privacy using applications personal devices and the internet all came to the forefront the pandemic has had large amounts of data being generated especially in the healthcare sector big data is accrued for the healthcare sector now more than ever due to the growing coronavirus pandemic the cloud has to be able to organize and share the data with its users securely quality of data looks for four things accuracy redundancy completeness and consistency users had to think about the fact that massive amounts of data are being shared globally different countries have certain laws and regulations that have to be adhered to differences in policy and jurisdiction give rise to the risk involved with the cloud workers are using their personal devices more now that they are working from home criminals see this increase as an opportunity to exploit people software is developed to infect people s devices and gain access to their cloud the current pandemic has put people in a situation where they are incredibly vulnerable and susceptible to attacks the change to remote work was so sudden that many companies simply were unprepared to deal with the tasks and subsequent workload they have found themselves deeply entrenched in tighter security measures have to be put in place to ease that newfound tension within organizations the attacks that can be made on cloud computing systems include man in the middle attacks phishing attacks authentication attacks and malware attacks one of the largest threats is considered to be malware attacks such as trojan horses recent research conducted in 2022 has revealed that the trojan horse injection method is a serious problem with harmful impacts on cloud computing systems a trojan attack on cloud systems tries to insert an application or service into the system that can impact the cloud services by changing or stopping the functionalities when the cloud system identifies the attacks as legitimate the service or application is performed which can damage and infect the cloud system encryption some advanced encryption algorithms which have been applied to cloud computing increase the protection of privacy in a practice called crypto shredding the keys can simply be deleted when there is no more use of the data attribute based encryption abe attribute based encryption is a type of public key encryption in which the secret key of a user and the ciphertext are dependent upon attributes e g the country in which he lives or the kind of subscription he has in such a system the decryption of a ciphertext is possible only if the set of attributes of the user key matches the attributes of the ciphertext some of the strengths of attribute based encryption are that it attempts to solve issues that exist in current public key infrastructure pki and identity based encryption ibe implementations by relying on attributes abe circumvents needing to share keys directly as with pki as well as having to know the identity of the receiver as with ibe these benefits come at a cost as abe suffers from the decryption key re distribution problem since decryption keys in abe only contain information regarding access structure or the attributes of the user it is hard to verify the user s actual identity thus malicious users can intentionally leak their attribute information so that unauthorized users can imitate and gain access ciphertext policy abe cp abe ciphertext policy abe cp abe is a type of public key encryption in the cp abe the encryptor controls the access strategy the main research work of cp abe is focused on the design of the access structure a ciphertext policy attribute based encryption scheme consists of four algorithms setup encrypt keygen and decrypt the setup algorithm takes security parameters and an attribute universe description as input and outputs public parameters and a master key the encryption algorithm takes data as input it then encrypts it to produce ciphertext that only a user that possesses a set of attributes that satisfies the access structure will decrypt the message the keygen algorithm then takes the master key and the user s attributes to develop a private key finally the decrypt algorithm takes the public parameters the ciphertext the private key and user attributes as input with this information the algorithm first checks if the users attributes satisfy the access structure and then decrypts the ciphertext to return the data key policy abe kp abe key policy attribute based encryption or kp abe is an important type of attribute based encryption kp abe allows senders to encrypt their messages under a set of attributes much like any attribute based encryption system for each encryption private user keys are then generated which contain decryption algorithms for deciphering the message and these private user keys grant users access to specific messages that they correspond to in a kp abe system ciphertexts or the encrypted messages are tagged by the creators with a set of attributes while the user s private keys are issued that specify which type of ciphertexts the key can decrypt the private keys control which ciphertexts a user is able to decrypt in kp abe the attribute sets are used to describe the encrypted texts and the private keys are associated to the specified policy that users will have for the decryption of the ciphertexts a drawback to kp abe is that in kp abe the encryptor does not control who has access to the encrypted data except through descriptive attributes which creates a reliance on the key issuer granting and denying access to users hence the creation of other abe systems such as ciphertext policy attribute based encryption fully homomorphic encryption fhe fully homomorphic encryption is a cryptosystem that supports arbitrary computation on ciphertext and also allows computing sum and product for the encrypted data without decryption another interesting feature of fully homomorphic encryption or fhe for short is that it allows operations to be executed without the need for a secret key fhe has been linked not only to cloud computing but to electronic voting as well fully homomorphic encryption has been especially helpful with the development of cloud computing and computing technologies however as these systems are developing the need for cloud security has also increased fhe aims to secure data transmission as well as cloud computing storage with its encryption algorithms its goal is to be a much more secure and efficient method of encryption on a larger scale to handle the massive capabilities of the cloud searchable encryption se searchable encryption is a cryptographic system that offers secure search functions over encrypted data se schemes can be classified into two categories se based on secret key or symmetric key cryptography and se based on public key cryptography in order to improve search efficiency symmetric key se generally builds keyword indexes to answer user queries this has the obvious disadvantage of providing multimodal access routes for unauthorized data retrieval bypassing the encryption algorithm by subjecting the framework to alternative parameters within the shared cloud environment compliance numerous laws and regulations pertaining to the storage and use of data in the us these include privacy or data protection laws payment card industry data security standard pci dss the health insurance portability and accountability act hipaa the sarbanes oxley act the federal information security management act of 2002 fisma and children s online privacy protection act of 1998 among others similar standards exist in other jurisdictions e g singapore s multi tier cloud security standard similar laws may apply in different legal jurisdictions and may differ quite markedly from those enforced in the us cloud service users may often need to be aware of the legal and regulatory differences between the jurisdictions for example data stored by a cloud service provider may be located in say singapore and mirrored in the us many of these regulations mandate particular controls such as strong access controls and audit trails and require regular reporting cloud customers must ensure that their cloud providers adequately fulfill such requirements as appropriate enabling them to comply with their obligations since to a large extent they remain accountable business continuity and data recovery cloud providers have business continuity and data recovery plans in place to ensure that service can be maintained in case of a disaster or an emergency and that any data loss will be recovered these plans may be shared with and reviewed by their customers ideally dovetailing with the customers own continuity arrangements joint continuity exercises may be appropriate simulating a major internet or electricity supply failure for instance log and audit trail in addition to producing logs and audit trails cloud providers work with their customers to ensure that these logs and audit trails are properly secured maintained for as long as the customer requires and are accessible for the purposes of forensic investigation e g ediscovery unique compliance requirements in addition to the requirements to which customers are subject the data centers used by cloud providers may also be subject to compliance requirements using a cloud service provider csp can lead to additional security concerns around data jurisdiction since customer or tenant data may not remain on the same system in the same data center or even within the same provider s cloud the european union s gdpr has introduced new compliance requirements for customer data legal and contractual issues aside from the security and compliance issues enumerated above cloud providers and their customers will negotiate terms around liability stipulating how incidents involving data loss or compromise will be resolved for example intellectual property and end of service when data and applications are ultimately returned to the customer in addition there are considerations for acquiring data from the cloud that may be involved in litigation these issues are discussed in service level agreements sla public records legal issues may also include records keeping requirements in the public sector where many agencies are required by law to retain and make available electronic records in a specific fashion this may be determined by legislation or law may require agencies to conform to the rules and practices set by a records keeping agency public agencies using cloud computing and storage must take these concerns into account see also computer security common vulnerabilities and exposures references further reading mowbray miranda 15 april 2009 the fog over the grimpen mire cloud computing and the law script ed 6 1 132 146 doi 10 2966 scrip 060109 132 mather tim kumaraswamy subra latif shahed 2009 cloud security and privacy an enterprise perspective on risks and compliance o reilly media inc isbn 9780596802769 winkler vic 2011 securing the cloud cloud computer security techniques and tactics elsevier isbn 9781597495929 ottenheimer davi 2012 securing the virtual environment how to defend the enterprise against attack wiley isbn 9781118155486 bs iso iec 27017 information technology security techniques code of practice for information security controls based on iso iec 27002 for cloud services 2015 bs iso iec 27018 information technology security techniques code of practice for protection of personally identifiable information pii in public clouds acting as pii processors 2014 bs iso iec 27036 4 information technology security techniques information security for supplier relationships guidelines for security of cloud services 2016 external links cloud security alliance check point cloud security cloud security solutions why cloud security requires multiple layers the beginner s guide to cloud security dod cloud computing security requirements guide cc srg archive archived 2018 10 21 at the wayback machine",
            "total_words": 5295,
            "unique_words_percentage": 26.137865911237018,
            "stopwords_percentage": 38.98016997167139
        },
        {
            "title": "Cloud computing research",
            "link": "https://en.wikipedia.org/wiki/Cloud_computing_research",
            "content": "many universities vendors institutes and government organizations are investing in cloud computing research in october 2007 the academic cloud computing initiative acci was announced as a multi university project designed to enhance students technical knowledge to address the challenges of cloud computing in april 2009 uc santa barbara released the first open source platform as a service appscale which is capable of running google app engine applications at scale on a multitude of infrastructures in april 2009 the st andrews cloud computing co laboratory was launched focusing on research in the important new area of cloud computing unique in the uk stacc aims to become an international centre of excellence for research and teaching in cloud computing and provides advice and information to businesses interested in cloud based services in october 2010 the tclouds trustworthy clouds project was started funded by the european commission s 7th framework programme the project s goal is to research and inspect the legal foundation and architectural design to build a resilient and trustworthy cloud of cloud infrastructure on top of that the project also develops a prototype to demonstrate its results in january 2011 the irmos eu funded project developed a real time cloud platform enabling interactive applications to be executed in cloud infrastructures in february 2011 enterprise ireland and the irish industrial development authority launched the irish centre for cloud computing and commerce to deliver industry led research on cloud architectures quality of service security and business and legal issues in july 2011 the high performance computing cloud hpccloud project was kicked off aiming at finding out the possibilities of enhancing performance on cloud environments while running the scientific applications development of hpccloud performance analysis toolkit which was funded by cim returning experts programme under the coordination of prof dr shajulin benedict in june 2011 the telecommunications industry association developed a cloud computing white paper to analyze the integration challenges and opportunities between cloud services and traditional u s telecommunications standards in december 2011 the vision cloud eu funded project proposed an architecture along with an implementation of a cloud environment for data intensive services aiming to provide a virtualized cloud storage infrastructure in october 2012 the centre for development of advanced computing released an open source complete cloud service software suite called meghdoot in october 2012 the eco2clouds eu funded project was launched to analyze the environmental impact of applications on the cloud and to optimize their deployment and scheduling based on a monitoring infrastructure based on bonfire proving ecometrics in february 2013 the bonfire project launched a multi site cloud experimentation and testing facility the facility provides transparent access to cloud resources with the control and observability necessary to engineer future cloud technologies in a way that is not restricted for example by current business models in october 2013 the cactos project short for content aware cloud simulation and optimisation was launched to address the specific problems data centre operators face due to the exploding heterogeneity of the underlying hardware in february 2015 cloudlightning a european commission funded horizon 2020 project was launched to address energy efficiency and high performance by developing a self organising self optimising heterogeneous cloud computing service delivery model its initial application domains genome processing oil and gas exploration and ray tracing in january 2017 recap an eu funded horizon 2020 project was launched to advance cloud and edge computing technology it develops mechanisms for reliable capacity provisioning to make application placement infrastructure management and capacity provisioning autonomous predictable and optimized european research in 2012 the european commission has issued an analysis of the relevance of the open research issues for commercial stabilisation in which various experts from industry and academia identify in particular the following major concerns open interoperation across proprietary cloud solutions at iaas paas and saas levels managing multitenancy at large scale and in heterogeneous environments dynamic and seamless elasticity from inhouse clouds to public clouds for unusual scale complexity and or infrequent requirements data management in a cloud environment taking the technical and legal constraints into consideration these findings have been refined into a research roadmap proposed by the cloud computing expert group on research in december 2012 which tries to lay out a timeline for the identified research topics according to their commercial relevance with the 8th framework programmes for research and technological development the european commission is trying to support the according research work along the lines of the europe 2020 strategy see also cloud computing cloud storage references ",
            "total_words": 748,
            "unique_words_percentage": 46.657754010695186,
            "stopwords_percentage": 33.02139037433155
        },
        {
            "title": "IBM Cloud",
            "link": "https://en.wikipedia.org/wiki/IBM_Cloud",
            "content": "ibm cloud formerly known as bluemix is a set of cloud computing services for business offered by the information technology company ibm services as of 2021 ibm cloud contains more than 170 services including compute storage networking database analytics machine learning and developer tools history softlayer softlayer technologies inc now ibm cloud was a dedicated server managed hosting and cloud computing provider founded in 2005 and acquired by ibm in 2013 softlayer initially specialized in hosting workloads for gaming companies and startups but shifted focus to enterprise workloads after its acquisition softlayer had bare metal compute offerings before other large cloud providers such as amazon web services softlayer has hosted workloads for companies such as the hartford whatsapp whirlpool daimler and macy s timeline year 2005 softlayer was established in 2005 by lance crosby and several of his ex coworkers year 2010 august gi partners acquired a majority equity stake in softlayer in august 2010 year 2010 november in november of that year it merged the company with the planet internet services softlayer s biggest competitor and consolidated the customer base under the softlayer brand year 2011 q1 in q1 2011 the company reported hosting more than 81 000 servers for more than 26 000 customers in locations throughout the united states year 2011 july in july 2011 the company announced plans for international expansion to amsterdam and singapore to add to the existing network of north american based data centers in dallas texas san jose california seattle washington houston texas and washington d c most of these data centers were leased via digital realty year 2013 june 4 on june 4 2013 ibm announced its acquisition of softlayer under undisclosed financial terms in a deal that according to reuters could have fetched more than 2 billion to form an ibm cloud services division at the time of acquisition softlayer was described as the biggest privately held cloud infrastructure provider iaas in the world year 2015 may as of may 2015 the company has 23 data centers in 11 different countries year 2018 by 2018 softlayer was renamed to ibm cloud initial launch of bluemix 2013 2016 in june 2013 ibm acquired softlayer a public cloud platform to serve as the foundation for its iaas offering bluemix was announced for public beta in february 2014 after having been developed since early 2013 bluemix was based on the open source cloud foundry project and ran on softlayer infrastructure ibm announced the general availability of the bluemix platform as a service paas offering in july 2014 by april 2015 bluemix included a suite of over 100 cloud based development tools including social mobile security analytics database and iot internet of things bluemix had grown to 83 000 users in india with growth of approximately 10 000 users each month a year after announcement bluemix had made little headway in the cloud computing platform space relative to its competition and remained substantially behind market leaders microsoft azure and amazon aws by august 2016 little had changed in market acceptance of the bluemix offering in february 2016 ibm bluemix includes ibm s function as a service faas system or serverless computing offering that is built using open source from the apache openwhisk incubator project largely credited to ibm for seeding this system equivalent to amazon lambda microsoft azure functions oracle cloud fn or google cloud functions allows calling of a specific function in response to an event without requiring any resource management from the developer re brand to ibm cloud since 2017 in may 2017 ibm released kubernetes support as the ibm bluemix container service later renamed to the ibm cloud kubernetes service iks iks was built using the open source kubernetes project this system equivalent to amazon web services eks microsoft azure aks or google cloud gke aims to provide a platform for automating deployment scaling and operations of application containers across clusters of hosts in october 2017 ibm announced that they would rebrand their cloud as ibm cloud brand merging all components thus retiring the bluemix and softlayer brands in march 2018 ibm launched an industry first managed kubernetes service on bare metal in august 2019 3 weeks after the close of red hat acquisition ibm launched a managed red hat openshift on ibm cloud in november 2019 ibm has announced that it had designed the world s first financial services ready public cloud and that bank of america was its first committed collaborator and anchor customer joined shortly thereafter in 2020 by bnp paribas as its first european anchor client ibm announced in april 2021 the general availability of ibm cloud for financial services including support for red hat openshift and other cloud native technologies in july 2021 it was announced that sap is onboarding two of its finance and data management solutions to ibm cloud for financial services in september 2021 it was caixabank s turn to boost digital capabilities with ibm cloud for financial services and onboarding to new ibm cloud multizone region in spain customer base in 2019 ibm partnered with the united states tennis association usta to provide new ai powered tools for the us open in may 2020 ibm announced agreements with six european companies including osram and crédit mutuel that use ibm cloud to access advanced technologies such as ai blockchain and analytics reviews ibm cloud continued to be considered a leader in bare metal in 2020 and distinguished itself by providing over 11 million possible custom configurations with the latest power intel and amd cpus and nvidia gpus environmental impact in 2021 ibm announced it would achieve net zero greenhouse gas emissions by 2030 references external links official website",
            "total_words": 945,
            "unique_words_percentage": 43.28042328042328,
            "stopwords_percentage": 31.322751322751323
        },
        {
            "title": "Cloud-native computing",
            "link": "https://en.wikipedia.org/wiki/Cloud-native_computing",
            "content": "cloud native computing is an approach in software development that utilizes cloud computing to build and run scalable applications in modern dynamic environments such as public private and hybrid clouds these technologies such as containers microservices serverless functions cloud native processors and immutable infrastructure deployed via declarative code are common elements of this architectural style cloud native technologies focus on minimizing users operational burden cloud native techniques enable loosely coupled systems that are resilient manageable and observable combined with robust automation they allow engineers to make high impact changes frequently and predictably with minimal toil this independence contributes to the overall resilience of the system as issues in one area do not necessarily cripple the entire application additionally such systems are easier to manage and monitor given their modular nature which simplifies tracking performance and identifying issues frequently cloud native applications are built as a set of microservices that run in open container initiative compliant containers such as containerd and may be orchestrated in kubernetes and managed and deployed using devops and git ci workflows although there is a large amount of competing open source that supports cloud native development the advantage of using containers is the ability to package all software needed to execute into one executable package the container runs in a virtualized environment which isolates the contained application from its environment see also cloud native computing foundation dapr references ",
            "total_words": 232,
            "unique_words_percentage": 63.793103448275865,
            "stopwords_percentage": 33.189655172413794
        },
        {
            "title": "Mobile cloud computing",
            "link": "https://en.wikipedia.org/wiki/Mobile_cloud_computing",
            "content": "mobile cloud computing mcc is the combination of cloud computing and mobile computing to bring rich computational resources to mobile users network operators as well as cloud computing providers the ultimate goal of mcc is to enable execution of rich mobile applications on a plethora of mobile devices with a rich user experience mcc provides business opportunities for mobile network operators as well as cloud providers more comprehensively mcc can be defined as a rich mobile computing technology that leverages unified elastic resources of varied clouds and network technologies toward unrestricted functionality storage and mobility to serve a multitude of mobile devices anywhere anytime through the channel of ethernet or internet regardless of heterogeneous environments and platforms based on the pay as you use principle architecture mcc uses computational augmentation approaches computations are executed remotely instead of on the device by which resource constraint mobile devices can utilize computational resources of varied cloud based resources in mcc there are four types of cloud based resources namely distant immobile clouds proximate immobile computing entities proximate mobile computing entities and hybrid combination of the other three model giant clouds such as amazon ec2 are in the distant immobile groups whereas cloudlet or surrogates are member of proximate immobile computing entities smartphones tablets handheld devices and wearable computing devices are part of the third group of cloud based resources which is proximate mobile computing entities vodafone orange and verizon have started to offer cloud computing services for companies challenges in the mcc landscape an amalgam of mobile computing cloud computing and communication networks to augment smartphones creates several complex challenges such as mobile computation offloading seamless connectivity long wan latency mobility management context processing energy constraint vendor data lock in security and privacy elasticity that hinder mcc success and adoption open research issues although significant research and development in mcc is available in the literature efforts in the following domains is still lacking architectural issues a reference architecture for heterogeneous mcc environment is a crucial requirement for unleashing the power of mobile computing towards unrestricted ubiquitous computing energy efficient transmission mcc requires frequent transmissions between cloud platform and mobile devices due to the stochastic nature of wireless networks the transmission protocol should be carefully designed context awareness issues context aware and socially aware computing are inseparable traits of contemporary handheld computers to achieve the vision of mobile computing among heterogeneous converged networks and computing devices designing resource efficient environment aware applications is an essential need live vm migration issues executing resource intensive mobile application via virtual machine vm migration based application offloading involves encapsulation of application in vm instance and migrating it to the cloud which is a challenging task due to additional overhead of deploying and managing vm on mobile devices mobile communication congestion issues mobile data traffic is tremendously hiking by ever increasing mobile user demands for exploiting cloud resources which impact on mobile network operators and demand future efforts to enable smooth communication between mobile and cloud endpoints trust security and privacy issues trust is an essential factor for the success of the burgeoning mcc paradigm it is because the data along with code component application complete vm is offloaded to the cloud for execution moreover just like software and mobile application piracy the mcc application development models are also affected by the piracy issue pirax is known to be the first specialized framework for controlling application piracy in mcc requirements mcc research groups and activities several academic and industrial research groups in mcc have been emerging since last few years some of the mcc research groups in academia with large number of researchers and publications include mdc mobile and distributed computing research group is at faculty of computer and information science king saud university mdc research group focuses on architectures platforms and protocols for mobile and distributed computing the group has developed algorithms tools and technologies which offer energy efficient fault tolerant scalable secure and high performance computing on mobile devices mobcc lab faculty of computer science and information technology university malaya the lab was established in 2010 under the high impact research grant ministry of higher education malaysia it has 17 researchers and has track of 22 published articles in international conference and peer reviewed cs journals icclab zürich university of applied sciences has a segment working on mcc the init cloud computing lab is a research lab within the institute of applied information technology init of zürich university of applied sciences zhaw it covers topic areas across the entire cloud computing technology stack mobile cloud lab institute of computer science university of tartu mobile cloud lab conducts research and teaching in the mobile computing and cloud computing domains the research topics of the group include cloud computing mobile application development mobile cloud mobile web services and migrating scientific computing and enterprise applications to the cloud smartlab data management systems laboratory department of computer science university of cyprus smartlab is a first of a kind open cloud of smartphones that enables a new line of systems oriented mobile computing research mobile cloud networking mobile cloud networking mcn was an eu fp7 large scale integrating project ip 15m euro funded by the european commission the mcn project was launched in november 2012 for the period of 36 month the project was coordinated by sap research and the icclab at the zurich university of applied science in total 19 partners from industry and academia established the first vision of mobile cloud computing the project was primarily motivated by an ongoing transformation that drives the convergence between the mobile communications and cloud computing industry enabled by the internet and is considered the first pioneer in the area of network function virtualization see also cloudlet cloud computing cloud collaboration mobile collaboration crowd computing references ",
            "total_words": 962,
            "unique_words_percentage": 41.891891891891895,
            "stopwords_percentage": 30.24948024948025
        },
        {
            "title": "Cloud computing issues",
            "link": "https://en.wikipedia.org/wiki/Cloud_computing_issues",
            "content": "cloud computing enables users to access scalable and on demand computing resources via the internet utilizing hardware and software virtualization it is a rapidly evolving technology capable of delivering extensible services efficiently supporting a wide range of applications from personal storage solutions to enterprise level systems despite its advantages cloud computing also faces several challenges privacy concerns remain a primary issue as users often lose direct control over their data once it is stored on servers owned and managed by cloud providers this loss of control can create uncertainties regarding data privacy unauthorized access and compliance with regional regulations such as the general data protection regulation gdpr the health insurance portability and accountability act hipaa and the california consumer privacy act ccpa service agreements and shared responsibility models define the boundaries of control and accountability between the cloud provider and the customer but misunderstandings or mismanagement in these areas can still result in security breaches or accidental data loss cloud providers offer tools such as aws artifact compliance documentation and audits azure compliance manager compliance assessments and risk analysis and google assured workloads region specific data compliance to assist customers in managing compliance requirements security issues in cloud computing are generally categorized into two broad groups the first involves risks faced by cloud service providers including vulnerabilities in their infrastructure software or third party dependencies the second includes risks faced by cloud customers such as misconfigurations inadequate access controls and accidental data exposure these risks are often amplified by human error or a lack of understanding of the shared responsibility model security responsibilities also vary depending on the service model whether infrastructure as a service iaas platform as a service paas or software as a service saas in general cloud providers are responsible for hardware security physical infrastructure and software updates while customers are responsible for data encryption identity and access management iam and application level security another significant concern is uncertainty regarding guaranteed quality of service qos particularly in multi tenant environments where resources are shared among customers major cloud providers address these concerns through service level agreements slas which define performance and uptime guarantees and often offer compensation in the form of service credits when guarantees are unmet automated management and remediation processes supported by tools such as aws cloudwatch azure monitor and google cloud operations suite help detect and respond to large scale failures despite these tools managing qos in highly distributed and multi tenant systems remains complex for latency sensitive workloads cloud providers have introduced edge computing solutions such as aws wavelength azure edge zones and google distributed cloud edge to minimize latency by processing data closer to the end user jurisdictional and regulatory requirements regarding data residency and sovereignty introduce further complexity data stored in one region may fall under the legal jurisdiction of that region creating potential conflicts for organizations operating across multiple geographies major cloud providers such as aws microsoft azure and google cloud address these concerns by offering region specific data centers and compliance management tools designed to align with regional regulations and legal frameworks factors influencing adoption and suitability of cloud computing the decision to adopt cloud computing or maintain on premises infrastructure depends on factors such as scalability cost structure latency requirements regulatory constraints and infrastructure customization organizations with variable or unpredictable workloads limited capital for upfront investments or a focus on rapid scalability benefit from cloud adoption startups saas companies and e commerce platforms often prefer the pay as you go operational expenditure opex model of cloud infrastructure additionally companies prioritizing global accessibility remote workforce enablement disaster recovery and leveraging advanced services such as ai ml and analytics are well suited for the cloud in recent years some cloud providers have started offering specialized services for high performance computing and low latency applications addressing some use cases previously exclusive to on premises setups on the other hand organizations with strict regulatory requirements highly predictable workloads or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable businesses in industries like defense government or those handling highly sensitive data often favor on premises setups for greater control and data sovereignty additionally companies with ultra low latency requirements such as high frequency trading hft firms rely on custom hardware e g fpgas and physical proximity to exchanges which most cloud providers cannot fully replicate despite recent advancements similarly tech giants like google meta and amazon build their own data centers due to economies of scale predictable workloads and the ability to customize hardware and network infrastructure for optimal efficiency however these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs in practice many organizations are increasingly adopting hybrid cloud architectures combining on premises infrastructure with cloud services this approach allows businesses to balance scalability cost effectiveness and control offering the benefits of both deployment models while mitigating their respective limitations cloud migration challenges according to the 2024 state of the cloud report by flexera approximately 50 of respondents identified the following top challenges when migrating workloads to public clouds understanding application dependencies comparing on premise and cloud costs assessing technical feasibility leaky abstractions cloud computing abstractions aim to simplify resource management but leaky abstractions can expose underlying complexities these variations in abstraction quality depend on the cloud vendor service and architecture mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize privacy the increased use of cloud computing services such as gmail and google docs has pressed the issue of privacy concerns of cloud computing services to the utmost importance the provider of such services lie in a position such that with the greater use of cloud computing services has given access to a plethora of data this access has the immense risk of data being disclosed either accidentally or deliberately the privacy of the companies can be compromised as all the information is sent to the cloud service provider privacy advocates have criticized the cloud model for giving hosting companies greater ease to control and thus to monitor at will communication between host company and end user and access user data with or without permission instances such as the secret nsa program working with at t and verizon which recorded over 10 million telephone calls between american citizens causes uncertainty among privacy advocates and the greater powers it gives to telecommunication companies to monitor user activity a cloud service provider csp can complicate data privacy because of the extent of virtualization virtual machines and cloud storage used to implement cloud service csp operations customer or tenant data may not remain on the same system or in the same data center or even within the same provider s cloud this can lead to legal concerns over jurisdiction while there have been efforts such as us eu safe harbor to harmonize the legal environment providers such as amazon still cater to major markets typically to the united states and the european union by deploying local infrastructure and allowing customers to select regions and availability zones cloud computing poses privacy concerns because the service provider can access the data that is on the cloud at any time it could accidentally or deliberately alter or even delete information this becomes a major concern as these service providers employ administrators which can leave room for potential unwanted disclosure of information on the cloud technical issues sometimes there can be some technical issues like servers might be down so at that time it becomes difficult to gain access to the resources at any time and from anywhere e g non availability of services can be due to denial of service attack to use the technique of cloud computing there should always be a strong internet connection without which we would not be able to take advantage of the cloud computing the other issue related to the cloud computing is that it consumes the great power of physical devices such as a smartphone sharing information without a warrant many cloud providers can share information with third parties if necessary for purposes of law and order even without a warrant that is permitted in their privacy policies which users have to agree to before they start using cloud services there are life threatening situations in which there is no time to wait for the police to issue a warrant many cloud providers can share information immediately with the police in such situations example of a privacy policy that allows this the dropbox privacy policy states that we may share information as discussed below law order we may disclose your information to third parties if we determine that such disclosure is reasonably necessary to a comply with the law b protect any person from death or serious bodily injury c prevent fraud or abuse of dropbox or our users or d protect dropbox s property rights previous situation about this the sydney morning herald reported about the mosman bomb hoax which was a life threatening situation that as to whether nsw police needed a warrant to access the information it was likely to have byrne said it depended on the process taken gmail does set out in their process in terms of their legal disclosure guidelines it can be done by a search warrant but there are exceptions that can apply in different parts of the world and different service providers for example facebook generally provides an exception for emergency life threatening situations that are signed off by law enforcement another computer forensic expert at it4ensics which works for large corporations dealing with matters like internal fraud scott lasak said that police would just contact google and being of a police or fbi background google would assist them whether or not they need to go through warrants or that sort of thing i m not sure but even for just an ip address they might not even need a warrant for something like that being of a police background nsw police would not comment on whether it had received help from google the search giant also declined to comment instead offering a standard statement on how it cooperated with law enforcement a spokesman for the online users lobby group electronic frontiers australia stephen collins said google was likely to have handed over the need information on the basis of probable cause or a warrant which he said was perfectly legitimate he also said it happens with relative frequency such things are rarely used in australia for trivial or malevolent purposes privacy solutions solutions to privacy in cloud computing include policy and legislation as well as end users choices for how data is stored the cloud service provider needs to establish clear and relevant policies that describe how the data of each cloud user will be accessed and used cloud service users can encrypt data that is processed or stored within the cloud to prevent unauthorized access cryptographic encryption mechanisms are certainly the best options in addition authentication and integrity protection mechanisms ensure that data only goes where the customer wants it to go and it is not modified in transit strong authentication is a mandatory requirement for any cloud deployment user authentication is the primary basis for access control and specially in the cloud environment authentication and access control are more important than ever since the cloud and all of its data are publicly accessible biometric identification technologies linking users biometrics information to their data are available these technologies use searchable encryption techniques and perform identification in an encrypted domain so that cloud providers or potential attackers do not gain access to sensitive data or even the contents of the individual queries compliance to comply with regulations including fisma hipaa and sox in the united states the data protection directive in the eu and the credit card industry s pci dss users may have to adopt community or hybrid deployment modes that are typically more expensive and may offer restricted benefits this is how google is able to manage and meet additional government policy requirements beyond fisma and rackspace cloud or qubespace are able to claim pci compliance many providers also obtain a sas 70 type ii audit but this has been criticised on the grounds that the hand picked set of goals and standards determined by the auditor and the auditee are often not disclosed and can vary widely providers typically make this information available on request under non disclosure agreement customers in the eu contracting with cloud providers outside the eu eea have to adhere to the eu regulations on export of personal data a multitude of laws and regulations have forced specific compliance requirements onto many companies that collect generate or store data these policies may dictate a wide array of data storage policies such as how long information must be retained the process used for deleting data and even certain recovery plans below are some examples of compliance laws or regulations united states the health insurance portability and accountability act hipaa requires a contingency plan that includes data backups data recovery and data access during emergencies the privacy laws of switzerland demand that private data including emails be physically stored in switzerland in the united kingdom the civil contingencies act of 2004 sets forth guidance for a business contingency plan that includes policies for data storage in a virtualized cloud computing environment customers may never know exactly where their data is stored in fact data may be stored across multiple data centers in an effort to improve reliability increase performance and provide redundancies this geographic dispersion may make it more difficult to ascertain legal jurisdiction if disputes arise fedramp u s federal agencies have been directed by the office of management and budget to use a process called fedramp federal risk and authorization management program to assess and authorize cloud products and services federal cio steven vanroekel issued a memorandum to federal agency chief information officers on december 8 2011 defining how federal agencies should use fedramp fedramp consists of a subset of nist special publication 800 53 security controls specifically selected to provide protection in cloud environments a subset has been defined for the fips 199 low categorization and the fips 199 moderate categorization the fedramp program has also established a joint accreditation board jab consisting of chief information officers from dod dhs and gsa the jab is responsible for establishing accreditation standards for 3rd party organizations who perform the assessments of cloud solutions the jab also reviews authorization packages and may grant provisional authorization to operate the federal agency consuming the service still has final responsibility for final authority to operate legal as with other changes in the landscape of computing certain legal issues arise with cloud computing including trademark infringement security concerns and sharing of proprietary data resources the electronic frontier foundation has criticized the united states government during the megaupload seizure process for considering that people lose property rights by storing data on a cloud computing service one important but not often mentioned problem with cloud computing is the problem of who is in possession of the data if a cloud company is the possessor of the data the possessor has certain legal rights if the cloud company is the custodian of the data then a different set of rights would apply the next problem in the legalities of cloud computing is the problem of legal ownership of the data many terms of service agreements are silent on the question of ownership these legal issues are not confined to the time period in which the cloud based application is actively being used there must also be consideration for what happens when the provider customer relationship ends in most cases this event will be addressed before an application is deployed to the cloud however in the case of provider insolvencies or bankruptcy the state of the data may become blurred vendor lock in because cloud computing is still relatively new standards are still being developed many cloud platforms and services are proprietary meaning that they are built on the specific standards tools and protocols developed by a particular vendor for its particular cloud offering this can make migrating off a proprietary cloud platform prohibitively complicated and expensive three types of vendor lock in can occur with cloud computing platform lock in cloud services tend to be built on one of several possible virtualization platforms for example vmware or xen migrating from a cloud provider using one platform to a cloud provider using a different platform could be very complicated data lock in since the cloud is still new standards of ownership i e who actually owns the data once it lives on a cloud platform are not yet developed which could make it complicated if cloud computing users ever decide to move data off of a cloud vendor s platform tools lock in if tools built to manage a cloud environment are not compatible with different kinds of both virtual and physical infrastructure those tools will only be able to manage data or apps that live in the vendor s particular cloud environment heterogeneous cloud computing is described as a type of cloud environment that prevents vendor lock in and aligns with enterprise data centers that are operating hybrid cloud models the absence of vendor lock in lets cloud administrators select their choice of hypervisors for specific tasks or to deploy virtualized infrastructures to other enterprises without the need to consider the flavor of hypervisor in the other enterprise a heterogeneous cloud is considered one that includes on premises private clouds public clouds and software as a service clouds heterogeneous clouds can work with environments that are not virtualized such as traditional data centers heterogeneous clouds also allow for the use of piece parts such as hypervisors servers and storage from multiple vendors cloud piece parts such as cloud storage systems offer apis but they are often incompatible with each other the result is complicated migration between backends and makes it difficult to integrate data spread across various locations this has been described as a problem of vendor lock in the solution to this is for clouds to adopt common standards heterogeneous cloud computing differs from homogeneous clouds which have been described as those using consistent building blocks supplied by a single vendor intel general manager of high density computing jason waxman is quoted as saying that a homogeneous system of 15 000 servers would cost 6 million more in capital expenditure and use 1 megawatt of power service lock in within the same vendor service lock in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor making it challenging to switch to alternative services within the same vendor when their needs change open source open source software has provided the foundation for many cloud computing implementations prominent examples being the hadoop framework and vmware s cloud foundry in november 2007 the free software foundation released the affero general public license a version of gplv3 intended to close a perceived legal loophole associated with free software designed to run over a network open standards most cloud providers expose apis that are typically well documented often under a creative commons license but also unique to their implementation and thus not interoperable some vendors have adopted others apis and there are a number of open standards under development with a view to delivering interoperability and portability as of november 2012 the open standard with broadest industry support is probably openstack founded in 2010 by nasa and rackspace and now governed by the openstack foundation openstack supporters include amd intel canonical suse linux red hat cisco dell hp ibm yahoo huawei and now vmware security security is generally a desired state of being free from harm anything that compromises the state of an entity s well being as defined in information security it is a condition in which an information asset is protected against its confidentiality quality or state of being free from unauthorized or insecure disclosure contrary to the defined access rights as listed in the access control list and or matrix integrity a quality or state of being whole as complete as original and uncorrupted as functionally proven by the hash integrity values and availability a desired state of an information resource being accessible only by authorized parties as listed in access control list and or matrix in the desired state and at the right time security is an important domain in as far as cloud computing is concerned there are a number of issues to be addressed if the cloud is to be perfectly secure a condition i doubt will ever be achieved martin muduva 2015 as cloud computing is achieving increased popularity concerns are being voiced about the security issues introduced through adoption of this new model the effectiveness and efficiency of traditional protection mechanisms are being reconsidered as the characteristics of this innovative deployment model can differ widely from those of traditional architectures an alternative perspective on the topic of cloud security is that this is but another although quite broad case of applied security and that similar security principles that apply in shared multi user mainframe security models apply with cloud security the relative security of cloud computing services is a contentious issue that may be delaying its adoption physical control of the private cloud equipment is more secure than having the equipment off site and under someone else s control physical control and the ability to visually inspect data links and access ports is required in order to ensure data links are not compromised issues barring the adoption of cloud computing are due in large part to the private and public sectors unease surrounding the external management of security based services it is the very nature of cloud computing based services private or public that promote external management of provided services this delivers great incentive to cloud computing service providers to prioritize building and maintaining strong management of secure services security issues have been categorized into sensitive data access data segregation privacy bug exploitation recovery accountability malicious insiders management console security account control and multi tenancy issues solutions to various cloud security issues vary from cryptography particularly public key infrastructure pki to use of multiple cloud providers standardization of apis and improving virtual machine support and legal support cloud computing offers many benefits but is vulnerable to threats as cloud computing uses increase it is likely that more criminals find new ways to exploit system vulnerabilities many underlying challenges and risks in cloud computing increase the threat of data compromise to mitigate the threat cloud computing stakeholders should invest heavily in risk assessment to ensure that the system encrypts to protect data establishes trusted foundation to secure the platform and infrastructure and builds higher assurance into auditing to strengthen compliance security concerns must be addressed to maintain trust in cloud computing technology data breach is one of the big concerns in cloud computing a compromised server could significantly harm the users as well as cloud providers a variety of information could be stolen these include credit card and social security numbers addresses and personal messages the u s now requires cloud providers to notify customers of breaches once notified customers now have to worry about identity theft and fraud while providers have to deal with federal investigations lawsuits and reputational damage customer lawsuits and settlements have resulted in over 1 billion in losses to cloud providers availability a cloud provider may shut down without warning for instance the anki robot company suddenly went bankrupt in 2019 making 1 5 million robots unresponsive to voice command sustainability although cloud computing is often assumed to be a form of green computing there is currently no way to measure how green computers are the primary environmental problem associated with the cloud is energy use phil radford of greenpeace said we are concerned that this new explosion in electricity use could lock us into old polluting energy sources instead of the clean energy available today greenpeace ranks the energy usage of the top ten big brands in cloud computing and successfully urged several companies to switch to clean energy on december 15 2011 greenpeace and facebook announced together that facebook would shift to use clean and renewable energy to power its own operations soon thereafter apple agreed to make all of its data centers coal free by the end of 2013 and doubled the amount of solar energy powering its maiden nc data center following suit salesforce agreed to shift to 100 clean energy by 2020 citing the servers effects on the environmental effects of cloud computing in areas where climate favors natural cooling and renewable electricity is readily available the environmental effects will be more moderate the same holds true for traditional data centers thus countries with favorable conditions such as finland sweden and switzerland are trying to attract cloud computing data centers energy efficiency in cloud computing can result from energy aware scheduling and server consolidation however in the case of distributed clouds over data centers with different sources of energy including renewable energy the use of energy efficiency reduction could result in a significant carbon footprint reduction abuse as with privately purchased hardware customers can purchase the services of cloud computing for nefarious purposes this includes password cracking and launching attacks using the purchased services in 2009 a banking trojan illegally used the popular amazon service as a command and control channel that issued software updates and malicious instructions to pcs that were infected by the malware it governance the introduction of cloud computing requires an appropriate it governance model to ensure a secured computing environment and to comply with all relevant organizational information technology policies as such organizations need a set of capabilities that are essential when effectively implementing and managing cloud services including demand management relationship management data security management application lifecycle management risk and compliance management a danger lies with the explosion of companies joining the growth in cloud computing by becoming providers however many of the infrastructural and logistical concerns regarding the operation of cloud computing businesses are still unknown this over saturation may have ramifications for the industry as a whole consumer end storage the increased use of cloud computing could lead to a reduction in demand for high storage capacity consumer end devices due to cheaper low storage devices that stream all content via the cloud becoming more popular in a wired article jake gardner explains that while unregulated usage is beneficial for it and tech moguls like amazon the anonymous nature of the cost of consumption of cloud usage makes it difficult for business to evaluate and incorporate it into their business plans ambiguity of terminology outside of the information technology and software industry the term cloud can be found to reference a wide range of services some of which fall under the category of cloud computing while others do not the cloud is often used to refer to a product or service that is discovered accessed and paid for over the internet but is not necessarily a computing resource the term cloud retains the aura of something noumenal and numinous examples of service that are sometimes referred to as the cloud include but are not limited to crowd sourcing cloud printing crowd funding cloud manufacturing performance interference and noisy neighbors due to its multi tenant nature and resource sharing cloud computing must also deal with the noisy neighbor effect this effect in essence indicates that in a shared infrastructure the activity of a virtual machine on a neighboring core on the same physical host may lead to increased performance degradation of the vms in the same physical host due to issues such as e g cache contamination due to the fact that the neighboring vms may be activated or deactivated at arbitrary times the result is an increased variation in the actual performance of cloud resources this effect seems to be dependent on the nature of the applications that run inside the vms but also other factors such as scheduling parameters and the careful selection may lead to optimized assignment in order to minimize the phenomenon this has also led to difficulties in comparing various cloud providers on cost and performance using traditional benchmarks for service and application performance as the time period and location in which the benchmark is performed can result in widely varied results this observation has led in turn to research efforts to make cloud computing applications intrinsically aware of changes in the infrastructure so that the application can automatically adapt to avoid failure monopolies and privatization of cyberspace philosopher slavoj žižek points out that although cloud computing enhances content accessibility this access is increasingly grounded in the virtually monopolistic privatization of the cloud which provides this access according to him this access necessarily mediated through a handful of companies ensures a progressive privatization of global cyberspace žižek criticizes the argument purported by supporters of cloud computing that this phenomenon is part of the natural evolution of the internet sustaining that the quasi monopolies set prices at will but also filter the software they provide to give its universality a particular twist depending on commercial and ideological interests limitations of service level agreements typically cloud providers service level agreements slas do not encompass all forms of service interruptions exclusions typically include planned maintenance downtime resulting from external factors such as network issues human errors like misconfigurations natural disasters force majeure events or security breaches typically customers bear the responsibility of monitoring sla compliance and must file claims for any unmet slas within a designated timeframe customers should be aware of how deviations from slas are calculated as these parameters may vary by service these requirements can place a considerable burden on customers additionally sla percentages and conditions can differ across various services within the same provider with some services lacking any sla altogether in cases of service interruptions due to hardware failures in the cloud provider the company typically does not offer monetary compensation instead eligible users may receive credits as outlined in the corresponding sla cloud cost overruns in a report by gartner a survey of 200 it leaders revealed that 69 experienced budget overruns in their organizations cloud expenditures during 2023 conversely 31 of it leaders whose organizations stayed within budget attributed their success to accurate forecasting and budgeting proactive monitoring of spending and effective optimization the 2024 flexera state of cloud report identifies the top cloud challenges as managing cloud spend followed by security concerns and lack of expertise public cloud expenditures exceeded budgeted amounts by an average of 15 the report also reveals that cost savings is the top cloud initiative for 60 of respondents furthermore 65 measure cloud progress through cost savings while 42 prioritize shorter time to market indicating that cloud s promise of accelerated deployment is often overshadowed by cost concerns implementation challenges applications hosted in the cloud are susceptible to the fallacies of distributed computing a series of misconceptions that can lead to significant issues in software development and deployment see also cloud computing references ",
            "total_words": 5203,
            "unique_words_percentage": 29.6367480299827,
            "stopwords_percentage": 37.01710551604843
        },
        {
            "title": "History of cloud computing",
            "link": "https://en.wikipedia.org/wiki/History_of_cloud_computing",
            "content": "the concept of the cloud computing as a platform for distributed computing traces its roots back to 1993 at that time apple spin off general magic and at t utilized the term in the context of their telescript and personal link technologies in an april 1994 feature by wired titled bill and andy s excellent adventure ii andy hertzfeld elaborated on telescript general magic s distributed programming language he described the expansive potential of the cloud the beauty of telescript is that now instead of just having a device to program we now have the entire cloud out there where a single program can go and travel to many different sources of information and create a sort of a virtual service no one had conceived that before the example jim white uses now is a date arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show and everything is communicated to both parties early history in 1963 the defense advanced research projects agency darpa funded project mac the first computer time sharing system during the 1960s the initial concepts of time sharing became popularized via remote job entry rje this terminology was mostly associated with large vendors such as ibm and dec full time sharing solutions were available by the early 1970s on such platforms as multics on ge hardware cambridge ctss and the earliest unix ports on dec hardware yet the data center model where users submitted jobs to operators to run on ibm mainframes was overwhelmingly predominant in the late 1980s the invention of the world wide web led to internet expansion and on premises data centers in the 1990s telecommunications companies who previously offered primarily dedicated point to point data circuits began offering virtual private network vpn services with comparable quality of service but at a lower cost by switching traffic as they saw fit to balance server use they could use overall network bandwidth more effectively they began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for cloud computing extended this boundary to cover all servers as well as the network infrastructure as computers became more diffused scientists and technologists explored ways to make large scale computing power available to more users through time sharing they experimented with algorithms to optimize the infrastructure platform and applications to prioritize tasks to be executed by cpus and to increase efficiency for end users at the same time application service providers became popular and later evolved into software as a service saas in 1999 medidata launched rave the first electronic data capture software for clinical data the use of the cloud metaphor for virtualized services dates at least to general magic in 1994 where it was used to describe the universe of places that mobile agents in the telescript environment could go as described by andy hertzfeld the beauty of telescript says andy is that now instead of just having a device to program we now have the entire cloud out there where a single program can go and travel to many different sources of information and create a sort of a virtual service the use of the cloud metaphor is credited to general magic communications employee david hoffman based on long standing use in networking and telecom in addition to use by general magic itself it was also used in promoting at t s associated personal link services 2000s in 2002 amazon established its subsidiary amazon web services which allows developers to build applications independently in 2006 amazon introduced simple storage service s3 in march and elastic compute cloud ec2 in august these services were among the first to use server virtualization to provide iaas on a pay as you go basis in the same year google launched google docs a saas model to edit and save documents online in 2007 netflix launches its online video streaming service the first saas streaming site also ibm and google partnered with universities university of washington carnegie mellon university mit stanford university of maryland and uc berkeley to create a research server farm this would later become the cluster exploratory program when the national science foundation funded the project in early 2008 in april of 2008 google released the beta version of google app engine a paas that provides a fully managed infrastructure and platform for users to create web applications in mid 2018 gartner noted the potential for cloud computing to reshape the relationship between it service consumers users and providers nasa s nebula becomes the first open source software for deploying private and hybrid clouds in early 2009 later in the same year the french government announced the andromède project to establish a national cloud computing service the government committed 285 million to the initiative the initiative ultimately failed leading to the shutdown of cloudwatt on 1 february 2020 2010s in february 2010 microsoft launched microsoft azure in february following its announcement in october 2008 five months later rackspace hosting and nasa initiated an open source cloud software project openstack this project aimed to facilitate organizations in offering cloud computing services on standard hardware the early codebase was sourced from nasa s nebula platform and rackspace s cloud files platform in march of 2011 ibm introduced the ibm smartcloud framework designed to support the smarter planet initiative later that year the us government established the federal risk management program fedramp becoming the first government wide cloud services accreditation program with standardized risk assessment methodologies for cloud products and services later on october 12 icloud was launched allowing users to store personal information across multiple devices and share with other users in june 2012 on june 7 oracle announced the oracle cloud in may google compute engine was released in preview and subsequently rolled out into general availability in december 2013 also in 2013 docker launched as a paas model to host containers in the cloud for software development in december 2019 amazon launched aws outposts a service that extends aws infrastructure services apis and tools to customer datacenters co location spaces or on premises facilities 2020s since the global pandemic of 2020 cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees notably remote workers for example zoom grew over 160 in 2020 alone security and privacy are still a major concern due to security breaches and one of the main focuses of research cloudchain a cloud oriented blockchain system is designed to increase the layers of security currently global spending on cloud computing services has reached 706 billion and the international data corporation predicts it to reach 1 3 trillion by 2025 references further reading yeo shinjoung 2023 behind the search box google and the global internet industry u of illinois press 2023 isbn 10 0252087127 online",
            "total_words": 1164,
            "unique_words_percentage": 46.391752577319586,
            "stopwords_percentage": 36.08247422680412
        },
        {
            "title": "Cloud computing architecture",
            "link": "https://en.wikipedia.org/wiki/Cloud_computing_architecture",
            "content": "cloud computing architecture refers to the components and subcomponents required for cloud computing these components typically consist of a front end platform fat client thin client mobile back end platforms servers storage a cloud based delivery and a network internet intranet intercloud combined these components make up cloud computing architecture client platforms cloud computing architectures consist of front end platforms called clients or cloud clients these clients are servers fat or thick clients thin clients zero clients tablets and mobile devices that users directly interact with these client platforms interact with the cloud data storage via an application middle ware via a web browser or through a virtual session virtual sessions in particular require secure encryption algorithm frame working which spans the entire interface zero client the zero or ultra thin client initializes the network to gather required configuration files that then tell it where its os binaries are stored the entire zero client device runs via the network this creates a single point of failure in that if the network goes down the device is rendered useless storage an online network storage where data is stored and accessible to multiple clients cloud storage is generally deployed in the following configurations public cloud private cloud community cloud or some combination of the three also known as hybrid cloud in order to be effective the cloud storage needs to be agile flexible scalable multi tenancy and secure delivery software as a service saas the software as a service saas service model involves the cloud provider installing and maintaining software in the cloud and users running the software from cloud over the internet or intranet the users client machines require no installation of any application specific software since cloud applications run in the cloud saas is scalable and system administrators may load the applications on several servers in the past each customer would purchase and load their own copy of the application to each of their own servers but with the saas the customer can access the application without installing the software locally saas typically involves a monthly or annual fee software as a service provides the equivalent of installed applications in the traditional non cloud computing delivery of applications software as a service has four common approaches single instance multi instance multi tenant flex tenancy of these flex tenancy is considered the most user adaptive saas paradigm in designated multi input four way manifold models such systems are based on simplified encryption methods that target listed data sequences over multiple passes the simplicity of this concept makes flex tenancy saas popular among those without informatics processing experience such as basic maintenance and custodial staff in franchise businesses development as a service daas development as a service is web based community shared tool set this is the equivalent to locally installed development tools in the traditional non cloud computing delivery of development tools data as a service daas data as a service web based design construct where cloud data is accessed through a defined api layer daas services are often considered as a specialized subset of a software as a service saas offering platform as a service paas platform as a service is cloud computing service which provides the users with application platforms and databases as a service this is equivalent to middleware in the traditional non cloud computing delivery of application platforms and databases infrastructure as a service iaas infrastructure as a service is taking the physical hardware and going completely virtual e g all servers networks storage and system management all existing in the cloud this is the equivalent to infrastructure and hardware in the traditional non cloud computing method running in the cloud in other words businesses pay a fee monthly or annually to run virtual servers networks storage from the cloud this will mitigate the need for a data center heating cooling and maintaining hardware at the local level networking generally the cloud network layer should offer high bandwidth and low latency allowing users to have uninterrupted access to their data and applications agile network on demand access to resources requires the ability to move quickly and efficiently between servers and possibly even clouds network security security is always important but when you are dealing with multi tenancy it becomes much more important because you re dealing with segregating multiple customers see also cloud collaboration cloud computing cloud computing comparison cloud database cloud storage further reading reese g 2009 cloud application architectures building applications and infrastructure in the cloud sebastopol ca o reilly media inc 2009 rhoton j and haukioja r 2011 cloud computing architected solution design handbook recursive limited 2011 isbn 0 9563556 1 7 shroff dr gautam enterprise cloud computing technology architecture applications references ",
            "total_words": 788,
            "unique_words_percentage": 41.75126903553299,
            "stopwords_percentage": 33.37563451776649
        }
    ],
    "Cybersecurity": [
        {
            "title": "Computer security",
            "link": "https://en.wikipedia.org/wiki/Computer_security",
            "content": "computer security also cybersecurity digital security or information technology it security is the protection of computer software systems and networks from threats that can lead to unauthorized information disclosure theft or damage to hardware software or data as well as from the disruption or misdirection of the services they provide the significance of the field stems from the expanded reliance on computer systems the internet and wireless network standards its importance is further amplified by the growth of smart devices including smartphones televisions and the various devices that constitute the internet of things iot cybersecurity has emerged as one of the most significant new challenges facing the contemporary world due to both the complexity of information systems and the societies they support security is particularly crucial for systems that govern large scale systems with far reaching physical effects such as power distribution elections and finance although many aspects of computer security involve digital security such as electronic passwords and encryption physical security measures such as metal locks are still used to prevent unauthorized tampering it security is not a perfect subset of information security therefore does not completely align into the security convergence schema vulnerabilities and attacks a vulnerability refers to a flaw in the structure execution functioning or internal oversight of a computer or system that compromises its security most of the vulnerabilities that have been discovered are documented in the common vulnerabilities and exposures cve database an exploitable vulnerability is one for which at least one working attack or exploit exists actors maliciously seeking vulnerabilities are known as threats vulnerabilities can be researched reverse engineered hunted or exploited using automated tools or customized scripts various people or parties are vulnerable to cyber attacks however different groups are likely to experience different types of attacks more than others in april 2023 the united kingdom department for science innovation technology released a report on cyber attacks over the previous 12 months they surveyed 2 263 uk businesses 1 174 uk registered charities and 554 education institutions the research found that 32 of businesses and 24 of charities overall recall any breaches or attacks from the last 12 months these figures were much higher for medium businesses 59 large businesses 69 and high income charities with 500 000 or more in annual income 56 yet although medium or large businesses are more often the victims since larger companies have generally improved their security over the last decade small and midsize businesses smbs have also become increasingly vulnerable as they often do not have advanced tools to defend the business smbs are most likely to be affected by malware ransomware phishing man in the middle attacks and denial of service dos attacks normal internet users are most likely to be affected by untargeted cyberattacks these are where attackers indiscriminately target as many devices services or users as possible they do this using techniques that take advantage of the openness of the internet these strategies mostly include phishing ransomware water holing and scanning to secure a computer system it is important to understand the attacks that can be made against it and these threats can typically be classified into one of the following categories backdoor a backdoor in a computer system a cryptosystem or an algorithm is any secret method of bypassing normal authentication or security controls these weaknesses may exist for many reasons including original design or poor configuration due to the nature of backdoors they are of greater concern to companies and databases as opposed to individuals backdoors may be added by an authorized party to allow some legitimate access or by an attacker for malicious reasons criminals often use malware to install backdoors giving them remote administrative access to a system once they have access cybercriminals can modify files steal personal information install unwanted software and even take control of the entire computer backdoors can be very hard to detect and are usually discovered by someone who has access to the application source code or intimate knowledge of the operating system of the computer denial of service attack denial of service attacks dos are designed to make a machine or network resource unavailable to its intended users attackers can deny service to individual victims such as by deliberately entering a wrong password enough consecutive times to cause the victim s account to be locked or they may overload the capabilities of a machine or network and block all users at once while a network attack from a single ip address can be blocked by adding a new firewall rule many forms of distributed denial of service ddos attacks are possible where the attack comes from a large number of points in this case defending against these attacks is much more difficult such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques including distributed reflective denial of service drdos where innocent systems are fooled into sending traffic to the victim with such attacks the amplification factor makes the attack easier for the attacker because they have to use little bandwidth themselves to understand why attackers may carry out these attacks see the attacker motivation section physical access attacks a direct access attack is when an unauthorized user an attacker gains physical access to a computer most likely to directly copy data from it or steal information attackers may also compromise security by making operating system modifications installing software worms keyloggers covert listening devices or using wireless microphones even when the system is protected by standard security measures these may be bypassed by booting another operating system or tool from a cd rom or other bootable media disk encryption and the trusted platform module standard are designed to prevent these attacks direct service attackers are related in concept to direct memory attacks which allow an attacker to gain direct access to a computer s memory the attacks take advantage of a feature of modern computers that allows certain devices such as external hard drives graphics cards or network cards to access the computer s memory directly eavesdropping eavesdropping is the act of surreptitiously listening to a private computer conversation communication usually between hosts on a network it typically occurs when a user connects to a network where traffic is not secured or encrypted and sends sensitive business data to a colleague which when listened to by an attacker could be exploited data transmitted across an open network allows an attacker to exploit a vulnerability and intercept it via various methods unlike malware direct access attacks or other forms of cyber attacks eavesdropping attacks are unlikely to negatively affect the performance of networks or devices making them difficult to notice in fact the attacker does not need to have any ongoing connection to the software at all the attacker can insert the software onto a compromised device perhaps by direct insertion or perhaps by a virus or other malware and then come back some time later to retrieve any data that is found or trigger the software to send the data at some determined time using a virtual private network vpn which encrypts data between two points is one of the most common forms of protection against eavesdropping using the best form of encryption possible for wireless networks is best practice as well as using https instead of an unencrypted http programs such as carnivore and narusinsight have been used by the federal bureau of investigation fbi and nsa to eavesdrop on the systems of internet service providers even machines that operate as a closed system i e with no contact with the outside world can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware tempest is a specification by the nsa referring to these attacks malware malicious software malware is any software code or computer program intentionally written to harm a computer system or its users once present on a computer it can leak sensitive details such as personal information business information and passwords can give control of the system to the attacker and can corrupt or delete data permanently another type of malware is ransomware which is when malware installs itself onto a victim s machine encrypts their files and then turns around and demands a ransom usually in bitcoin to return that data to the user types of malware include some of the following viruses are a specific type of malware and are normally a malicious code that hijacks software with the intention to do damage and spread copies of itself copies are made with the aim to spread to other programs on a computer worms are similar to viruses however viruses can only function when a user runs opens a compromised program worms are self replicating malware that spread between programs apps and devices without the need for human interaction trojan horses are programs that pretend to be helpful or hide themselves within desired or legitimate software to trick users into installing them once installed a rat remote access trojan can create a secret backdoor on the affected device to cause damage spyware is a type of malware that secretly gathers information from an infected computer and transmits the sensitive information back to the attacker one of the most common forms of spyware are keyloggers which record all of a user s keyboard inputs keystrokes to allow hackers to harvest usernames passwords bank account and credit card numbers scareware as the name suggests is a form of malware which uses social engineering manipulation to scare shock trigger anxiety or suggest the perception of a threat in order to manipulate users into buying or installing unwanted software these attacks often begin with a sudden pop up with an urgent message usually warning the user that they ve broken the law or their device has a virus man in the middle attacks man in the middle attacks mitm involve a malicious attacker trying to intercept surveil or modify communications between two parties by spoofing one or both party s identities and injecting themselves in between types of mitm attacks include ip address spoofing is where the attacker hijacks routing protocols to reroute the targets traffic to a vulnerable network node for traffic interception or injection message spoofing via email sms or ott messaging is where the attacker spoofs the identity or carrier service while the target is using messaging protocols like email sms or ott ip based messaging apps the attacker can then monitor conversations launch social attacks or trigger zero day vulnerabilities to allow for further attacks wifi ssid spoofing is where the attacker simulates a wifi base station ssid to capture and modify internet traffic and transactions the attacker can also use local network addressing and reduced network defenses to penetrate the target s firewall by breaching known vulnerabilities sometimes known as a pineapple attack thanks to a popular device see also malicious association dns spoofing is where attackers hijack domain name assignments to redirect traffic to systems under the attackers control in order to surveil traffic or launch other attacks ssl hijacking typically coupled with another media level mitm attack is where the attacker spoofs the ssl authentication and encryption protocol by way of certificate authority injection in order to decrypt surveil and modify traffic see also tls interception multi vector polymorphic attacks surfacing in 2017 a new class of multi vector polymorphic cyber threats combine several types of attacks and change form to avoid cybersecurity controls as they spread multi vector polymorphic attacks as the name describes are both multi vectored and polymorphic firstly they are a singular attack that involves multiple methods of attack in this sense they are multi vectored i e the attack can use multiple means of propagation such as via the web email and applications however they are also multi staged meaning that they can infiltrate networks and move laterally inside the network the attacks can be polymorphic meaning that the cyberattacks used such as viruses worms or trojans constantly change morph making it nearly impossible to detect them using signature based defences phishing phishing is the attempt of acquiring sensitive information such as usernames passwords and credit card details directly from users by deceiving the users phishing is typically carried out by email spoofing instant messaging text message or on a phone call they often direct users to enter details at a fake website whose look and feel are almost identical to the legitimate one the fake website often asks for personal information such as login details and passwords this information can then be used to gain access to the individual s real account on the real website preying on a victim s trust phishing can be classified as a form of social engineering attackers can use creative ways to gain access to real accounts a common scam is for attackers to send fake electronic invoices to individuals showing that they recently purchased music apps or others and instructing them to click on a link if the purchases were not authorized a more strategic type of phishing is spear phishing which leverages personal or organization specific details to make the attacker appear like a trusted source spear phishing attacks target specific individuals rather than the broad net cast by phishing attempts privilege escalation privilege escalation describes a situation where an attacker with some level of restricted access is able to without authorization elevate their privileges or access level for example a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data or even become root and have full unrestricted access to a system the severity of attacks can range from attacks simply sending an unsolicited email to a ransomware attack on large amounts of data privilege escalation usually starts with social engineering techniques often phishing privilege escalation can be separated into two strategies horizontal and vertical privilege escalation horizontal escalation or account takeover is where an attacker gains access to a normal user account that has relatively low level privileges this may be through stealing the user s username and password once they have access they have gained a foothold and using this foothold the attacker then may move around the network of users at this same lower level gaining access to information of this similar privilege vertical escalation however targets people higher up in a company and often with more administrative power such as an employee in it with a higher privilege using this privileged account will then enable the attacker to invade other accounts side channel attack any computational system affects its environment in some form this effect it has on its environment can range from electromagnetic radiation to residual effect on ram cells which as a consequence make a cold boot attack possible to hardware implementation faults that allow for access or guessing of other values that normally should be inaccessible in side channel attack scenarios the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure the target information in a side channel can be challenging to detect due to its low amplitude when combined with other signals social engineering social engineering in the context of computer security aims to convince a user to disclose secrets such as passwords card numbers etc or grant physical access by for example impersonating a senior executive bank a contractor or a customer this generally involves exploiting people s trust and relying on their cognitive biases a common scam involves emails sent to accounting and finance department personnel impersonating their ceo and urgently requesting some action one of the main techniques of social engineering are phishing attacks in early 2016 the fbi reported that such business email compromise bec scams had cost us businesses more than 2 billion in about two years in may 2016 the milwaukee bucks nba team was the victim of this type of cyber scam with a perpetrator impersonating the team s president peter feigin resulting in the handover of all the team s employees 2015 w 2 tax forms spoofing spoofing is an act of pretending to be a valid entity through the falsification of data such as an ip address or username in order to gain access to information or resources that one is otherwise unauthorized to obtain spoofing is closely related to phishing there are several types of spoofing including email spoofing is where an attacker forges the sending from or source address of an email ip address spoofing where an attacker alters the source ip address in a network packet to hide their identity or impersonate another computing system mac spoofing where an attacker modifies the media access control mac address of their network interface controller to obscure their identity or to pose as another biometric spoofing where an attacker produces a fake biometric sample to pose as another user address resolution protocol arp spoofing where an attacker sends spoofed address resolution protocol onto a local area network to associate their media access control address with a different host s ip address this causes data to be sent to the attacker rather than the intended host in 2018 the cybersecurity firm trellix published research on the life threatening risk of spoofing in the healthcare industry tampering tampering describes a malicious modification or alteration of data it is an intentional but unauthorized act resulting in the modification of a system components of systems its intended behavior or data so called evil maid attacks and security services planting of surveillance capability into routers are examples html smuggling html smuggling allows an attacker to smuggle a malicious code inside a particular html or web page html files can carry payloads concealed as benign inert data in order to defeat content filters these payloads can be reconstructed on the other side of the filter when a target user opens the html the malicious code is activated the web browser then decodes the script which then unleashes the malware onto the target s device information security practices employee behavior can have a big impact on information security in organizations cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization information security culture is the totality of patterns of behavior in an organization that contributes to the protection of information of all kinds andersson and reimers 2014 found that employees often do not see themselves as part of their organization s information security effort and often take actions that impede organizational changes indeed the verizon data breach investigations report 2020 which examined 3 950 security breaches discovered 30 of cybersecurity incidents involved internal actors within a company research shows information security culture needs to be improved continuously in information security culture from analysis to change authors commented it s a never ending process a cycle of evaluation and change or maintenance to manage the information security culture five steps should be taken pre evaluation strategic planning operative planning implementation and post evaluation pre evaluation to identify the awareness of information security within employees and to analyze the current security policies strategic planning to come up with a better awareness program clear targets need to be set assembling a team of skilled professionals is helpful to achieve it operative planning a good security culture can be established based on internal communication management buy in security awareness and a training program implementation four stages should be used to implement the information security culture they are commitment of the management communication with organizational members courses for all organizational members commitment of the employees post evaluation to assess the success of the planning and implementation and to identify unresolved areas of concern computer protection countermeasures in computer security a countermeasure is an action device procedure or technique that reduces a threat a vulnerability or an attack by eliminating or preventing it by minimizing the harm it can cause or by discovering and reporting it so that corrective action can be taken some common countermeasures are listed in the following sections security by design security by design or alternately secure by design means that the software has been designed from the ground up to be secure in this case security is considered a main feature the uk government s national cyber security centre separates secure cyber design principles into five sections before a secure system is created or updated companies should ensure they understand the fundamentals and the context around the system they are trying to create and identify any weaknesses in the system companies should design and centre their security around techniques and defences which make attacking their data or systems inherently more challenging for attackers companies should ensure that their core services that rely on technology are protected so that the systems are essentially never down although systems can be created which are safe against a multitude of attacks that does not mean that attacks will not be attempted despite one s security all companies systems should aim to be able to detect and spot attacks as soon as they occur to ensure the most effective response to them companies should create secure systems designed so that any attack that is successful has minimal severity these design principles of security by design can include some of the following techniques the principle of least privilege where each part of the system has only the privileges that are needed for its function that way even if an attacker gains access to that part they only have limited access to the whole system automated theorem proving to prove the correctness of crucial software subsystems code reviews and unit testing approaches to make modules more secure where formal correctness proofs are not possible defense in depth where the design is such that more than one subsystem needs to be violated to compromise the integrity of the system and the information it holds default secure settings and design to fail secure rather than fail insecure see fail safe for the equivalent in safety engineering ideally a secure system should require a deliberate conscious knowledgeable and free decision on the part of legitimate authorities in order to make it insecure audit trails track system activity so that when a security breach occurs the mechanism and extent of the breach can be determined storing audit trails remotely where they can only be appended to can keep intruders from covering their tracks full disclosure of all vulnerabilities to ensure that the window of vulnerability is kept as short as possible when bugs are discovered security architecture security architecture can be defined as the practice of designing computer systems to achieve security goals these goals have overlap with the principles of security by design explored above including to make initial compromise of the system difficult and to limit the impact of any compromise in practice the role of a security architect would be to ensure the structure of a system reinforces the security of the system and that new changes are safe and meet the security requirements of the organization similarly techopedia defines security architecture as a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment it also specifies when and where to apply security controls the design process is generally reproducible the key attributes of security architecture are the relationship of different components and how they depend on each other determination of controls based on risk assessment good practices finances and legal matters the standardization of controls practicing security architecture provides the right foundation to systematically address business it and security concerns in an organization security measures a state of computer security is the conceptual ideal attained by the use of three processes threat prevention detection and response these processes are based on various policies and system components which include the following limiting the access of individuals using user account access controls and using cryptography can protect systems files and data respectively firewalls are by far the most common prevention systems from a network security perspective as they can if properly configured shield access to internal network services and block certain kinds of attacks through packet filtering firewalls can be both hardware and software based firewalls monitor and control incoming and outgoing traffic of a computer network and establish a barrier between a trusted network and an untrusted network intrusion detection system ids products are designed to detect network attacks in progress and assist in post attack forensics while audit trails and logs serve a similar function for individual systems response is necessarily defined by the assessed security requirements of an individual system and may cover the range from simple upgrade of protections to notification of legal authorities counter attacks and the like in some special cases the complete destruction of the compromised system is favored as it may happen that not all the compromised resources are detected cyber security awareness training to cope with cyber threats and attacks forward web proxy solutions can prevent the client to visit malicious web pages and inspect the content before downloading to the client machines today computer security consists mainly of preventive measures like firewalls or an exit procedure a firewall can be defined as a way of filtering network data between a host or a network and another network such as the internet they can be implemented as software running on the machine hooking into the network stack or in the case of most unix based operating systems such as linux built into the operating system kernel to provide real time filtering and blocking another implementation is a so called physical firewall which consists of a separate machine filtering network traffic firewalls are common amongst machines that are permanently connected to the internet some organizations are turning to big data platforms such as apache hadoop to extend data accessibility and machine learning to detect advanced persistent threats in order to ensure adequate security the confidentiality integrity and availability of a network better known as the cia triad must be protected and is considered the foundation to information security to achieve those objectives administrative physical and technical security measures should be employed the amount of security afforded to an asset can only be determined when its value is known vulnerability management vulnerability management is the cycle of identifying fixing or mitigating vulnerabilities especially in software and firmware vulnerability management is integral to computer security and network security vulnerabilities can be discovered with a vulnerability scanner which analyzes a computer system in search of known vulnerabilities such as open ports insecure software configuration and susceptibility to malware in order for these tools to be effective they must be kept up to date with every new update the vendor release typically these updates will scan for the new vulnerabilities that were introduced recently beyond vulnerability scanning many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities in some sectors this is a contractual requirement reducing vulnerabilities the act of assessing and reducing vulnerabilities to cyber attacks is commonly referred to as information technology security assessments they aim to assess systems for risk and to predict and test for their vulnerabilities while formal verification of the correctness of computer systems is possible it is not yet common operating systems formally verified include sel4 and sysgo s pikeos but these make up a very small percentage of the market it is possible to reduce an attacker s chances by keeping systems up to date with security patches and updates and by hiring people with expertise in security large companies with significant threats can hire security operations centre soc analysts these are specialists in cyber defences with their role ranging from conducting threat analysis to investigating reports of any new issues and preparing and testing disaster recovery plans whilst no measures can completely guarantee the prevention of an attack these measures can help mitigate the damage of possible attacks the effects of data loss damage can be also reduced by careful backing up and insurance outside of formal assessments there are various methods of reducing vulnerabilities two factor authentication is a method for mitigating unauthorized access to a system or sensitive information it requires something you know a password or pin and something you have a card dongle cellphone or another piece of hardware this increases security as an unauthorized person needs both of these to gain access protecting against social engineering and direct computer access physical attacks can only happen by non computer means which can be difficult to enforce relative to the sensitivity of the information training is often involved to help mitigate this risk by improving people s knowledge of how to protect themselves and by increasing people s awareness of threats however even in highly disciplined environments e g military organizations social engineering attacks can still be difficult to foresee and prevent inoculation derived from inoculation theory seeks to prevent social engineering and other fraudulent tricks and traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts hardware protection mechanisms hardware based or assisted computer security also offers an alternative to software only computer security using devices and methods such as dongles trusted platform modules intrusion aware cases drive locks disabling usb ports and mobile enabled access may be considered more secure due to the physical access or sophisticated backdoor access required in order to be compromised each of these is covered in more detail below usb dongles are typically used in software licensing schemes to unlock software capabilities but they can also be seen as a way to prevent unauthorized access to a computer or other device s software the dongle or key essentially creates a secure encrypted tunnel between the software application and the key the principle is that an encryption scheme on the dongle such as advanced encryption standard aes provides a stronger measure of security since it is harder to hack and replicate the dongle than to simply copy the native software to another machine and use it another security application for dongles is to use them for accessing web based content such as cloud software or virtual private networks vpns in addition a usb dongle can be configured to lock or unlock a computer trusted platform modules tpms secure devices by integrating cryptographic capabilities onto access devices through the use of microprocessors or so called computers on a chip tpms used in conjunction with server side software offer a way to detect and authenticate hardware devices preventing unauthorized network and data access computer case intrusion detection refers to a device typically a push button switch which detects when a computer case is opened the firmware or bios is programmed to show an alert to the operator when the computer is booted up the next time drive locks are essentially software tools to encrypt hard drives making them inaccessible to thieves tools exist specifically for encrypting external drives as well disabling usb ports is a security option for preventing unauthorized and malicious access to an otherwise secure computer infected usb dongles connected to a network from a computer inside the firewall are considered by the magazine network world as the most common hardware threat facing computer networks disconnecting or disabling peripheral devices like camera gps removable storage etc that are not in use mobile enabled access devices are growing in popularity due to the ubiquitous nature of cell phones built in capabilities such as bluetooth the newer bluetooth low energy le near field communication nfc on non ios devices and biometric validation such as thumbprint readers as well as qr code reader software designed for mobile devices offer new secure ways for mobile phones to connect to access control systems these control systems provide computer security and can also be used for controlling access to secure buildings iommus allow for hardware based sandboxing of components in mobile and desktop computers by utilizing direct memory access protections physical unclonable functions pufs can be used as a digital fingerprint or a unique identifier to integrated circuits and hardware providing users the ability to secure the hardware supply chains going into their systems secure operating systems one use of the term computer security refers to technology that is used to implement secure operating systems using secure operating systems is a good way of ensuring computer security these are systems that have achieved certification from an external security auditing organization the most popular evaluations are common criteria cc secure coding in software engineering secure coding aims to guard against the accidental introduction of security vulnerabilities it is also possible to create software designed from the ground up to be secure such systems are secure by design beyond this formal verification aims to prove the correctness of the algorithms underlying a system important for cryptographic protocols for example capabilities and access control lists within computer systems two of the main security models capable of enforcing privilege separation are access control lists acls and role based access control rbac an access control list acl with respect to a computer file system is a list of permissions associated with an object an acl specifies which users or system processes are granted access to objects as well as what operations are allowed on given objects role based access control is an approach to restricting system access to authorized users used by the majority of enterprises with more than 500 employees and can implement mandatory access control mac or discretionary access control dac a further approach capability based security has been mostly restricted to research operating systems capabilities can however also be implemented at the language level leading to a style of programming that is essentially a refinement of standard object oriented design an open source project in the area is the e language user security training the end user is widely recognized as the weakest link in the security chain and it is estimated that more than 90 of security incidents and breaches involve some kind of human error among the most commonly recorded forms of errors and misjudgment are poor password management sending emails containing sensitive data and attachments to the wrong recipient the inability to recognize misleading urls and to identify fake websites and dangerous email attachments a common mistake that users make is saving their user id password in their browsers to make it easier to log in to banking sites this is a gift to attackers who have obtained access to a machine by some means the risk may be mitigated by the use of two factor authentication as the human component of cyber risk is particularly relevant in determining the global cyber risk an organization is facing security awareness training at all levels not only provides formal compliance with regulatory and industry mandates but is considered essential in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats the focus on the end user represents a profound cultural change for many security practitioners who have traditionally approached cybersecurity exclusively from a technical perspective and moves along the lines suggested by major security centers to develop a culture of cyber awareness within the organization recognizing that a security aware user provides an important line of defense against cyber attacks digital hygiene related to end user training digital hygiene or cyber hygiene is a fundamental principle relating to information security and as the analogy with personal hygiene shows is the equivalent of establishing simple routine measures to minimize the risks from cyber threats the assumption is that good cyber hygiene practices can give networked users another layer of protection reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network especially from common cyberattacks cyber hygiene should also not be mistaken for proactive cyber defence a military term the most common acts of digital hygiene can include updating malware protection cloud back ups passwords and ensuring restricted admin rights and network firewalls as opposed to a purely technology based defense against threats cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline or education it can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal or collective digital security as such these measures can be performed by laypeople not just security experts cyber hygiene relates to personal hygiene as computer viruses relate to biological viruses or pathogens however while the term computer virus was coined almost simultaneously with the creation of the first working computer viruses the term cyber hygiene is a much later invention perhaps as late as 2000 by internet pioneer vint cerf it has since been adopted by the congress and senate of the united states the fbi eu institutions and heads of state difficulty of responding to breaches responding to attempted security breaches is often very difficult for a variety of reasons including identifying attackers is difficult as they may operate through proxies temporary anonymous dial up accounts wireless connections and other anonymizing procedures which make back tracing difficult and are often located in another jurisdiction if they successfully breach security they have also often gained enough administrative access to enable them to delete logs to cover their tracks the sheer number of attempted attacks often by automated vulnerability scanners and computer worms is so large that organizations cannot spend time pursuing each law enforcement officers often lack the skills interest or budget to pursue attackers furthermore identifying attackers across a network may necessitate collecting logs from multiple locations within the network and across various countries a process that can be both difficult and time consuming where an attack succeeds and a breach occurs many jurisdictions now have in place mandatory security breach notification laws types of security and privacy systems at risk the growth in the number of computer systems and the increasing reliance upon them by individuals businesses industries and governments means that there are an increasing number of systems at risk financial systems the computer systems of financial regulators and financial institutions like the u s securities and exchange commission swift investment banks and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains websites and apps that accept or store credit card numbers brokerage accounts and bank account information are also prominent hacking targets because of the potential for immediate financial gain from transferring money making purchases or selling the information on the black market in store payment systems and atms have also been tampered with in order to gather customer account data and pins the ucla internet report surveying the digital future 2000 found that the privacy of personal data created barriers to online sales and that more than nine out of 10 internet users were somewhat or very concerned about credit card security the most common web technologies for improving security between browsers and websites are named ssl secure sockets layer and its successor tls transport layer security identity management and authentication services and domain name services allow companies and consumers to engage in secure communications and commerce several versions of ssl and tls are commonly used today in applications such as web browsing e mail internet faxing instant messaging and voip voice over ip there are various interoperable implementations of these technologies including at least one implementation that is open source open source allows anyone to view the application s source code and look for and report vulnerabilities the credit card companies visa and mastercard cooperated to develop the secure emv chip which is embedded in credit cards further developments include the chip authentication program where banks give customers hand held card readers to perform online secure transactions other developments in this arena include the development of technology such as instant issuance which has enabled shopping mall kiosks acting on behalf of banks to issue on the spot credit cards to interested customers utilities and industrial equipment computers control functions at many utilities including coordination of telecommunications the power grid nuclear power plants and valve opening and closing in water and gas networks the internet is a potential attack vector for such machines if connected but the stuxnet worm demonstrated that even equipment controlled by computers not connected to the internet can be vulnerable in 2014 the computer emergency readiness team a division of the department of homeland security investigated 79 hacking incidents at energy companies aviation the aviation industry is very reliant on a series of complex systems which could be attacked a simple power outage at one airport can cause repercussions worldwide much of the system relies on radio transmissions which could be disrupted and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore there is also potential for attack from within an aircraft implementing fixes in aerospace systems poses a unique challenge because efficient air transportation is heavily affected by weight and volume improving security by adding physical devices to airplanes could increase their unloaded weight and could potentially reduce cargo or passenger capacity in europe with the pan european network service and newpens and in the us with the nextgen program air navigation service providers are moving to create their own dedicated networks many modern passports are now biometric passports containing an embedded microchip that stores a digitized photograph and personal information such as name gender and date of birth in addition more countries are introducing facial recognition technology to reduce identity related fraud the introduction of the epassport has assisted border officials in verifying the identity of the passport holder thus allowing for quick passenger processing plans are under way in the us the uk and australia to introduce smartgate kiosks with both retina and fingerprint recognition technology the airline industry is moving from the use of traditional paper tickets towards the use of electronic tickets e tickets these have been made possible by advances in online credit card transactions in partnership with the airlines long distance bus companies are also switching over to e ticketing transactions today the consequences of a successful attack range from loss of confidentiality to loss of system integrity air traffic control outages loss of aircraft and even loss of life consumer devices desktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target smartphones tablet computers smart watches and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras microphones gps receivers compasses and accelerometers which could be exploited and may collect personal information including sensitive health information wifi bluetooth and cell phone networks on any of these devices could be used as attack vectors and sensors might be remotely activated after a successful breach the increasing number of home automation devices such as the nest thermostat are also potential targets healthcare today many healthcare providers and health insurance companies use the internet to provide enhanced products and services examples are the use of tele health to potentially offer better quality and access to healthcare or fitness trackers to lower insurance premiums hospitals increasingly use interconnected devices within their networks this is called internet of things iot connecting multiple devices within the hospital introduces various benefits such as automated detection of patient parameters electronic dose adjustments and decision support for clinicians however as these devices serve as potential access points to the hospital network security threats increase and hospitals have to introduce adequate security measures which for example comply with the health insurance portability and accountability act hipaa the health care company humana partners with webmd oracle corporation eds and microsoft to enable its members to access their health care records as well as to provide an overview of health care plans patient records are increasingly being placed on secure in house networks alleviating the need for extra storage space large corporations large corporations are common targets in many cases attacks are aimed at financial gain through identity theft and involve data breaches examples include the loss of millions of clients credit card and financial details by home depot staples target corporation and equifax medical records have been targeted in general identify theft health insurance fraud and impersonating patients to obtain prescription drugs for recreational purposes or resale although cyber threats continue to increase 62 of all organizations did not increase security training for their business in 2015 not all attacks are financially motivated however security firm hbgary federal had a serious series of attacks in 2011 from hacktivist group anonymous in retaliation for the firm s ceo claiming to have infiltrated their group and sony pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers automobiles vehicles are increasingly computerized with engine timing cruise control anti lock brakes seat belt tensioners door locks airbags and advanced driver assistance systems on many models additionally connected cars may use wifi and bluetooth to communicate with onboard consumer devices and the cell phone network self driving cars are expected to be even more complex all of these systems carry some security risks and such issues have gained wide attention simple examples of risk include a malicious compact disc being used as an attack vector and the car s onboard microphones being used for eavesdropping however if access is gained to a car s internal controller area network the danger is much greater and in a widely publicized 2015 test hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch manufacturers are reacting in numerous ways with tesla in 2016 pushing out some security fixes over the air into its cars computer systems in the area of autonomous vehicles in september 2016 the united states department of transportation announced some initial safety standards and called for states to come up with uniform policies additionally e drivers licenses are being developed using the same technology for example mexico s licensing authority icv has used a smart card platform to issue the first e drivers licenses to the city of monterrey in the state of nuevo león shipping shipping companies have adopted rfid radio frequency identification technology as an efficient digitally secure tracking device unlike a barcode rfid can be read up to 20 feet away rfid is used by fedex and ups government government and military computer systems are commonly attacked by activists and foreign powers local and regional government infrastructure such as traffic light controls police and intelligence agency communications personnel records as well as student records the fbi cia and pentagon all utilize secure controlled access technology for any of their buildings however the use of this form of technology is spreading into the entrepreneurial world more and more companies are taking advantage of the development of digitally secure controlled access technology ge s acuvision for example offers a single panel platform for access control alarm monitoring and digital recording internet of things and physical vulnerabilities the internet of things iot is the network of physical objects such as devices vehicles and buildings that are embedded with electronics software sensors and network connectivity that enables them to collect and exchange data concerns have been raised that this is being developed without appropriate consideration of the security challenges involved while the iot creates opportunities for more direct integration of the physical world into computer based systems it also provides opportunities for misuse in particular as the internet of things spreads widely cyberattacks are likely to become an increasingly physical rather than simply virtual threat if a front door s lock is connected to the internet and can be locked unlocked from a phone then a criminal could enter the home at the press of a button from a stolen or hacked phone people could stand to lose much more than their credit card numbers in a world controlled by iot enabled devices thieves have also used electronic means to circumvent non internet connected hotel door locks an attack aimed at physical infrastructure or human lives is often called a cyber kinetic attack as iot devices and appliances become more widespread the prevalence and potential damage of cyber kinetic attacks can increase substantially medical systems medical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated including both in hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps there are many reports of hospitals and hospital organizations getting hacked including ransomware attacks windows xp exploits viruses and data breaches of sensitive data stored on hospital servers on 28 december 2016 the us food and drug administration released its recommendations for how medical device manufacturers should maintain the security of internet connected devices but no structure for enforcement energy sector in distributed generation systems the risk of a cyber attack is real according to daily energy insider an attack could cause a loss of power in a large area for a long period of time and such an attack could have just as severe consequences as a natural disaster the district of columbia is considering creating a distributed energy resources der authority within the city with the goal being for customers to have more insight into their own energy use and giving the local electric utility pepco the chance to better estimate energy demand the d c proposal however would allow third party vendors to create numerous points of energy distribution which could potentially create more opportunities for cyber attackers to threaten the electric grid telecommunications perhaps the most widely known digitally secure telecommunication device is the sim subscriber identity module card a device that is embedded in most of the world s cellular devices before any service can be obtained the sim card is just the beginning of this digitally secure environment the smart card web servers draft standard scws defines the interfaces to an http server in a smart card tests are being conducted to secure ota over the air payment and credit card information from and to a mobile phone combination sim dvd devices are being developed through smart video card technology which embeds a dvd compliant optical disc into the card body of a regular sim card other telecommunication developments involving digital security include mobile signatures which use the embedded sim card to generate a legally binding electronic signature cost and impact of security breaches serious financial damage has been caused by security breaches but because there is no standard model for estimating the cost of an incident the only data available is that which is made public by the organizations involved several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general the 2003 loss estimates by these firms range from 13 billion worms and viruses only to 226 billion for all forms of covert attacks the reliability of these estimates is often challenged the underlying methodology is basically anecdotal however reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions according to the classic gordon loeb model analyzing the optimal investment level in information security one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss i e the expected value of the loss resulting from a cyber information security breach attacker motivation as with physical security the motivations for breaches of computer security vary between attackers some are thrill seekers or vandals some are activists others are criminals looking for financial gain state sponsored attackers are now common and well resourced but started with amateurs such as markus hess who hacked for the kgb as recounted by clifford stoll in the cuckoo s egg attackers motivations can vary for all types of attacks from pleasure to political goals for example hacktivists may target a company or organization that carries out activities they do not agree with this would be to create bad publicity for the company by having its website crash high capability hackers often with larger backing or state sponsorship may attack based on the demands of their financial backers these attacks are more likely to attempt more serious attack an example of a more serious attack was the 2015 ukraine power grid hack which reportedly utilised the spear phising destruction of files and denial of service attacks to carry out the full attack additionally recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas the growth of the internet mobile technologies and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations all critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference a key aspect of threat modeling for any system is identifying the motivations behind potential attacks and the individuals or groups likely to carry them out the level and detail of security measures will differ based on the specific system being protected for instance a home personal computer a bank and a classified military network each face distinct threats despite using similar underlying technologies computer security incident management computer security incident management is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack an incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure the intended outcome of a computer security incident response plan is to contain the incident limit damage and assist recovery to business as usual responding to compromises quickly can mitigate exploited vulnerabilities restore services and processes and minimize losses incident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage typical incident response plans contain a set of written instructions that outline the organization s response to a cyberattack without a documented plan in place an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles processes and procedures during an escalation slowing the organization s response and resolution there are four key components of a computer security incident response plan preparation preparing stakeholders on the procedures for handling computer security incidents or compromises detection and analysis identifying and investigating suspicious activity to confirm a security incident prioritizing the response based on impact and coordinating notification of the incident containment eradication and recovery isolating affected systems to prevent escalation and limit impact pinpointing the genesis of the incident removing malware affected systems and bad actors from the environment and restoring systems and data when a threat no longer remains post incident activity post mortem analysis of the incident its root cause and the organization s response with the intent of improving the incident response plan and future response efforts notable attacks and breaches some illustrative examples of different types of computer security breaches are given below robert morris and the first computer worm in 1988 60 000 computers were connected to the internet and most were mainframes minicomputers and professional workstations on 2 november 1988 many started to slow down because they were running a malicious code that demanded processor time and that spread itself to other computers the first internet computer worm the software was traced back to 23 year old cornell university graduate student robert tappan morris who said he wanted to count how many machines were connected to the internet rome laboratory in 1994 over a hundred intrusions were made by unidentified crackers into the rome laboratory the us air force s main command and research facility using trojan horses hackers were able to obtain unrestricted access to rome s networking systems and remove traces of their activities the intruders were able to obtain classified files such as air tasking order systems data and furthermore able to penetrate connected networks of national aeronautics and space administration s goddard space flight center wright patterson air force base some defense contractors and other private sector organizations by posing as a trusted rome center user tjx customer credit card details in early 2007 american apparel and home goods company tjx announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card debit card check and merchandise return transactions stuxnet attack in 2010 the computer worm known as stuxnet reportedly ruined almost one fifth of iran s nuclear centrifuges it did so by disrupting industrial programmable logic controllers plcs in a targeted attack this is generally believed to have been launched by israel and the united states to disrupt iran s nuclear program although neither has publicly admitted this global surveillance disclosures in early 2013 documents provided by edward snowden were published by the washington post and the guardian exposing the massive scale of nsa global surveillance there were also indications that the nsa may have inserted a backdoor in a nist standard for encryption this standard was later withdrawn due to widespread criticism the nsa additionally were revealed to have tapped the links between google s data centers target and home depot breaches a ukrainian hacker known as rescator broke into target corporation computers in 2013 stealing roughly 40 million credit cards and then home depot computers in 2014 stealing between 53 and 56 million credit card numbers warnings were delivered at both corporations but ignored physical security breaches using self checkout machines are believed to have played a large role the malware utilized is absolutely unsophisticated and uninteresting says jim walter director of threat intelligence operations at security technology company mcafee meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings the size of the thefts has resulted in major attention from state and federal united states authorities and the investigation is ongoing office of personnel management data breach in april 2015 the office of personnel management discovered it had been hacked more than a year earlier in a data breach resulting in the theft of approximately 21 5 million personnel records handled by the office the office of personnel management hack has been described by federal officials as among the largest breaches of government data in the history of the united states data targeted in the breach included personally identifiable information such as social security numbers names dates and places of birth addresses and fingerprints of current and former government employees as well as anyone who had undergone a government background check it is believed the hack was perpetrated by chinese hackers ashley madison breach in july 2015 a hacker group is known as the impact team successfully breached the extramarital relationship website ashley madison created by avid life media the group claimed that they had taken not only company data but user data as well after the breach the impact team dumped emails from the company s ceo to prove their point and threatened to dump customer data unless the website was taken down permanently when avid life media did not take the site offline the group released two more compressed files one 9 7gb and the second 20gb after the second data dump avid life media ceo noel biderman resigned but the website remained to function colonial pipeline ransomware attack in june 2021 the cyber attack took down the largest fuel pipeline in the u s and led to shortages across the east coast legal issues and global regulation international legal issues of cyber attacks are complicated in nature there is no global base of common rules to judge and eventually punish cybercrimes and cybercriminals and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack often the local authorities cannot take action due to lack of laws under which to prosecute proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies computer viruses switch from one country to another from one jurisdiction to another moving around the world using the fact that we don t have the capability to globally police operations like this so the internet is as if someone given free plane tickets to all the online criminals of the world the use of techniques such as dynamic dns fast flux and bullet proof servers add to the difficulty of investigation and enforcement role of government the role of the government is to make regulations to force companies and organizations to protect their systems infrastructure and information from any cyberattacks but also to protect its own national infrastructure such as the national power grid the government s regulatory role in cyberspace is complicated for some cyberspace was seen as a virtual space that was to remain free of government intervention as can be seen in many of today s libertarian blockchain and bitcoin discussions many government officials and experts think that the government should do more and that there is a crucial need for improved regulation mainly due to the failure of the private sector to solve efficiently the cybersecurity problem r clarke said during a panel discussion at the rsa security conference in san francisco he believes that the industry only responds when you threaten regulation if the industry doesn t respond to the threat you have to follow through on the other hand executives from the private sector agree that improvements are necessary but think that government intervention would affect their ability to innovate efficiently daniel r mccarthy analyzed this public private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order on 22 may 2020 the un security council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace according to un secretary general antónio guterres new technologies are too often used to violate rights international actions many different teams and organizations exist including the forum of incident response and security teams first is the global association of csirts the us cert at t apple cisco mcafee microsoft are all members of this international team the council of europe helps protect societies worldwide from the threat of cybercrime through the convention on cybercrime the purpose of the messaging anti abuse working group maawg is to bring the messaging industry together to work collaboratively and to successfully address the various forms of messaging abuse such as spam viruses denial of service attacks and other messaging exploitations france telecom facebook at t apple cisco sprint are some of the members of the maawg enisa the european network and information security agency enisa is an agency of the european union with the objective to improve network and information security in the european union europe on 14 april 2016 the european parliament and the council of the european union adopted the general data protection regulation gdpr the gdpr which came into force on 25 may 2018 grants individuals within the european union eu and the european economic area eea the right to the protection of personal data the regulation requires that any entity that processes personal data incorporate data protection by design and by default it also requires that certain organizations appoint a data protection officer dpo national actions computer emergency response teams most countries have their own computer emergency response team to protect network security canada since 2010 canada has had a cybersecurity strategy this functions as a counterpart document to the national strategy and action plan for critical infrastructure the strategy has three main pillars securing government systems securing vital private cyber systems and helping canadians to be secure online there is also a cyber incident management framework to provide a coordinated response in the event of a cyber incident the canadian cyber incident response centre ccirc is responsible for mitigating and responding to threats to canada s critical infrastructure and cyber systems it provides support to mitigate cyber threats technical support to respond recover from targeted cyber attacks and provides online tools for members of canada s critical infrastructure sectors it posts regular cybersecurity bulletins operates an online reporting tool where individuals and organizations can report a cyber incident to inform the general public on how to protect themselves online public safety canada has partnered with stop think connect a coalition of non profit private sector and government organizations and launched the cyber security cooperation program they also run the getcybersafe portal for canadian citizens and cyber security awareness month during october public safety canada aims to begin an evaluation of canada s cybersecurity strategy in early 2015 australia australian federal government announced an 18 2 million investment to fortify the cybersecurity resilience of small and medium enterprises smes and enhance their capabilities in responding to cyber threats this financial backing is an integral component of the soon to be unveiled 2023 2030 australian cyber security strategy slated for release within the current week a substantial allocation of 7 2 million is earmarked for the establishment of a voluntary cyber health check program facilitating businesses in conducting a comprehensive and tailored self assessment of their cybersecurity upskill this avant garde health assessment serves as a diagnostic tool enabling enterprises to ascertain the robustness of australia s cyber security regulations furthermore it affords them access to a repository of educational resources and materials fostering the acquisition of skills necessary for an elevated cybersecurity posture this groundbreaking initiative was jointly disclosed by minister for cyber security clare o neil and minister for small business julie collins india some provisions for cybersecurity have been incorporated into rules framed under the information technology act 2000 the national cyber security policy 2013 is a policy framework by the ministry of electronics and information technology meity which aims to protect the public and private infrastructure from cyberattacks and safeguard information such as personal information of web users financial and banking information and sovereign data cert in is the nodal agency which monitors the cyber threats in the country the post of national cyber security coordinator has also been created in the prime minister s office pmo the indian companies act 2013 has also introduced cyber law and cybersecurity obligations on the part of indian directors some provisions for cybersecurity have been incorporated into rules framed under the information technology act 2000 update in 2013 south korea following cyberattacks in the first half of 2013 when the government news media television stations and bank websites were compromised the national government committed to the training of 5 000 new cybersecurity experts by 2017 the south korean government blamed its northern counterpart for these attacks as well as incidents that occurred in 2009 2011 and 2012 but pyongyang denies the accusations united states cyber plan the united states has its first fully formed cyber plan in 15 years as a result of the release of this national cyber plan in this policy the us says it will protect the country by keeping networks systems functions and data safe promote american wealth by building a strong digital economy and encouraging strong domestic innovation peace and safety should be kept by making it easier for the us to stop people from using computer tools for bad things working with friends and partners to do this and increase the united states impact around the world to support the main ideas behind an open safe reliable and compatible internet the new u s cyber strategy seeks to allay some of those concerns by promoting responsible behavior in cyberspace urging nations to adhere to a set of norms both through international law and voluntary standards it also calls for specific measures to harden u s government networks from attacks like the june 2015 intrusion into the u s office of personnel management opm which compromised the records of about 4 2 million current and former government employees and the strategy calls for the u s to continue to name and shame bad cyber actors calling them out publicly for attacks when possible along with the use of economic sanctions and diplomatic pressure legislation the 1986 18 u s c 1030 the computer fraud and abuse act is the key legislation it prohibits unauthorized access or damage of protected computers as defined in 18 u s c 1030 e 2 although various other measures have been proposed none have succeeded in 2013 executive order 13636 improving critical infrastructure cybersecurity was signed which prompted the creation of the nist cybersecurity framework in response to the colonial pipeline ransomware attack president joe biden signed executive order 14028 on may 12 2021 to increase software security standards for sales to the government tighten detection and security on existing systems improve information sharing and training establish a cyber safety review board and improve incident response standardized government testing services the general services administration gsa has standardized the penetration test service as a pre vetted support service to rapidly address potential vulnerabilities and stop adversaries before they impact us federal state and local governments these services are commonly referred to as highly adaptive cybersecurity services hacs agencies the department of homeland security has a dedicated division responsible for the response system risk management program and requirements for cybersecurity in the united states called the national cyber security division the division is home to us cert operations and the national cyber alert system the national cybersecurity and communications integration center brings together government organizations responsible for protecting computer networks and networked infrastructure the third priority of the fbi is to protect the united states against cyber based attacks and high technology crimes and they along with the national white collar crime center nw3c and the bureau of justice assistance bja are part of the multi agency task force the internet crime complaint center also known as ic3 in addition to its own specific duties the fbi participates alongside non profit organizations such as infragard the computer crime and intellectual property section ccips operates in the united states department of justice criminal division the ccips is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks in 2017 ccips published a framework for a vulnerability disclosure program for online systems to help organizations clearly describe authorized vulnerability disclosure and discovery conduct thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the computer fraud and abuse act 18 u s c 1030 the united states cyber command also known as uscybercom has the mission to direct synchronize and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners it has no role in the protection of civilian networks the u s federal communications commission s role in cybersecurity is to strengthen the protection of critical communications infrastructure to assist in maintaining the reliability of networks during disasters to aid in swift recovery after and to ensure that first responders have access to effective communications services the food and drug administration has issued guidance for medical devices and the national highway traffic safety administration is concerned with automotive cybersecurity after being criticized by the government accountability office and following successful attacks on airports and claimed attacks on airplanes the federal aviation administration has devoted funding to securing systems on board the planes of private manufacturers and the aircraft communications addressing and reporting system concerns have also been raised about the future next generation air transportation system the us department of defense dod issued dod directive 8570 in 2004 supplemented by dod directive 8140 requiring all dod employees and all dod contract personnel involved in information assurance roles and activities to earn and maintain various industry information technology it certifications in an effort to ensure that all dod personnel involved in network infrastructure defense have minimum levels of it industry recognized knowledge skills and abilities ksa andersson and reimers 2019 report these certifications range from comptia s a and security through the ics2 org s cissp etc computer emergency readiness team computer emergency response team is a name given to expert groups that handle computer security incidents in the us two distinct organizations exist although they do work closely together us cert part of the national cyber security division of the united states department of homeland security cert cc created by the defense advanced research projects agency darpa and run by the software engineering institute sei u s nrc 10 cfr 73 54 cybersecurity in the context of u s nuclear power plants the u s nuclear regulatory commission nrc outlines cybersecurity requirements under 10 cfr part 73 specifically in 73 54 nei 08 09 cybersecurity plan for nuclear power plants the nuclear energy institute s nei 08 09 document cyber security plan for nuclear power reactors outlines a comprehensive framework for cybersecurity in the nuclear power industry drafted with input from the u s nrc this guideline is instrumental in aiding licensees to comply with the code of federal regulations cfr which mandates robust protection of digital computers and equipment and communications systems at nuclear power plants against cyber threats modern warfare there is growing concern that cyberspace will become the next theater of warfare as mark clayton from the christian science monitor wrote in a 2015 article titled the new cyber arms race in the future wars will not just be fought by soldiers with guns or with planes that drop bombs they will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities transportation communications and energy such attacks could also disable military networks that control the movement of troops the path of jet fighters the command and control of warships this has led to new terms such as cyberwarfare and cyberterrorism the united states cyber command was created in 2009 and many other countries have similar forces there are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be careers cybersecurity is a fast growing field of it concerned with reducing organizations risk of hack or data breaches according to research from the enterprise strategy group 46 of organizations say that they have a problematic shortage of cybersecurity skills in 2016 up from 28 in 2015 commercial government and non governmental organizations all employ cybersecurity professionals the fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance health care and retail however the use of the term cybersecurity is more prevalent in government job descriptions typical cybersecurity job titles and descriptions include security analyst analyzes and assesses vulnerabilities in the infrastructure software hardware networks investigates using available tools and countermeasures to remedy the detected vulnerabilities and recommends solutions and best practices analyzes and assesses damage to the data infrastructure as a result of security incidents examines available recovery tools and processes and recommends solutions tests for compliance with security policies and procedures may assist in the creation implementation or management of security solutions security engineer performs security monitoring security and data logs analysis and forensic analysis to detect security incidents and mount the incident response investigates and utilizes new technologies and processes to enhance security capabilities and implement improvements may also review code or perform other security engineering methodologies security architect designs a security system or major components of a security system and may head a security design team building a new security system chief information security officer ciso a high level management position responsible for the entire information security division staff the position may include hands on technical work chief security officer cso a high level management position responsible for the entire security division staff a newer position is now deemed needed as security risks grow data protection officer dpo a dpo is tasked with monitoring compliance with data protection laws such as gdpr data protection policies awareness raising training and audits security consultant specialist intelligence broad titles that encompass any one or all of the other roles or titles tasked with protecting computers networks software data or information systems against viruses worms spyware malware intrusion detection unauthorized access denial of service attacks and an ever increasing list of attacks by hackers acting as individuals or as part of organized crime or foreign governments student programs are also available for people interested in beginning a career in cybersecurity meanwhile a flexible and effective option for information security professionals of all experience levels to keep studying is online security training including webcasts a wide range of certified courses are also available in the united kingdom a nationwide set of cybersecurity forums known as the u k cyber security forum were established supported by the government s cybersecurity strategy in order to encourage start ups and innovation and to address the skills gap identified by the u k government in singapore the cyber security agency has issued a singapore operational technology ot cybersecurity competency framework otccf the framework defines emerging cybersecurity roles in operational technology the otccf was endorsed by the infocomm media development authority imda it outlines the different ot cybersecurity job positions as well as the technical skills and core competencies necessary it also depicts the many career paths available including vertical and lateral advancement opportunities terminology the following terms used with regards to computer security are explained below access authorization restricts access to a computer to a group of users through the use of authentication systems these systems can protect either the whole computer such as through an interactive login screen or individual services such as a ftp server there are many methods for identifying and authenticating users such as passwords identification cards smart cards and biometric systems anti virus software consists of computer programs that attempt to identify thwart and eliminate computer viruses and other malicious software malware applications are executable code so general corporate practice is to restrict or block users the power to install them to install them only when there is a demonstrated need e g software needed to perform assignments to install only those which are known to be reputable preferably with access to the computer code used to create the application and to reduce the attack surface by installing as few as possible they are typically run with least privilege with a robust process in place to identify test and install any released security patches or updates for them for example programs can be installed into an individual user s account which limits the program s potential access as well as being a means control which users have specific exceptions to policy in linux freebsd openbsd and other unix like operating systems there is an option to further restrict an application using chroot or other means of restricting the application to its own sandbox for example linux provides namespaces and cgroups to further restrict the access of an application to system resources generalized security frameworks such as selinux or apparmor help administrators control access java and other languages which compile to java byte code and run in the java virtual machine can have their access to other applications controlled at the virtual machine level some software can be run in software containers which can even provide their own set of system libraries limiting the software s or anyone controlling it access to the server s versions of the libraries authentication techniques can be used to ensure that communication end points are who they say they are automated theorem proving and other verification tools can be used to enable critical algorithms and code used in secure systems to be mathematically proven to meet their specifications backups are one or more copies kept of important computer files typically multiple copies will be kept at different locations so that if a copy is stolen or damaged other copies will still exist capability and access control list techniques can be used to ensure privilege separation and mandatory access control capabilities vs acls discusses their use chain of trust techniques can be used to attempt to ensure that all software loaded has been certified as authentic by the system s designers confidentiality is the nondisclosure of information except to another authorized person cryptographic techniques can be used to defend data in transit between systems reducing the probability that the data exchange between systems can be intercepted or modified cyber attribution is an attribution of cybercrime i e finding who perpetrated a cyberattack cyberwarfare is an internet based conflict that involves politically motivated attacks on information and information systems such attacks can for example disable official websites and networks disrupt or disable essential services steal or alter classified data and cripple financial systems data integrity is the accuracy and consistency of stored data indicated by an absence of any alteration in data between two updates of a data record encryption is used to protect the confidentiality of a message cryptographically secure ciphers are designed to make any practical attempt of breaking them infeasible symmetric key ciphers are suitable for bulk encryption using shared keys and public key encryption using digital certificates can provide a practical solution for the problem of securely communicating when no key is shared in advance endpoint security software aids networks in preventing malware infection and data theft at network entry points made vulnerable by the prevalence of potentially infected devices such as laptops mobile devices and usb drives firewalls serve as a gatekeeper system between networks allowing only traffic that matches defined rules they often include detailed logging and may include intrusion detection and intrusion prevention features they are near universal between company local area networks and the internet but can also be used internally to impose traffic rules between networks if network segmentation is configured a hacker is someone who seeks to breach defenses and exploit weaknesses in a computer system or network honey pots are computers that are intentionally left vulnerable to attack by crackers they can be used to catch crackers and to identify their techniques intrusion detection systems are devices or software applications that monitor networks or systems for malicious activity or policy violations a microkernel is an approach to operating system design which has only the near minimum amount of code running at the most privileged level and runs other elements of the operating system such as device drivers protocol stacks and file systems in the safer less privileged user space pinging the standard ping application can be used to test if an ip address is in use if it is attackers may then try a port scan to detect which services are exposed a port scan is used to probe an ip address for open ports to identify accessible network services and applications a key logger is spyware that silently captures and stores each keystroke that a user types on the computer s keyboard social engineering is the use of deception to manipulate individuals to breach security logic bombs is a type of malware added to a legitimate program that lies dormant until it is triggered by a specific event a unikernel is a computer program that runs on a minimalistic operating system where a single application is allowed to run as opposed to a general purpose operating system where many applications can run at the same time this approach to minimizing the attack surface is adopted mostly in cloud environments where software is deployed in virtual machines zero trust security means that no one is trusted by default from inside or outside the network and verification is required from everyone trying to gain access to resources on the network history since the internet s arrival and with the digital transformation initiated in recent years the notion of cybersecurity has become a familiar subject in both our professional and personal lives cybersecurity and cyber threats have been consistently present for the last 60 years of technological change in the 1970s and 1980s computer security was mainly limited to academia until the conception of the internet where with increased connectivity computer viruses and network intrusions began to take off after the spread of viruses in the 1990s the 2000s marked the institutionalization of organized attacks such as distributed denial of service this led to the formalization of cybersecurity as a professional discipline the april 1967 session organized by willis ware at the spring joint computer conference and the later publication of the ware report were foundational moments in the history of the field of computer security ware s work straddled the intersection of material cultural political and social concerns a 1977 nist publication introduced the cia triad of confidentiality integrity and availability as a clear and simple way to describe key security goals while still relevant many more elaborate frameworks have since been proposed however in the 1970s and 1980s there were no grave computer threats because computers and the internet were still developing and security threats were easily identifiable more often threats came from malicious insiders who gained unauthorized access to sensitive documents and files although malware and network breaches existed during the early years they did not use them for financial gain by the second half of the 1970s established computer firms like ibm started offering commercial access control systems and computer security software products one of the earliest examples of an attack on a computer network was the computer worm creeper written by bob thomas at bbn which propagated through the arpanet in 1971 the program was purely experimental in nature and carried no malicious payload a later program reaper was created by ray tomlinson in 1972 and used to destroy creeper between september 1986 and june 1987 a group of german hackers performed the first documented case of cyber espionage the group hacked into american defense contractors universities and military base networks and sold gathered information to the soviet kgb the group was led by markus hess who was arrested on 29 june 1987 he was convicted of espionage along with two co conspirators on 15 feb 1990 in 1988 one of the first computer worms called the morris worm was distributed via the internet it gained significant mainstream media attention in 1993 netscape started developing the protocol ssl shortly after the national center for supercomputing applications ncsa launched mosaic 1 0 the first web browser in 1993 netscape had ssl version 1 0 ready in 1994 but it was never released to the public due to many serious security vulnerabilities these weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users however in february 1995 netscape launched version 2 0 the national security agency nsa is responsible for the protection of u s information systems and also for collecting foreign intelligence the agency analyzes commonly used software and system configurations to find security flaws which it can use for offensive purposes against competitors of the united states nsa contractors created and sold click and shoot attack tools to us agencies and close allies but eventually the tools made their way to foreign adversaries in 2016 nsas own hacking tools were hacked and they have been used by russia and north korea nsa s employees and contractors have been recruited at high salaries by adversaries anxious to compete in cyberwarfare in 2007 the united states and israel began exploiting security flaws in the microsoft windows operating system to attack and damage equipment used in iran to refine nuclear materials iran responded by heavily investing in their own cyberwarfare capability which it began using against the united states notable scholars see also references further reading branch jordan 24 september 2020 what s in a name metaphors and cybersecurity international organization 75 1 cambridge university press cup 39 70 doi 10 1017 s002081832000051x issn 0020 8183 s2cid 224886794 costigan sean hennessy michael 2016 cybersecurity a generic reference curriculum pdf nato isbn 978 9284501960 archived pdf from the original on 10 march 2017 fuller christopher j 11 june 2018 the roots of the united states cyber in security doc diplomatic history 43 1 oxford university press oup 157 185 doi 10 1093 dh dhy038 issn 0145 2096 bob yonah jeremy 21 august 2021 ex idf cyber intel official reveals secrets behind cyber offense the jerusalem post kim peter 2014 the hacker playbook practical guide to penetration testing seattle createspace independent publishing platform isbn 978 1494932633 lee newton 2015 counterterrorism and cybersecurity total information awareness 2nd ed springer isbn 978 3319172439 montagnani maria lillà cavallo mirta antonella 2018 cybersecurity and liability in a big data world market and competition law review 2 2 elsevier bv 71 98 doi 10 2139 ssrn 3220475 issn 1556 5068 s2cid 216704215 ssrn 3220475 shariati marzieh bahmani faezeh shams fereidoon 2011 enterprise information security a review of architectures and frameworks from interoperability perspective procedia computer science 3 elsevier bv 537 543 doi 10 1016 j procs 2010 12 089 issn 1877 0509 singer p w friedman allan 2014 cybersecurity and cyberwar what everyone needs to know oxford university press isbn 978 0199918119 wu chwan hwa john irwin j david 2013 introduction to computer networks and cybersecurity boca raton crc press isbn 978 1466572133 cybersecurity best practices cybersecurity and infrastructure security agency cisa n d retrieved april 24 2024 from https www cisa gov topics cybersecurity best practices sztyber betley a syfert m kościelny j m górecka z 2023 controller cyber attack detection and isolation sensors 14248220 sensors 14248220 23 5 2778 doi 10 3390 s23052778",
            "total_words": 15066,
            "unique_words_percentage": 21.74432497013142,
            "stopwords_percentage": 37.879994690030536
        },
        {
            "title": "Cybersecurity and Infrastructure Security Agency",
            "link": "https://en.wikipedia.org/wiki/Cybersecurity_and_Infrastructure_Security_Agency",
            "content": "the cybersecurity and infrastructure security agency cisa is a component of the united states department of homeland security dhs responsible for cybersecurity and infrastructure protection across all levels of government coordinating cybersecurity programs with u s states and improving the government s cybersecurity protections against private and nation state hackers cybersecurity is now considered as important part of individuals and families as well as organizations governments educational institutions and our business it is essential for families and parents to protect the children and family members from online fraud the term cyber attack covers a wide variety of actions ranging from simple probes to defacing websites to denial of service to espionage and destruction the agency began in 2007 as the dhs national protection and programs directorate with the cybersecurity and infrastructure security agency act of 2018 cisa s footprint grew to include roles in securing elections and the census managing national special security events and the u s response to the covid 19 pandemic it has also been involved in 5g network security and hardening the us grid against electromagnetic pulses emps the office for bombing prevention leads the national counter ied effort currently headquartered in arlington virginia in 2025 cisa is planning to move its headquarters along with 6 500 employees to a new 10 story 620 000 sq ft building on the consolidated dhs st elizabeths campus headquarters history the national protection and programs directorate nppd was formed in 2007 as a component of the united states department of homeland security nppd s goal was to advance the department s national security mission by reducing and eliminating threats to u s critical physical and cyber infrastructure on november 16 2018 president trump signed into law the cybersecurity and infrastructure security agency act of 2018 which elevated the mission of the former nppd within dhs establishing the cybersecurity and infrastructure security agency cisa cisa is a successor agency to nppd and assists both other government agencies and private sector organizations in addressing cybersecurity issues former nppd under secretary christopher krebs was cisa s first director and former deputy under secretary matthew travis was its first deputy director on january 22 2019 cisa issued its first emergency directive 19 01 mitigate dns infrastructure tampering warning that an active attacker is targeting government organizations using dns spoofing techniques to perform man in the middle attacks research group fireeye stated that initial research suggests the actor or actors responsible have a nexus to iran in 2020 cisa created a website titled rumor control to rebut disinformation associated with the 2020 united states presidential election on november 12 2020 cisa issued a press release asserting there is no evidence that any voting system deleted or lost votes changed votes or was in any way compromised on the same day director krebs indicated that he expected to be dismissed from his post by the trump administration krebs was subsequently fired by president trump on november 17 2020 via tweet for his comments regarding the security of the election according to various reports and statistics the scale and frequency of cyber attacks have been steadily increasing in recent years for example the number of data breaches reported in 2020 alone reached a record high of 3 932 a 48 increase compared to the previous year with over 37 billion records exposed globally and also the average cost of a data breach in 2020 was estimated to be 3 86 million with an average time to identify and contain a breach of 280 days on july 12 2021 the senate confirmed jen easterly by a voice vote easterly s nomination had been reported favorably out of senate committee on homeland security and governmental affairs on june 16 but a floor vote had been reportedly held delayed by senator rick scott over broader national security concerns until the president or vice president had visited the southern border with mexico easterly hired new staff to monitor online disinformation to enhance what she called the nation s cognitive infrastructure and utilized the existing rumor control website during the 2021 elections in september 2022 cisa released their 2023 2025 cisa strategic plan the first comprehensive strategy document since the agency was established in 2018 cyber security has become a matter of global interest and importance already more than 50 nations have officially published some form of strategy document outlining their official stance on cyberspace cyber crime and or cyber security organization cisa divisions include the cybersecurity division national cybersecurity and communications integration center capacity building joint cyber defense collaborative mission engineering office of the technical director threat hunting vulnerability management infrastructure security division bombing prevention chemical security exercises infrastructure assessment analysis school safety strategy performance resources emergency communications division national risk management center integrated operations division regions 1 through 10 stakeholder engagement division council management international sector management strategic relations programs the continuous diagnostics and mitigations program provides cybersecurity tools and services to federal agencies cisa issues binding operational directives that require federal government agencies to take action against specific cybersecurity risks in march 2021 cisa assumed control of the gov top level domain tld from the general services administration cisa manages the approval of domains and operates the tld domain name system nameservers in april 2021 cisa removed the fee for registering domains in january 2023 cloudflare received a 7 2m contract to provide dns registry and hosting services for the tld cisa provides incident response services to the federal executive branch and us based entities cisa manages the einstein intrusion detection system to detect malicious activity on federal government agency networks the national defense authorization act for fiscal year 2021 granted cisa the authority to issue administrative subpoenas in order to identify the owners of internet connected critical infrastructure related devices with specific vulnerabilities in 2021 cisa issued 47 subpoenas in august 2021 easterly stated one could argue we re in the business of critical infrastructure and the most critical infrastructure is our cognitive infrastructure so building that resilience to misinformation and disinformation i think is incredibly important in 2021 cisa released a report that provided guidance for how to navigate and prevent ransomware incidents this was due to a significant jump in recent attacks related to ransomware committees cybersecurity advisory committee in 2021 the agency created the cybersecurity advisory committee with the following members steve adler mayor city of austin texas marene allison chief information security officer johnson johnson lori beer chief information officer jpmorgan chase robert chesney james a baker iii chair in the rule of law and world affairs university of texas school of law thomas fanning chairman president and ceo southern company vijaya gadde patrick d gallagher chancellor university of pittsburgh ronald green executive vice president and chief security officer mastercard niloofar razi howe board member tenable kevin mandia chief executive officer mandiant jeff moss president def con communications nuala o connor senior vice president chief counsel digital citizenship walmart nicole perlroth cybersecurity journalist matthew prince chief executive officer cloudflare ted schlein general partner kleiner perkins and caufield byers stephen schmidt chief information security officer amazon web services suzanne spaulding senior advisor for homeland security csis alex stamos partner krebs stamos group kate starbird associate professor human centered design engineering university of washington george stathakopoulos vice president of corporate information security apple alicia tate nadeau arng ret director illinois emergency management agency nicole wong principal nwong strategies chris young executive vice president of business development strategy and ventures microsoft directors see also list of federal agencies in the united states florida digital service references external links official website",
            "total_words": 1264,
            "unique_words_percentage": 49.68354430379747,
            "stopwords_percentage": 29.03481012658228
        },
        {
            "title": "Richard Forno",
            "link": "https://en.wikipedia.org/wiki/Richard_Forno",
            "content": "richard forno is a consultant lecturer and writer in the area of washington dc education forno earned a bachelor s degree in international relations from american university in 1994 a master s degree in international relations from salve regina university in 2002 and a phd degree in internet studies from curtin university of technology in 2010 he is also a graduate of valley forge military academy and college and the united states naval war college work dr forno is graduate program director for university of maryland s cybersecurity program co founder of the maryland cyber challenge conference and a visiting scientist at the software engineering institute of carnegie mellon university at one time forno was the chief information security officer for network solutions forno writes and publishes on his web site articles on technology computer security and politics roughly at the rate of one every two weeks an example of his articles is the joke of federal cybersecurity oversight the joke of federal cybersecurity oversight dated march 2004 toward the beginning of the essay forno lists a series of news articles mostly from cnet news com that describe inadequacies in the federal government s computer security he also notes his opinion that the mass media in its drive to attract attention tends to ignore news of positive developments in federal computer security in the rest of the essay forno criticizes the government for insecure administration of information technology granted popular enterprise technology is nowhere as secure as it should be but today s federal cybersecurity woes result more from flawed technology management practices than flawed technology to that end we need to foster and reward innovative effective management processes in the federal computer security arena and terminate the current technology management and oversight philosophy that tolerates and rewards idleness and mediocrity while doing little to actually eliminate them references other",
            "total_words": 310,
            "unique_words_percentage": 54.83870967741935,
            "stopwords_percentage": 37.41935483870968
        },
        {
            "title": "LevelBlue",
            "link": "https://en.wikipedia.org/wiki/LevelBlue",
            "content": "levelblue formerly at t cybersecurity is a managed security service provider cybersecurity investor willjam ventures officially launched levelblue a joint venture with at t to form a new standalone managed cybersecurity services business at rsa conference 2024 levelblue offers managed security services consulting threat intelligence and research products levelblue is a managed security services business product and services include cybersecurity consulting services that provide assessment planning and advisory through the levelblue consulting team levelblue consulting services include security strategy and build cyber resilience through zero trust risk mitigation and compliance assurance managed security services for network security threat detection and response and endpoint security offer a strategic extension to help organizations simplify cybersecurity to deliver greater insights levelblue managed security services drive efficiency in security operations identify costs and complexity and associate them with business outcomes and pinpoint where to adapt and scale as business evolves threat intelligence from levelblue labs identifies threats and accelerates threat detection and response through its threat intelligence platform enriched by machine learning and security expertise the platform is backed by the levelblue open threat exchange otx a community of over 235 000 security professionals who submit 20 million plus threat indicators daily levelblue also offers third party integrations through its open xdr platform usm anywhere levelblue operates across four global security operations centers socs and three global network operations centers nocs that are monitored 24 7 365 providing service and support references external links official website",
            "total_words": 242,
            "unique_words_percentage": 58.26446280991735,
            "stopwords_percentage": 24.380165289256198
        },
        {
            "title": "Capture the flag (cybersecurity)",
            "link": "https://en.wikipedia.org/wiki/Capture_the_flag_(cybersecurity)",
            "content": "capture the flag ctf in computer security is an exercise in which participants attempt to find text strings called flags which are secretly hidden in purposefully vulnerable programs or websites they can be used for both competitive or educational purposes in two main variations of ctfs participants either steal flags from other participants attack defense style ctfs or from organizers jeopardy style challenges a mixed competition combines these two styles competitions can include hiding flags in hardware devices they can be both online or in person and can be advanced or entry level the game is inspired by the traditional outdoor sport of the same name ctfs are used as a tool for developing and refining cybersecurity skills making them popular in both professional and academic settings overview capture the flag ctf is a cybersecurity competition that is used to test and develop computer security skills it was first developed in 1996 at def con the largest cybersecurity conference in the united states which is hosted annually in las vegas nevada the conference hosts a weekend of cybersecurity competitions including their flagship ctf two popular ctf formats are jeopardy and attack defense both formats test participant s knowledge in cybersecurity but differ in objective in the jeopardy format participating teams must complete as many challenges of varying point values from a various categories such as cryptography web exploitation and reverse engineering in the attack defense format competing teams must defend their vulnerable computer systems while attacking their opponent s systems the exercise involves a diverse array of tasks including exploitation and cracking passwords but there is little evidence showing how these tasks translate into cybersecurity knowledge held by security experts recent research has shown that the capture the flag tasks mainly covered technical knowledge but lacked social topics like social engineering and awareness on cybersecurity educational applications ctfs have been shown to be an effective way to improve cybersecurity education through gamification there are many examples of ctfs designed to teach cybersecurity skills to a wide variety of audiences including picoctf organized by the carnegie mellon cylab which is oriented towards high school students and arizona state university supported pwn college beyond educational ctf events and resources ctfs has been shown to be a highly effective way to instill cybersecurity concepts in the classroom ctfs have been included in undergraduate computer science classes such as introduction to information security at the national university of singapore ctfs are also popular in military academies they are often included as part of the curriculum for cybersecurity courses with the nsa organized cyber exercise culminating in a ctf competition between the us service academies and military colleges competitions many ctf organizers register their competition with the ctftime platform this allows the tracking of the position of teams over time and across competitions these include plaid parliament of pwning more smoked leet chicken dragon sector dcua eat sleep pwn repeat perfect blue organizers and blue water overall the plaid parliament of pwning and dragon sector have both placed first worldwide the most with three times each community competitions every year there are dozens of ctfs organized in a variety of formats many ctfs are associated with cybersecurity conferences such as def con hitcon and bsides the def con ctf an attack defence ctf is notable for being one of the oldest ctf competitions to exist and has been variously referred to as the world series superbowl and olympics of hacking by media outlets the nyu tandon hosted cybersecurity awareness worldwide csaw ctf is one of the largest open entry competitions for students learning cybersecurity from around the world in 2021 it hosted over 1200 teams during the qualification round in addition to conference organized ctfs many ctf clubs and teams organize ctf competitions many ctf clubs and teams are associated with universities such as the cmu associated plaid parliament of pwning which hosts plaidctf and the asu associated shellphish government supported competitions governmentally supported ctf competitions include the darpa cyber grand challenge and enisa european cybersecurity challenge in 2023 the us space force sponsored hack a sat ctf competition included for the first time a live orbital satellite for participants to exploit corporate supported competitions corporations and other organizations sometimes use ctfs as a training or evaluation exercise the benefits of ctfs are similar to those of using ctfs in an educational environment in addition to internal ctf exercises some corporations such as google and tencent host publicly accessible ctf competitions in popular culture in mr robot a qualification round for the def con ctf competition is depicted in the season 3 opener eps3 0_power saver mode h the logo for def con can se seen in the background in the undeclared war a ctf is depicted in the opening scene of the series as a recruitment exercise used by gchq go go squid a chinese television series is based around training for and competing in highly stylized ctf competitions see also wargame hacking cyberwarfare preparedness hackathons competitive programming cybersecurity in popular culture privacy references external links ctftime org an archive of historic current and future ctf competitions ",
            "total_words": 853,
            "unique_words_percentage": 45.48651817116061,
            "stopwords_percentage": 35.63892145369285
        },
        {
            "title": "NIST Cybersecurity Framework",
            "link": "https://en.wikipedia.org/wiki/NIST_Cybersecurity_Framework",
            "content": "the nist cybersecurity framework csf is a set of voluntary guidelines designed to help organizations assess and improve their ability to prevent detect and respond to cybersecurity risks developed by the u s national institute of standards and technology nist the framework was initially published in 2014 for critical infrastructure sectors but has since been widely adopted across various industries including government and private enterprises globally the framework integrates existing standards guidelines and best practices to provide a structured approach to cybersecurity risk management the csf is composed of three primary components the core implementation tiers and profiles the core outlines five key cybersecurity functions identify protect detect respond and recover each of which is further divided into specific categories and subcategories these functions offer a high level outcome driven approach to managing cybersecurity risks the implementation tiers help organizations assess the sophistication of their cybersecurity practices while the profiles allow for customization based on an organization s unique risk profile and needs since its inception the csf has undergone several updates to reflect the evolving nature of cybersecurity version 1 1 released in 2018 introduced enhancements related to supply chain risk management and self assessment processes the most recent update version 2 0 was published in 2024 expanding the framework s applicability and adding new guidance on cybersecurity governance and continuous improvement practices the nist cybersecurity framework is used internationally and has been translated into multiple languages it serves as a benchmark for cybersecurity standards helping organizations align their practices with recognized global standards such as iso iec 27001 and cobit while widely praised the framework has been criticized for the cost and complexity involved in its implementation particularly for small and medium sized enterprises overview the nist cybersecurity framework csf is a set of guidelines developed by the u s national institute of standards and technology nist to help organizations manage and mitigate cybersecurity risks it draws from existing standards guidelines and best practices to provide a flexible and scalable approach to cybersecurity the framework provides a high level taxonomy of cybersecurity outcomes and offers a methodology for assessing and managing those outcomes additionally it addresses the protection of privacy and civil liberties in a cybersecurity context the csf has been translated into multiple languages and is widely used by governments businesses and organizations across various sectors according to a 2016 survey 70 of organizations view the nist cybersecurity framework as a best practice for computer security though some have noted that implementation can require significant investment the framework is designed to be flexible and adaptable providing high level guidance that allows individual organizations to determine the specifics of implementation based on their unique needs and risk profiles version 1 0 of the framework was published in 2014 primarily targeting operators of critical infrastructure a public draft of version 1 1 was released for comment in 2017 and the final version was published on april 16 2018 version 1 1 retained compatibility with the original framework while introducing additional guidance on areas such as supply chain risk management version 2 0 released in 2024 further expanded the framework s scope and introduced new guidelines on self assessment and cybersecurity governance the framework consists of three main components the core profiles and tiers the core provides a comprehensive set of activities outcomes and references related to various aspects of cybersecurity the implementation tiers help organizations assess their cybersecurity practices and sophistication while the profiles allow organizations to tailor the framework to their specific requirements and risk assessments organizations typically start by developing a current profile to describe their existing cybersecurity practices and outcomes from there they can create a target profile to outline the desired future state and define the steps needed to achieve it alternatively organizations can adopt a baseline profile based on their sector or specific industry needs research indicates that the nist cybersecurity framework has the potential to influence cybersecurity standards both within the united states and internationally particularly in sectors where formal cybersecurity standards are still emerging this influence could foster better international cybersecurity practices benefiting businesses that operate across borders and contributing to global cybersecurity efforts functions and categories of cybersecurity activities the nist cybersecurity framework organizes its core material into five functions which are subdivided into a total of 23 categories for each category it defines a number of subcategories of cybersecurity outcomes and security controls with 108 subcategories in all for each subcategory it also provides informative resources referencing specific sections of a variety of other information security standards including iso 27001 cobit nist sp 800 53 ansi isa 62443 and the council on cybersecurity critical security controls ccs csc now managed by the center for internet security special publications sp aside most of the informative references requires a paid membership or purchase to access their respective guides the cost and complexity of the framework has resulted in bills from both houses of congress that direct nist to create cybersecurity framework guides that are more accessible to small and medium businesses here are the functions and categories along with their unique identifiers and definitions as stated in the framework document identify develop the organizational understanding to manage cybersecurity risk to systems assets data and capabilities asset management id am the data personnel devices systems and facilities that enable the organization to achieve business purposes are identified and managed consistent with their relative importance to business objectives and the organization s risk strategy business environment id be the organization s mission objectives stakeholders and activities are understood and prioritized this information is used to inform cybersecurity roles responsibilities and risk management decisions governance id gv the policies procedures and processes to manage and monitor the organization s regulatory legal risk environmental and operational requirements are understood and inform the management of cybersecurity risk risk assessment id ra the organization understands the cybersecurity risk to organizational operations including mission functions image or reputation organizational assets and individuals risk management strategy id rm the organization s priorities constraints risk tolerances and assumptions are established and used to support operational risk decisions supply chain risk management id sc the organization s priorities constraints risk tolerances and assumptions are established and used to support risk decisions associated with managing supply chain risk the organization has in place the processes to identify assess and manage supply chain risks protect develop and implement the appropriate safeguards to ensure delivery of critical infrastructure services access control pr ac access to assets and associated facilities is limited to authorized users processes or devices and to authorized activities and transactions awareness and training pr at the organization s personnel and partners are provided cybersecurity awareness education and are adequately trained to perform their information security related duties and responsibilities consistent with related policies procedures and agreements data security pr ds information and records data are managed consistent with the organization s risk strategy to protect the confidentiality integrity and availability of information information protection processes and procedures pr ip security policies that address purpose scope roles responsibilities management commitment and coordination among organizational entities processes and procedures are maintained and used to manage protection of information systems and assets maintenance pr ma maintenance and repairs of industrial control and information system components is performed consistent with policies and procedures protective technology pr pt technical security solutions are managed to ensure the security and resilience of systems and assets consistent with related policies procedures and agreements detect develop and implement the appropriate activities to identify the occurrence of a cybersecurity event anomalies and events de ae anomalous activity is detected in a timely manner and the potential impact of events is understood security continuous monitoring de cm the information system and assets are monitored at discrete intervals to identify cybersecurity events and verify the effectiveness of protective measures detection processes de dp detection processes and procedures are maintained and tested to ensure timely and adequate awareness of anomalous events respond develop and implement the appropriate activities to take action regarding a detected cybersecurity incident response planning rs rp response processes and procedures are executed and maintained to ensure timely response to detected cybersecurity events communications rs co response activities are coordinated with internal and external stakeholders as appropriate to include external support from law enforcement agencies analysis rs an analysis is conducted to ensure adequate response and support recovery activities mitigation rs mi activities are performed to prevent expansion of an event mitigate its effects and eradicate the incident improvements rs im organizational response activities are improved by incorporating lessons learned from current and previous detection response activities recover develop and implement the appropriate activities to maintain plans for resilience and to restore any capabilities or services that were impaired due to a cybersecurity incident recovery planning rc rp recovery processes and procedures are executed and maintained to ensure timely restoration of systems or assets affected by cybersecurity events improvements rc im recovery planning and processes are improved by incorporating lessons learned into future activities communications rc co restoration activities are coordinated with internal and external parties such as coordinating centers internet service providers owners of attacking systems victims other csirts and vendors updates in 2021 nist released security measures for eo critical software use under executive order eo 14028 to outline security measures intended to better protect the use of deployed eo critical software in agencies operational environments journey to csf 2 0 the nist cybersecurity framework is meant to be a living document meaning it will be updated and improved over time to keep up with changes in technology and cybersecurity threats as well as to integrate best practices and lessons learned since releasing version 1 1 in 2018 stakeholders have provided feedback that the csf needed to be updated in february 2022 nist released a request for information on ways to improve the csf and released a subsequent concept paper in january of 2023 with proposed changes most recently nist released its discussion draft the nist cybersecurity framework 2 0 core with implementation examples and has requested public comments be submitted by november 4 2023 main changes the following is a list of the major changes to the framework from version 1 1 to 2 0 the title of the framework has changed from framework for improving critical infrastructure cybersecurity to cybersecurity framework the scope of the framework has been updated to reflect the large population of organizations that use the framework implementation examples have been added to provide practical and action oriented processes to help users achieve the csf subcategories additionally the framework profiles have been revised and expanded to demonstrate the various purposes of the profiles a new function govern has been added to provide organizational context and the roles and responsibilities associated with developing a cybersecurity governance model there is also an additional category in this function focused on cybersecurity supply chain risk management the latest update also provides greater information on cybersecurity assessments by placing greater importance on the continuous improvement of security through a new improvement category in the identify function see also cybersecurity cyber security standards privacy critical infrastructure protection iso iec 27001 2013 an information security standard from the international organization for standardization cobit control objectives for information and related technologies a related framework from isaca nist special publication 800 53 security and privacy controls for federal information systems and organizations risk management framework us federally mandated risk framework broader scope than cybersecurity references this article incorporates public domain material from nist cybersecurity framework pdf national institute of standards and technology external links official website harnessing the power of the nist cybersecurity framework nistir 8374 draft cybersecurity framework profile for ransomware risk management preliminary draft informative references home derived relationship mapping informative reference catalog",
            "total_words": 1961,
            "unique_words_percentage": 31.71851096379398,
            "stopwords_percentage": 33.70729219785824
        },
        {
            "title": "Ghana",
            "link": "https://en.wikipedia.org/wiki/Ghana",
            "content": "ghana officially the republic of ghana is a country in west africa it lies adjacent to the gulf of guinea and the atlantic ocean to the south sharing a border with ivory coast in the west burkina faso in the north and togo in the east ghana covers an area of 239 567 km2 92 497 sq mi spanning diverse ecologies from coastal savannas to tropical rainforests with nearly 35 million inhabitants ghana is the second most populous country in west africa the capital and largest city is accra other significant cities include kumasi tamale and sekondi takoradi in 1957 ghana became the first colony in sub saharan africa to achieve sovereignty under the leadership of kwame nkrumah the earliest kingdoms to emerge in ghana were the kingdom of dagbon in the north and the bonoman in the south with bonoman existing in the area during the 11th century the ashanti empire and other akan kingdoms in the south emerged over the centuries beginning in the 15th century the portuguese empire followed by other european powers contested the area for trading rights until the british ultimately established control of the coast by the 19th century following more than a century of colonial resistance the current borders of the country took shape encompassing four separate british colonial territories gold coast ashanti the northern territories and british togoland these were unified as an independent dominion within the commonwealth of nations on 6 march 1957 ghana became the first colony in sub saharan africa to achieve sovereignty that is gain independence under president kwame nkrumah it became influential in decolonisation efforts and the pan african movement ghana is a multi ethnic country with linguistic and religious groups while the akan are the largest ethnic group they constitute a plurality most ghanaians are christians 71 3 almost a fifth are muslims a tenth practice traditional faiths or report no religion ghana is a unitary constitutional democracy led by a president who is head of state and head of government for political stability in africa ghana ranked seventh in the 2012 ibrahim index of african governance and fifth in the 2012 fragile states index it has maintained since 1993 one of the freest and most stable governments on the continent and it performs relatively well in healthcare economic growth and human development so that it has a significant influence in west africa and africa as a whole ghana is highly integrated in international affairs being a founding member of the non aligned movement and the african union and a member of the economic community of west african states the group of 24 and the commonwealth of nations etymology the name ghana comes from wagadu a vast empire in west africa from the 3rd to 12th centuries wagadu was termed ghana by arab traders involved in the trans saharan trade ghana is thought to originate from the title kaya maghan of the rulers of wagadu which translates as ruler of gold as the gold coast colony prepared for independence the nation s leader and first prime minister later first president kwame nkrumah the one who led ghana to independence settled on ghana aiming to evoke a sense of unity and liberation among the ghanaian people the name was a powerful reminder of their shared heritage and the legacy of the ancient empire that once thrived in the wider region it encapsulated the aspirations of the ghanaian people for self governance progress and a future marked by dignity and resilience history medieval kingdoms the earliest recorded kingdoms to emerge in modern ghana were the mole dagbon states before the unification of dagbon societies were decentralised and headed by the tindaamba singular tindana these decentralised states were unified by king gbewaa who lived a long life and formed a stable peaceful society dagbon extended beyond the boundaries of present day ghana kingdoms that emerged from dagbon include the mossi kingdoms of burkina faso and bouna kingdom of ivory coast the kingdom enjoyed great prosperity establishing ghana s earliest educational systems and using a writing script prior to european invasion female chiefs who rule over male subjects are present in the kingdom and inheritance is both patrilineal and matrilineal the yaa naa is the king of dagbon and the gundo naa is the queen the kingdom remained uncolonised in 1896 germany invaded eastern dagbon naya and burnt down its capital yendi during the battle of adibo the akan speaking peoples began to move into what later became ghana toward the 15th century by the 16th century the akans were established in the akan state called bonoman for which the brong ahafo region was named from the 17th century akans emerged from what is believed to have been the bonoman area to create akan states mainly based on gold trading these states included bonoman brong ahafo region ashanti ashanti region denkyira western north region mankessim kingdom central region and akwamu eastern region by the 19th century the territory of the southern part of ghana was included in the kingdom of ashanti the government of the ashanti empire operated first as a loose network and eventually as a centralised kingdom with a specialised bureaucracy centred in the capital city of kumasi prior to akan contact with europeans the akan people created an economy based on principally gold and gold bar precious metals which were traded with other states in africa the ga dangme and ewe migrated westward from south western nigeria the ewe formerly known as dogbo migrated from oyo area with their gbe speaking kinsmen adja fon phla phera and ogun gun and in transition settled at ketou in benin republic tado in togo dogbo nyigbo in benin republic and with nortsie a walled town in present day togo as their final dispersal point their dispersal from nortsie was necessitated by the high handed rule of king agorkorli agɔ akɔli who was the reigning monarch of the tribe at that time the ewe in ghana speak three principal dialects anlo along the coast tongu along the volta river and ewedome in the hill country side the ga dangme occupy the greater accra region and parts of the eastern region while the ewe are found in the volta region as well as the neighbouring togo benin republic and nigeria around badagry area european contact and colonialism akan trade with european states began after contact with the portuguese in the 15th century european contact was by the portuguese people who came to the gold coast region in the 15th century to trade the portuguese then established the portuguese gold coast costa do ouro focused on the availability of gold the portuguese built a trading lodge at a coastal settlement called anomansah the perpetual drink which they renamed são jorge da mina in 1481 king john ii of portugal commissioned diogo de azambuja to build the elmina castle which was completed in three years by 1598 the dutch had joined the portuguese in the gold trade establishing the dutch gold coast nederlandse bezittingen ter kuste van guinea dutch properties at the guinea coast and building forts at fort komenda and kormantsi in 1617 the dutch captured the elmina castle from the portuguese and axim in 1642 fort st anthony european traders had joined in gold trading by the 17th century including the swedes establishing the swedish gold coast svenska guldkusten and denmark norway establishing the danish gold coast danske guldkyst or dansk guinea european traders participated in the atlantic slave trade in this area more than 30 forts and castles were built by the merchants the germans established the brandenburger gold coast or groß friedrichsburg in 1874 great britain established control over some parts of the country assigning these areas the status of the british gold coast military engagements occurred between british colonial powers and akan nation states the kingdom of ashanti defeated the british some times in the 100 year long anglo ashanti wars and eventually lost with the war of the golden stool in 1900 transition to independence in 1947 the newly formed united gold coast convention led by the big six called for self government within the shortest possible time following the 1946 gold coast legislative election kwame nkrumah a ghanaian nationalist who led ghana from 1957 to 1966 as the country s first prime minister and president formed the convention people s party in 1949 with the motto self government now the party initiated a positive action campaign involving non violent protests strikes and non cooperation with the british authorities nkrumah was arrested and sentenced to one year imprisonment during this time in the gold coast s 1951 general election he was elected to parliament and was released from prison he became prime minister in 1952 and began a policy of africanization at midnight of march 6 1957 the gold coast ashanti the northern territories and british togoland were unified as one single independent dominion within the british commonwealth under the name ghana this was done under the ghana independence act 1957 the current flag of ghana consisting of the colours red gold green and a black star dates back to this unification on 1 july 1960 following the ghanaian constitutional referendum and ghanaian presidential election nkrumah declared ghana a republic and assumed the presidency 6 march is the nation s independence day and 1 july is celebrated as republic day nkrumah led an authoritarian regime in ghana as he repressed political opposition and conducted elections that were not free and fair in 1964 a constitutional amendment made ghana a one party state with nkrumah as president for life of both the nation and its party nkrumah was the first african head of state to promote the concept of pan africanism which he had been introduced to during his studies at lincoln university pennsylvania in the united states at the time when marcus garvey was known for his back to africa movement he merged the teachings of garvey martin luther king jr and the naturalised ghanaian scholar w e b du bois into the formation of 1960s ghana osagyefo dr kwame nkrumah as he became known played an instrumental part in the founding of the non aligned movement and in establishing the kwame nkrumah ideological institute to teach his ideologies of communism and socialism his life achievements were recognised by ghanaians during his centenary birthday celebration and the day was instituted as a public holiday in ghana founders day operation cold chop and aftermath the government of nkrumah was subsequently overthrown in a coup by the ghana armed forces codenamed operation cold chop this occurred while nkrumah was abroad with zhou enlai in the people s republic of china on a mission to hanoi vietnam to help end the vietnam war the coup took place on 24 february 1966 led by colonel emmanuel kwasi kotoka and brigadier akwasi afrifa the national liberation council was formed chaired by lieutenant general joseph a ankrah a series of alternating military and civilian governments often affected by economic instabilities ruled ghana from 1966 ending with the ascent to power of flight lieutenant jerry john rawlings of the provisional national defence council in 1981 these changes resulted in the suspension of the constitution in 1981 and the banning of political parties the economy soon declined so rawlings negotiated a structural adjustment plan changing many old economic policies and growth recovered during the mid 1980s a new constitution restoring multi party system politics was promulgated in the presidential election of 1992 in which rawlings was elected and again in the general election of 1996 in a tribal war in northern ghana in 1994 between the konkomba and other ethnic groups including the nanumba dagomba and gonja between 1 000 and 2 000 people were killed and 150 000 people were displaced after the 2000 general election john kufuor of the new patriotic party became president of ghana on 7 january 2001 and was re elected in 2004 thus also serving two terms the term limit as president of ghana and marking the first time under the fourth republic that power was transferred from one legitimately elected head of state and head of government to another nana akufo addo the ruling party candidate was defeated in a very close 2008 general election by john atta mills of the national democratic congress mills died of natural causes and was succeeded by vice president john mahama on 24 july 2012 following the 2012 general election mahama became president in his own right and ghana was described as a stable democracy as a result of the 2016 general election nana akufo addo became president on 7 january 2017 he was re elected after a tightly contested election in 2020 to combat deforestation on 11 june 2021 ghana inaugurated green ghana day with the aim of planting five million trees in a concentrated effort to preserve the country s rainforest cover geography ghana is located on the gulf of guinea a few degrees north of the equator it spans an area of 238 540 km2 92 101 sq mi and has an atlantic coastline that stretches 560 kilometres 350 miles on the gulf of guinea in the atlantic ocean to its south dodi island and bobowasi island are near the south coast it lies between latitudes 4 45 n and 11 n and longitudes 1 15 e and 3 15 w the prime meridian passes through ghana specifically through tema ghana is geographically closer to the intersection of the prime meridian and the equator than any other country since this point 0 0 is located in the atlantic ocean approximately 614 km 382 mi off the south east coast of ghana grasslands mixed with south coastal shrublands and forests dominate ghana with forest extending northward from the coast 320 kilometres 200 miles and eastward for a maximum of about 270 kilometres 170 miles with locations for mining of industrial minerals and timber ghana is home to five terrestrial ecoregions eastern guinean forests guinean forest savanna mosaic west sudanian savanna central african mangroves and guinean mangroves the white volta river and its tributary black volta flow south through ghana to lake volta the world s third largest reservoir by volume and largest by surface area formed by the hydroelectric akosombo dam completed in 1965 the volta flows out of lake volta into the gulf of guinea the northernmost part of ghana is pulmakong and the southernmost part of ghana is cape three points the climate of ghana is tropical and there is wet season and dry season ghana sits at the intersection of three hydro climatic zones changes in rainfall weather conditions and sea level rise affect the salinity of coastal waters this is expected to negatively affect both farming and fisheries in 2015 the government produced a document titled ghana s intended nationally determined contribution following that ghana signed the paris climate agreement in 2016 politics ghana is a unitary presidential constitutional democracy with a parliamentary multi party system that is dominated by two parties the national democratic congress ndc and the new patriotic party npp ghana alternated between civilian and military governments until january 1993 when the military government gave way to the fourth republic of ghana after presidential and parliamentary elections in late 1992 the 1992 constitution of ghana divides powers among a commander in chief of the ghana armed forces president of ghana parliament parliament of ghana cabinet cabinet of ghana council of state ghanaian council of state and an independent judiciary judiciary of ghana the government is elected by universal suffrage after every four years nana akufo addo won the presidency in the general election in 2016 defeating incumbent john mahama he also won the 2020 election after the presidential election results were challenged at the supreme court by flagbearer of the ndc john mahama presidents are limited to two four year terms in office the president can serve a second term only upon re election the 2012 fragile states index indicated that ghana is ranked the 67th least fragile state in the world and the fifth least fragile state in africa ghana ranked 112th out of 177 countries on the index ghana ranked as the 64th least corrupt and politically corrupt country in the world out of all 174 countries ranked and ranked as the fifth least corrupt and politically corrupt country in africa out of 53 countries in the 2012 transparency international corruption perception index ghana was ranked seventh in africa out of 53 countries in the 2012 ibrahim index of african governance the ibrahim index is a comprehensive measure of african government based on variables which reflect the success with which governments deliver essential political goods to its citizens according to 2023 v dem democracy indices ghana is ranked 67th electoral democracy worldwide and 10th electoral democracy in africa foreign relations since independence ghana has been devoted to ideals of nonalignment and is a founding member of the non aligned movement ghana favours international and regional political and economic co operation and is an active member of the united nations and the african union ghana has a strong relationship with the united states three recent u s presidents bill clinton george w bush and barack obama and a vice president kamala harris have made diplomatic trips to ghana many ghanaian diplomats and politicians hold positions in international organisations including ghanaian diplomat and former secretary general of the united nations kofi annan international criminal court judge akua kuenyehia as well as former president jerry john rawlings and former president john agyekum kufuor who both served as diplomats of the united nations in september 2010 president john atta mills visited china on an official visit mills and china s former president hu jintao marked the 50th anniversary of diplomatic ties between the two nations at the great hall of the people china reciprocated with an official visit in november 2011 by the vice chairman of the standing committee of the national people s congress of china zhou tienong who visited ghana and met with ghana s president john mahama china recently became one of the top investing countries of ghana which predominantly focus on infrastructure natural resources and manufacturing sectors have promoted economic growth job creation and technology transfer in ghana however concerns regarding the sustainability of chinese financed projects environmental impacts and the lack of transparency in their investments call for a careful assessment of these collaborations iranian president mahmoud ahmadinejad met with mahama in 2013 to hold discussions on strengthening the non aligned movement and also co chair a bilateral meeting between ghana and iran at the ghanaian presidential palace flagstaff house the sustainable development goals sdg were integrated into ghana s development agenda and the budget according to reports the sdgs were implemented through a decentralized planning approach this allows for stakeholders participation such as in un agencies traditional leaders civil society organizations academia and others the 17 sdgs are a global call to action to end poverty among others and the un and its partners in the country are working towards achieving them according to the president nana akufo addo ghana was the first sub saharan african country to achieve the goal of halving poverty as contained in goal 1 of the millennium development goals military in 1957 the ghana armed forces gaf consisted of its headquarters support services three battalions of infantry and a reconnaissance squadron with armoured vehicles president nkrumah aimed at rapidly expanding the gaf to support the united states of africa ambitions thus in 1961 4th and 5th battalions were established and in 1964 6th battalion was established from a parachute airborne unit originally raised in 1963 today ghana is a regional power and regional hegemon in his book shake hands with the devil canadian forces commander roméo dallaire highly rated the gaf soldiers and military personnel the military operations and military doctrine of the gaf are conceptualised in the constitution ghana s law on armed force military strategy and kofi annan international peacekeeping training centre agreements to which gaf is attestator gaf military operations are executed under the auspices and imperium of the ministry of defence although ghana is relatively peaceful and is often considered being one of the least violent countries in the region ghana has experienced political violence in the past and 2017 has thus far seen an upward trend in incidents motivated by political grievances law enforcement the ghana police service and the criminal investigation department are the main law enforcement agencies responsible for the detection of crime maintenance of law and order and the maintenance of internal peace and security the ghana police service has eleven specialised police units including a militarized police rapid deployment force and marine police unit the ghana police service operates in 12 divisions ten covering the regions of ghana one assigned specifically to the seaport and industrial hub of tema and the twelfth being the railways ports and harbours division the ghana police service s marine police unit and division handles issues that arise from the country s offshore oil and gas industry the ghana prisons service and the sub division borstal institute for juveniles administers incarceration ghana retains and exercises the death penalty for treason corruption robbery piracy drug trafficking rape and homicide the new sustainable development goals adopted by the united nations call for the international community to come together to promote the rule of law support equal access to justice for all reduce corruption and develop effective accountable and transparent institutions at all levels ghana is used as a key narcotics industry transshipment point by traffickers usually from south america as well as some from other african nations in 2013 the un chief of the office on drugs and crime stated that west africa is completely weak in terms of border control and the big drug cartels from colombia and latin america have chosen africa as a way to reach europe there is not a wide or popular knowledge about the narcotics industry and intercepted narcotics within ghana since it is an underground economy the social context within which narcotic trafficking storage transportation and repacking systems exist in ghana and the state s location along the gulf of guinea makes ghana an attractive country for the narcotics business the narcotics control board has impounded container ships at the sekondi naval base in the takoradi harbour these ships were carrying thousands of kilograms of cocaine with a street value running into billions of ghana cedis however drug seizures saw a decline in 2011 drug cartels are using new methods in narcotics production and narcotics exportation to avoid ghanaian security agencies underdeveloped institutions porous open borders and the existence of established smuggling organisations contribute to ghana s position in the narcotics industry president mills initiated ongoing efforts to reduce the role of airports in ghana s drug trade human rights homosexual acts are prohibited by law in ghana according to a 2013 survey by the pew research center 96 of ghanaians believe that homosexuality should not be accepted by society sometimes elderly women in ghana are accused of witchcraft particularly in rural ghana issues of witchcraft mainly remain as speculations based on superstitions within families in some parts of northern ghana there exist what are called witch camps these are said to house a total of around 1 000 people accused of witchcraft the ghanaian government has announced that it intends to close the camps economy ghana possesses industrial minerals hydrocarbons and precious metals it is an emerging designated digital economy with mixed economy hybridisation and an emerging market it has an economic plan target known as the ghana vision 2020 this plan envisions ghana as the first african country to become a developed country between 2020 and 2029 and a newly industrialised country between 2030 and 2039 this excludes fellow group of 24 member and sub saharan african country south africa which is a newly industrialised country ghana s economy has ties to the chinese yuan renminbi along with ghana s vast gold reserves in 2013 the bank of ghana began circulating the renminbi throughout ghanaian state owned banks and to the ghana public as hard currency along with the national ghanaian cedi for second national trade currency between 2012 and 2013 38 of rural dwellers were experiencing poverty whereas only 11 of urban dwellers were urban areas hold greater opportunity for employment particularly in informal trade while nearly all 94 percent of rural poor households participate in the agricultural sector the volta river authority and the ghana national petroleum corporation both state owned are the two major electricity producers the akosombo dam built on the volta river in 1965 along with the bui dam the kpong dam and several other hydroelectric dams provide hydropower in addition the government sought to build the second nuclear power plant in africa the ghana stock exchange is the fifth largest on continental africa and 3rd largest in sub saharan africa with a market capitalisation of gh 57 2 billion or cn 180 4 billion in 2012 with the south africa jse limited as first the ghana stock exchange was the second best performing stock exchange in sub saharan africa in 2013 ghana produces high quality cocoa it is the second largest producer of cocoa globally and its icco membership helps in its international cocoa trade ghana is classified as a middle income country services account for 50 of gdp followed by manufacturing 24 1 extractive industries 5 and taxes 20 9 ghana has an increasing primary manufacturing economy and export of digital technology goods along with assembling and exporting automobiles and ships diverse resource rich exportation of industrial minerals agricultural products primarily cocoa petroleum and natural gas and industries such as information and communications technology primarily via ghana s state digital technology corporation rlg communications which manufactures tablet computers with smartphones and various consumer electronics urban electric cars have been manufactured in ghana since 2014 it announced plans to issue government debt by way of social and green bonds in autumn 2021 making it the first african country to do so the country which was planning to borrow up to 5 billion in international markets would use the proceeds from these sustainable bonds to refinance debt used for social and environmental projects and pay for educational or health only a few other nations have sold them so far including chile and ecuador the country will use the proceeds to forge ahead with a free secondary school initiative started in 2017 among other programs despite having recorded its lowest economic growth rate in 37 years in 2020 it produces and exports hydrocarbons such as sweet crude oil and natural gas the 100 state owned filling station company ghana oil company is the number 1 petroleum and gas filling station and the 100 state owned state oil company ghana national petroleum corporation oversees hydrocarbon exploration and production of petroleum and natural gas reserves ghana aims to further increase the output of oil to 2 2 million barrels 350 000 m3 per day and gas to 34 000 000 cubic metres 1 2 10 9 cu ft per day the jubilee oil field which contains up to 3 billion barrels 480 000 000 m3 of sweet crude oil was discovered in 2007 ghana is believed to have up to 5 billion barrels 790 000 000 m3 to 7 billion barrels 1 1 109 m3 of petroleum in reserves which is the fifth largest in africa and the 21st to 25th largest proven reserves in the world it also has up to 1 7 1011 cubic metres 6 10 12 cu ft of natural gas in reserves the government has drawn up plans to nationalise petroleum and natural gas reserves to increase government revenue in 2015 ghana produced 88 metric tonnes of gold as per the our world in data report as of 2019 ghana was the 7th largest producer of gold in the world producing 140 tonnes that year this record saw ghana surpass south africa in output for the first time making ghana the largest gold producer in africa in addition to gold ghana exports silver timber diamonds bauxite and manganese and has other mineral deposits ghana ranks 9th in the world in diamond export and reserve size the government has drawn up plans to nationalize mining industry to increase government revenue shortages of electricity in 2015 and 2016 led to dumsor persistent irregular and unpredictable electric power outages increasing the interest in renewables as of 2019 there is a surplus of electricity the judicial system of ghana deals with corruption economic malpractice and lack of economic transparency according to transparency international s corruption perceptions index of 2018 out of 180 countries ghana was ranked 78th with a score of 41 on a scale where a 0 9 score means highly corrupt and a 90 100 score means very clean this was based on perceived levels of public sector corruption science and technology ghana launched a cellular mobile network in 1992 it was later connected to the internet and introduced adsl broadband services it was ranked 99th in the global innovation index in 2024 the ghana space science and technology centre gsstc and ghana space agency ghsa oversee space exploration and space programmes gsstc and ghsa worked to have a national security observational satellite launched into orbit in 2015 ghana s annual space exploration expenditure has been 1 of its gdp to support research in science and technology in 2012 ghana was elected to chair the commission on science and technology for sustainable development in the south comsats ghana has a joint effort in space exploration with the south african national space agency tourism in 2011 tourists visiting ghana numbered 1 087 000 with arrivals including south americans asians europeans and north americans among the attractions and tourist destinations are waterfalls such as kintampo waterfalls and the largest waterfall in west africa wli waterfalls the coastal palm lined sandy beaches caves mountains rivers and reservoirs and lakes such as lake bosumtwi and the largest human made lake in the world by surface area lake volta dozens of forts and castles world heritage sites nature reserves and national parks notable castles are cape coast castle and the elmina castle castles mark where blood was shed in the slave trade and preserve and promote the african heritage stolen and destroyed through the slave trade the world heritage convention of unesco named ghana s castles and forts as world heritage monuments based on the criterion the castles and forts of ghana shaped not only ghana s history but that of the world over four centuries as the focus of first the gold trade and then the slave trade they are a significant and emotive symbol of european african encounters and of the starting point of the african diaspora the world economic forum statistics in 2010 showed that out of the world s favourite tourist destinations ghana was ranked 108th out of 139 countries the country had moved two places up from the 2009 rankings in 2011 forbes magazine published that ghana was ranked the 11th most friendly country in the world the assertion was based on a survey in 2010 of a cross section of travellers of all the african countries that were included in the survey ghana ranked highest tourism is the fourth highest earner of foreign exchange for the country in 2024 ghana ranked as the 55th most peaceful country in the world up and down the coastline surfing spots have been identified and cultivated by locals and internationals surfers have made trips to the country to sample the waves surfers carried their boards amid traditional fishing vessels according to destination pride a data driven search platform used to visualize the world s lgbtq laws rights and social sentiment ghana s pride score is 22 out of 100 demographics as of 2024 united nations reports ghana has a population of 34 581 288 as of 2018 around 29 of the population is under the age of 15 while persons aged 15 64 make up 57 8 of the population the 2010 census reported that the largest ethnic groups are the akan 47 3 the mole dagbani 18 5 the ewe 13 9 the ga dangme 7 4 the gurma 5 7 and the guan 3 7 as of 2024 united nations reports the median age of ghanaian citizens is 21 years old ghana contributes 0 42 to the total world population with recent legal immigration of skilled workers who possess ghana cards there is a small population of chinese malaysian indian middle eastern and european nationals in 2010 the ghana immigration service reported many economic migrants and illegal immigrants inhabiting ghana 14 6 or 3 1 million of ghana s 2010 population predominantly nigerians burkinabe citizens togolese citizens and malian citizens in 1969 under the ghana aliens compliance order enacted by then prime minister kofi abrefa busia the border guard unit deported more than 3 000 000 aliens and illegal immigrants in three months as they made up 20 of the population at the time in 2013 there was a mass deportation of illegal miners more than 4 000 of whom were chinese nationals languages english is the official language of ghana additionally there are eleven languages that have the status of government sponsored languages akan languages asante twi akuapem twi fante bono which have a high degree of mutual intelligibility and nzema which is less intelligible with the above dangme ewe ga guan kasem mole dagbani languages dagaare and dagbanli of these asante twi is the most widely spoken because ghana is surrounded by french speaking countries french is widely taught in schools and used for commercial and international economic exchanges since 2005 ghana has been an associate member of the organisation internationale de la francophonie the global organisation that unites french speaking countries 84 nations on six continents in 2005 more than 350 000 ghanaian children studied french in schools since then its status has been progressively updated to a mandatory language in every junior high school and it is in the process of becoming an official language ghanaian pidgin english also known as kru english or in akan kroo brofo is a variety of west african pidgin english spoken in accra and in the southern towns it can be divided into two varieties referred to as uneducated or non institutionalized pidgin and educated or institutionalized pidgin the former associated with uneducated or illiterate people and the latter acquired and used in institutions such as universities religion christianity is the largest religion in ghana with 71 3 of the population being members of various christian denominations as of the 2021 census islam is practised by 20 of the total population according to a 2012 report by pew research 51 of muslims are followers of sunni islam while approximately 16 belong to the ahmadiyya movement and around 8 identify with shia islam while the remainder are non denominational muslims there is no significant link between ethnicity and religion in ghana ghana has around 150 000 jehovah s witnesses universal health care and life expectancy ghana has a universal health care system national health insurance scheme nhis which is strictly designated for ghanaian nationals health care is variable throughout ghana and in 2012 more than 12 million ghanaian nationals were covered by the nhis urban centres are well served and contain most of the hospitals clinics and pharmacies there are more than 200 hospitals and ghana is a destination for medical tourism in 2010 there were 0 1 physicians per 1 000 people and as of 2011 0 9 hospital beds per 1 000 people in 2010 5 2 of ghana s gdp was spent on health in 2020 the who announced ghana became the second country in the who african region to attain regulatory system maturity level 3 the second highest in the four tiered who classification of national medicines regulatory systems life expectancy at birth in 2024 was 69 for a female and 64 for a male in 2013 infant mortality was to 39 per 1 000 live births sources vary on life expectancy at birth the world health organization who estimated 62 years for men and 64 years for women born in 2016 the fertility rate declined from 3 99 2000 to 3 28 2010 with 2 78 in urban region and 3 94 in rural region the united nations reports a fertility decline from 6 95 1970 to 4 82 2000 to 3 93 live births per woman in 2017 as of 2012 the hiv aids prevalence was estimated at 1 40 among adults aged 15 49 education the education system is divided into three parts basic education secondary cycle and tertiary education basic education lasts 11 years ages 4 15 it is divided into kindergarten two years primary school two modules of three years and junior high three years junior high school ends with the basic education certificate examination once certified the pupil can proceed to the secondary cycle hence the pupil has the choice between general education offered by the senior high school and vocational education offered by the technical senior high school or the technical and vocational institutes senior high school lasts 3 years and leads to the west african senior school certificate examination which is a prerequisite for enrollment in a university bachelor s degree programme 7 polytechnics are open to vocational students a bachelor s degree requires four years of study it can be followed by a one or two year master s degree programme which can be followed by a phd programme of at least three years 9 a polytechnic programme lasts two or three years ghana possesses colleges of education some of the universities are the university of ghana kwame nkrumah university of science and technology and university of cape coast there are more than 95 of children in school the female and male ages 15 24 years literacy rate was 81 in 2010 with males at 82 and females at 80 an education system annually attracts foreign students particularly in the university sector ghana has a free education six year primary school education system beginning at age 6 the government largely funds basic education comprising public primary schools and public junior high schools senior high schools were subsidised by the government until september 2017 2018 academic year that senior high education became free at the higher education level the government funds more than 80 of resources provided to public universities polytechnics and teacher training colleges as part of the free compulsory universal basic education fcube the government supplies all basic education schools with all their textbooks and other educational supplies like exercise books senior high schools are provided with all their textbook requirements by the government private schools acquire their educational material from private suppliers culture food and drink ghanaian cuisine includes an assortment of soups and stews with varied seafoods most ghanaian soups are prepared with vegetables meat poultry or fish fish is important in the diet with tilapia roasted and fried whitebait smoked fish and crayfish all being common components of ghanaian dishes banku akple is a common starchy food made from ground corn maize and cornmeal based staples kɔmi kenkey and banku akple are usually accompanied by some form of fried fish chinam or grilled tilapia and a very spicy condiment made from raw red and green chillies onions and tomatoes pepper sauce banku and tilapia is a combo served in most restaurants fufu is the most common exported ghanaian dish and is a delicacy across the african diaspora rice is an established staple meal across the country with various rice based dishes serving as breakfast lunch and dinner the main variants are waakye plain rice and stew eight kontomire or tomato gravy fried rice and jollof rice literature clothing during the 13th century ghanaians developed their unique art of adinkra printing hand printed and hand embroidered adinkra clothes were made and used exclusively by royalty for devotional ceremonies each of the motifs that make up the corpus of adinkra symbolism has a name and meaning derived from a proverb a historical event human attitude ethology plant life form or shapes of inanimate and man made objects the meanings of the motifs may be categorised into aesthetics ethics human relations and concepts the adinkra symbols have a decorative function as tattoos but also represent objects that encapsulate evocative messages that convey traditional wisdom aspects of life or the environment there are many symbols with distinct meanings often linked with proverbs in the words of anthony appiah they were one of the means in a pre literate society for supporting the transmission of a complex and nuanced body of practice and belief along with the adinkra cloth ghanaians use many cloth fabrics for their traditional attire the different ethnic groups have their own individual cloth the most well known is the kente cloth kente is a very important national costume and clothing and these clothes are used to make traditional and modern kente attire different symbols and different colours mean different things kente is the most famous of all the ghanaian clothes kente is a ceremonial cloth hand woven on a horizontal treadle loom and strips measuring about 4 inches wide are sewn together into larger pieces of cloths cloths come in various colours sizes and designs and are worn during very important social and religious occasions in a cultural context kente is more important than just a cloth as it is a visual representation of history and also a form of written language through weaving the term kente has its roots in the akan word kɛntɛn which means a basket and the first kente weavers used raffia fibres to weave cloths that looked like kenten a basket and thus were referred to as kenten ntoma meaning basket cloth the original akan name of the cloth was nsaduaso or nwontoma meaning a cloth hand woven on a loom however kente is the most frequently used term today kente is also woven by the ewe people ewe kente in the volta region the main weaving centres are agortime area and agbozume agbozume has a vibrant kente market attracting patrons from all over west africa and the diaspora contemporary ghanaian fashion includes traditional and modern styles and fabrics and has made its way into the african and global fashion scene the cloth known as african print fabric was created out of dutch wax textiles it is believed that in the late 19th century dutch ships on their way to asia stocked with machine made textiles that mimicked indonesian batik stopped at many west african ports on the way the fabrics did not do well in asia however in west africa mainly ghana where there was an already established market for cloths and textiles the client base grew and it was changed to include local and traditional designs colours and patterns to cater to the taste of the new consumers today outside of africa it is called ankara and it has a client base well beyond ghana and africa as a whole it is popular among caribbean peoples and african americans celebrities such as solange knowles and her sister beyoncé have been seen wearing african print attire many designers from countries in north america and europe are now using african prints and they have gained a global interest british luxury fashion house burberry created a collection around ghanaian styles american musician gwen stefani has repeatedly incorporated african prints into her clothing line and can often be seen wearing it internationally acclaimed ghanaian british designer ozwald boateng introduced african print suits in his 2012 collection music and dance music incorporates types of musical instruments such as the talking drum ensembles akan drum goje fiddle and koloko lute court music including the akan seperewa the akan atumpan the ga kpanlogo styles and log xylophones used in asonko music african jazz was created by kofi ghanaba a form of secular music is highlife highlife originated in the 19th and 20th centuries and spread throughout west africa in the 1990s a genre of music was created incorporating the influences of highlife afro reggae dancehall and hip hop this hybrid was called hiplife there are dances for occasions dances for celebrations include the adowa kpanlogo azonto klama agbadza borborbor and bamaya the nana otafrija pallbearing services also known as the dancing pallbearers come from the coastal town of prampram the group was featured in a bbc feature story in 2017 and footage from the story became part of an internet meme in the wake of the covid 19 world pandemic media chapter 12 of the 1992 constitution of ghana guarantees freedom of the press and independence of the media while chapter 2 prohibits censorship post independence private outlets closed during the military governments and media laws prevented criticism of government press freedoms were restored in 1992 and after the election in 2000 of kufuor the tensions between the private media and government decreased kufuor supported press freedom and repealed a libel law and maintained that the media had to act responsibly the media have been described as one of the most unfettered in africa in 1948 the gold coast film unit was set up in the information services department architecture there are two types of construction the series of adjacent buildings in an enclosure around a common and the round huts with grass roof the round huts with grass roof architecture are situated in the northern regions while the series of adjacent buildings are in the southern regions postmodern architecture and high tech architecture buildings are in the southern regions while heritage sites are evident in the more than 30 forts and castles in the country such as fort william and fort amsterdam ghana has museums that are situated inside castles and two are situated inside a fort the military museum and the national museum organise temporary exhibitions ghana has museums that allow an in depth look at specific regions with a number of museums providing insight into the traditions and history of the geographical areas the cape coast castle museum and st georges castle elmina castle museum offer guided tours the museum of science and technology provides its visitors with a look into the domain of scientific development through exhibits of objects of scientific and technological interest sports association football is the top spectator sport in ghana ghana has won the africa cup of nations four times the fifa u 20 world cup once and has participated in four consecutive fifa world cups 2006 2010 2014 and 2022 and has also won the fifa u 17 world cup twice the international federation of football history and statistics crowned asante kotoko sc as the african club of the 20th century ghana competes in the commonwealth games sending athletes in every edition since 1954 except for the 1986 games ghana has won 57 medals at the commonwealth games including 15 gold with all but one of their medals coming in athletics and boxing the country has also produced a number of boxers including azumah nelson a three time world champion nana yaw konadu also a three time world champion ike quartey and joshua clottey see also index of ghana related articles outline of ghana notes references further reading external links government ghana site the parliament of ghana site national commission on culture archived 30 april 2011 at the wayback machine site general information country profile from bbc news ghana from encyclopædia britannica ghana from ucb libraries govpubs ghana the world factbook central intelligence agency ghana profile from africa com wikimedia atlas of ghana the african activist archive project website has photographs of the all africa people s conference held in accra ghana 5 13 december 1958 including kwame nkrumah prime minister of ghana addressing the conference the american committee on africa delegation meeting with nkrumah and of patrick duncan and alfred hutchinson of south africa at the conference key development forecasts for ghana from international futures trade ghana 2012 summary trade statistics",
            "total_words": 8026,
            "unique_words_percentage": 30.00249190132071,
            "stopwords_percentage": 36.419137802143034
        },
        {
            "title": "Journal of Cybersecurity",
            "link": "https://en.wikipedia.org/wiki/Journal_of_Cybersecurity",
            "content": "the journal of cybersecurity is an open access peer reviewed academic journal of cybersecurity it is published by oxford university press it was first issued in 2015 its editors in chief are tyler moore and david pym the journal is a member of the committee on publication ethics cope the journal concentrates on the belief that computer science approaches are critical but are not enough to tackle cybersecurity threats moreover the article maintains the belief that interdisciplinary academic contributions are needed to understand the different facets of cybersecurity references ",
            "total_words": 89,
            "unique_words_percentage": 68.53932584269663,
            "stopwords_percentage": 41.57303370786517
        },
        {
            "title": "Cybersecurity engineering",
            "link": "https://en.wikipedia.org/wiki/Cybersecurity_engineering",
            "content": "cybersecurity engineering is a tech discipline focused on the protection of systems networks and data from unauthorized access cyberattacks and other malicious activities it applies engineering principles to the design implementation maintenance and evaluation of secure systems ensuring the integrity confidentiality and availability of information given the rising costs of cybercrimes which now amount to trillions of dollars in global economic losses each year organizations are seeking cybersecurity engineers to safeguard their data reduce potential damages and strengthen their defensive security systems history cybersecurity engineering began to take shape as a distinct field in the 1970s coinciding with the growth of computer networks and the internet initially security efforts focused on physical protection such as safeguarding mainframes and limiting access to sensitive areas however as systems became more interconnected digital security gained prominence in the 1970s the introduction of the first public key cryptosystems such as the rsa algorithm was a significant milestone enabling secure communications between parties that did not share a previously established secret during the 1980s the expansion of local area networks lans and the emergence of multi user operating systems such as unix highlighted the need for more sophisticated access controls and system audits the internet and the consolidation of security practices in the 1990s the rise of the internet alongside the advent of the world wide web www brought new challenges to cybersecurity the emergence of viruses worms and distributed denial of service ddos attacks required the development of new defensive techniques such as firewalls and antivirus software this period marked the solidification of the information security concept which began to include not only technical protections but also organizational policies and practices for risk mitigation modern era and technological advances in the 21st century the field of cybersecurity engineering expanded to tackle sophisticated threats including state sponsored attacks ransomware and phishing concepts like layered security architecture and the use of artificial intelligence for threat detection became critical the integration of frameworks such as the nist cybersecurity framework emphasized the need for a comprehensive approach that includes technical defense prevention response and incident recovery cybersecurity engineering has since expanded to encompass technical legal and ethical aspects reflecting the increasing complexity of the threat landscape core principles cybersecurity engineering is underpinned by several essential principles that are integral to creating resilient systems capable of withstanding and responding to cyber threats risk management involves identifying assessing and prioritizing potential risks to inform security decisions by understanding the likelihood and impact of various threats organizations can allocate resources effectively focusing on the most critical vulnerabilities defense in depth advocates for a layered security approach where multiple security measures are implemented at different levels of an organization by using overlapping controls such as firewalls intrusion detection systems and access controls an organization can better protect itself against diverse threats secure coding practices emphasizes the importance of developing software with security in mind techniques such as input validation proper error handling and the use of secure libraries help minimize vulnerabilities thereby reducing the risk of exploitation in production environments incident response and recovery effective incident response planning is crucial for managing potential security breaches organizations should establish predefined response protocols and recovery strategies to minimize damage restore systems quickly and learn from incidents to improve future security measures key areas of focus cybersecurity engineering works on several key areas they start with secure architecture designing systems and networks that integrate robust security features from the ground up this proactive approach helps mitigate risks associated with cyber threats during the design phase engineers engage in threat modeling to identify potential vulnerabilities and threats allowing them to develop effective countermeasures tailored to the specific environment this forward thinking strategy ensures that security is embedded within the infrastructure rather than bolted on as an afterthought penetration testing is another essential component of their work by simulating cyber attacks engineers can rigorously evaluate the effectiveness of existing security measures and uncover weaknesses before malicious actors exploit them this hands on testing approach not only identifies vulnerabilities but also helps organizations understand their risk landscape more comprehensively moreover cybersecurity engineers ensure that systems comply with regulatory and industry standards such as iso 27001 and nist guidelines compliance is vital not only for legal adherence but also for establishing a framework of best practices that enhance the overall security posture technologies and tools firewalls and ids ips firewalls whether hardware or software based are vital components of a cybersecurity infrastructure acting as barriers that control incoming and outgoing network traffic according to established security rules by preventing unauthorized access firewalls protect networks from potential threats complementing this intrusion detection systems ids continuously monitor network traffic to detect suspicious activities alerting administrators to potential breaches intrusion prevention systems ips enhance these measures by not only detecting threats but also actively blocking them in real time creating a more proactive security posture encryption encryption is a cornerstone of data protection employing sophisticated cryptographic techniques to secure sensitive information this process ensures that data is rendered unreadable to unauthorized users safeguarding both data at rest such as files stored on servers and data in transit like information sent over the internet by implementing encryption protocols organizations can maintain confidentiality and integrity protecting critical assets from cyber threats and data breaches security information and event management siem siem systems play a crucial role in modern cybersecurity engineering by aggregating and analyzing data from various sources across an organization s it environment they provide a comprehensive overview of security alerts and events enabling cybersecurity engineers to detect anomalies and respond to incidents swiftly by correlating information from different devices and applications siem tools enhance situational awareness and support compliance with regulatory requirements vulnerability assessment tools vulnerability assessment tools are essential for identifying and evaluating security weaknesses within systems and applications these tools conduct thorough scans to detect vulnerabilities categorizing them based on severity this prioritization allows cybersecurity engineers to focus on addressing the most critical vulnerabilities first thus reducing the organization s risk exposure and enhancing overall security effectiveness threat detection and response tdr tdr solutions utilize advanced analytics to sift through vast amounts of data identifying patterns that may indicate potential threats tools like security information and event management siem and user and entity behavior analytics ueba provide real time insights into security incidents enabling organizations to respond effectively to threats before they escalate traffic control and quality of service qos traffic control measures in cybersecurity engineering are designed to optimize the flow of data within networks mitigating risks such as distributed denial of service ddos attacks by utilizing technologies like web application firewalls waf and load balancers organizations can ensure secure and efficient traffic distribution additionally implementing quality of service qos protocols prioritizes critical applications and services ensuring they maintain operational integrity even in the face of potential security incidents or resource contention endpoint detection and response edr and extended detection and response xdr edr tools focus on monitoring and analyzing endpoint activities such as those on laptops and mobile devices to detect threats in real time xdr expands on edr by integrating multiple security products such as network analysis tools providing a more holistic view of an organization s security posture this comprehensive insight aids in the early detection and mitigation of threats across various points in the network standards and regulations various countries establish legislative frameworks that define requirements for the protection of personal data and information security across different sectors in the united states specific regulations play a critical role in safeguarding sensitive information the health insurance portability and accountability act hipaa outlines stringent standards for protecting health information ensuring that healthcare organizations maintain the confidentiality and integrity of patient data the sarbanes oxley act sox sets forth compliance requirements aimed at enhancing the accuracy and reliability of financial reporting and corporate governance thereby securing corporate data additionally the federal information security management act fisma mandates comprehensive security standards for federal agencies and their contractors ensuring a unified approach to information security across the government sector globally numerous other regulations also address data protection such as the general data protection regulation gdpr in the european union which sets a high standard for data privacy and empowers individuals with greater control over their personal information these frameworks collectively contribute to establishing robust cybersecurity measures and promote best practices across various industries education a career in cybersecurity engineering typically requires a strong educational foundation in information technology or a related field many professionals pursue a bachelor s degree in cybersecurity or computer engineering which covers essential topics such as network security cryptography and risk management for those seeking advanced knowledge a master s degree in cybersecurity engineering can provide deeper insights into specialized areas like ethical hacking secure software development and incident response strategies additionally hands on training through internships or lab experiences is highly valuable as it equips students with practical skills essential for addressing real world security challenges continuous education is crucial in this field with many engineers opting for certifications to stay current with industry trends and technologies security certifications are important credentials for professionals looking to demonstrate their expertise in cybersecurity practices key certifications include certified information systems security professional cissp globally recognized for security professionals certified information security manager cism focuses on security management certified ethical hacker ceh validates skills in penetration testing and ethical hacking references ",
            "total_words": 1563,
            "unique_words_percentage": 40.56301983365323,
            "stopwords_percentage": 31.285988483685223
        }
    ]
}